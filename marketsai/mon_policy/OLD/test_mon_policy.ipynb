{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test suite for env_template_sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from marketsai.mon_policy.env_mon_policy import MonPolicy\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "\n",
    "# environment config\n",
    "n_firms = 2\n",
    "n_inds = 200\n",
    "env_config = {\n",
    "    \"horizon\": 60,\n",
    "    \"n_inds\": n_inds,\n",
    "    \"n_firms\": n_firms,\n",
    "    \"eval_mode\": False,\n",
    "    \"analysis_mode\": False,\n",
    "    \"noagg\": False,\n",
    "    \"obs_idshock\": False,\n",
    "    \"seed_eval\": 2000,\n",
    "    \"seed_analisys\": 3000,\n",
    "    \"markup_min\": 1.2,\n",
    "    \"markup_max\": 2,\n",
    "    \"markup_min\": 1.2,\n",
    "    \"markup_start\": 1.3,\n",
    "    \"rew_mean\": 0,\n",
    "    \"rew_std\": 1,\n",
    "    \"parameters\": {\n",
    "        \"beta\": 0.95 ** (1 / 12),\n",
    "        \"log_g_bar\": 0.0021,\n",
    "        \"rho_g\": 0.61,\n",
    "        \"sigma_g\": 0.0019,\n",
    "        \"theta\": 1.5,\n",
    "        \"eta\": 10.5,\n",
    "        \"menu_cost\": 0.17,\n",
    "        \"sigma_z\": 0.038,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def process_rewards(r, BETA):\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, len(r))):\n",
    "        running_add = running_add * BETA + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate spaces\n",
    "env = MonPolicy(env_config)\n",
    "print(\n",
    "    \"action space type:\",\n",
    "    type(env.action_space[\"firm_0\"][\"move_prob\"].sample()),\n",
    "    type(env.action_space[\"firm_0\"][\"reset_markup\"].sample()),\n",
    "    \"action space sample:\",\n",
    "    {\n",
    "        \"move_prob\": env.action_space[\"firm_0\"][\"move_prob\"].sample(),\n",
    "        \"reset_markup\": env.action_space[\"firm_0\"][\"reset_markup\"].sample(),\n",
    "    },\n",
    ")\n",
    "print(\n",
    "    \"obs space type:\",\n",
    "    type(env.observation_space[\"firm_0\"].sample()),\n",
    "    \"obs space sample:\",\n",
    "    env.observation_space[\"firm_0\"].sample(),\n",
    ")\n",
    "obs_init = env.reset()\n",
    "print(\"inital observation\", obs_init)\n",
    "print(\n",
    "    \"obs_init contained in obs_space?\",\n",
    "    env.observation_space[\"firm_0\"].contains(obs_init[\"firm_0\"]),\n",
    ")\n",
    "if not env.observation_space[\"firm_0\"].contains(obs_init[\"firm_0\"]):\n",
    "    print(obs_init)\n",
    "print(\n",
    "    \"random number in [-1,1] contained in action_space?\",\n",
    "    env.action_space[\"firm_0\"][\"reset_markup\"].contains(\n",
    "        np.array([np.random.uniform(-1, 1)])\n",
    "    ),\n",
    ")\n",
    "obs, rew, done, info = env.step(\n",
    "    {\n",
    "        f\"firm_{i}\": {\n",
    "            \"move_prob\": env.action_space[\"firm_0\"][\"move_prob\"].sample(),\n",
    "            \"reset_markup\": env.action_space[\"firm_0\"][\"reset_markup\"].sample(),\n",
    "        }\n",
    "        for i in range(env.n_agents)\n",
    "    }\n",
    ")\n",
    "print(\n",
    "    \"obs after step contained in obs space?\",\n",
    "    env.observation_space[\"firm_0\"].contains(obs[\"firm_0\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_timing = {\n",
    "    \"time_init\": [],\n",
    "    \"time_reset\": [],\n",
    "    \"time_step\": [],\n",
    "    \"max_passthrough\": [],\n",
    "}\n",
    "env_config[\"analysis_mode\"] = True\n",
    "time_preinit = time.time()\n",
    "env = MonPolicy(env_config=env_config)\n",
    "time_postinit = time.time()\n",
    "env.reset()\n",
    "time_postreset = time.time()\n",
    "obs, rew, done, info = env.step(\n",
    "    {\n",
    "        f\"firm_{i}\": {\n",
    "            \"move_prob\": env.action_space[\"firm_0\"][\"move_prob\"].sample(),\n",
    "            \"reset_markup\": env.action_space[\"firm_0\"][\"reset_markup\"].sample(),\n",
    "        }\n",
    "        for i in range(env.n_agents)\n",
    "    }\n",
    ")\n",
    "time_poststep = time.time()\n",
    "\n",
    "data_timing[\"time_init\"].append((time_postinit - time_preinit) * 1000)\n",
    "data_timing[\"time_reset\"].append((time_postreset - time_postinit) * 1000)\n",
    "data_timing[\"time_step\"].append((time_poststep - time_postreset) * 1000)\n",
    "data_timing[\"max_passthrough\"].append(1 / (time_poststep - time_postreset))\n",
    "print(\"in ms:\", data_timing)\n",
    "print(\n",
    "    \"Size of obs:\",\n",
    "    sys.getsizeof(obs),\n",
    "    \"\\n\" \"Size of info:\",\n",
    "    sys.getsizeof(info),\n",
    "    \"\\n\" \"Size of env:\",\n",
    "    sys.getsizeof(env),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speed test\n",
    "epsilon_z = np.array([random.gauss(0, 1) for i in range(env.n_agents)])\n",
    "log_z = 0.01 * epsilon_z\n",
    "%timeit math.pow(math.e,log_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate\n",
    "SIMUL_PERIODS = 100000\n",
    "env = MonPolicy(env_config=env_config)\n",
    "mu_stats, rew_stats, g_stats = env.random_sample(SIMUL_PERIODS)\n",
    "print(\n",
    "    \"[mu_max, mu_min, mu_mean, mu_std]:\",\n",
    "    mu_stats,\n",
    "    \"\\n\" + \"[rew_max, rew_min, rew_mean, rew_std:]\",\n",
    "    rew_stats,\n",
    "    \"\\n\" + \"[g_max, g_min, g_mean, g_std:]\",\n",
    "    g_stats,\n",
    "    \"\\n\" + \"[rew_disc_max, rew_disc_min, rew_disc_mean, rew_disc_std:]\",\n",
    "    0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run analysis mode\n",
    "env_config_analysis = env_config.copy()\n",
    "env_config_analysis[\"analysis_mode\"] = True\n",
    "env = MonPolicy(env_config=env_config_analysis)\n",
    "mu_list = []\n",
    "rew_list = []\n",
    "shock_list = []\n",
    "\n",
    "env.reset()\n",
    "for t in range(200):\n",
    "    if t % 200 == 0:\n",
    "        obs = env.reset()\n",
    "    obs, rew, done, info = env.step(\n",
    "        {\n",
    "            f\"firm_{i}\": {\n",
    "                \"move_prob\": env.action_space[\"firm_0\"][\"move_prob\"].sample(),\n",
    "                \"reset_markup\": env.action_space[\"firm_0\"][\"reset_markup\"].sample(),\n",
    "            }\n",
    "            for i in range(env.n_agents)\n",
    "        }\n",
    "    )\n",
    "    shock_list.append(env.log_g)\n",
    "    mu_list.append(env.mu_ij[0])\n",
    "    rew_list.append(env.profits[0])\n",
    "disc_rew = process_rewards(rew_list, 0.99)\n",
    "print(\n",
    "    \"Discounted Rewards\",\n",
    "    disc_rew,\n",
    "    \"\\n\" + \"mu_stats:\",\n",
    "    [\n",
    "        np.max(mu_list),\n",
    "        np.min(mu_list),\n",
    "        np.mean(mu_list),\n",
    "        np.std(mu_list),\n",
    "    ],\n",
    "    \"\\n\" + \"reward_stats:\",\n",
    "    [np.max(rew_list), np.min(rew_list), np.mean(rew_list), np.std(rew_list)],\n",
    ")\n",
    "plt.plot(shock_list)\n",
    "plt.legend([\"shock\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run evaluation mode\n",
    "env_config_eval = env_config.copy()\n",
    "env_config_eval[\"eval_mode\"] = True\n",
    "env_config_eval[\"simul_mode\"] = True\n",
    "env = MonPolicy(env_config=env_config_eval)\n",
    "k_list = []\n",
    "rew_list = []\n",
    "shock_list = []\n",
    "\n",
    "env.reset()\n",
    "for t in range(200):\n",
    "    if t % 200 == 0:\n",
    "        obs = env.reset()\n",
    "    obs, rew, done, info = env.step(\n",
    "        {\n",
    "            f\"firm_{i}\": {\n",
    "                \"move_prob\": env.action_space[\"firm_0\"][\"move_prob\"].sample(),\n",
    "                \"reset_markup\": env.action_space[\"firm_0\"][\"reset_markup\"].sample(),\n",
    "            }\n",
    "            for i in range(env.n_agents)\n",
    "        }\n",
    "    )\n",
    "    shock_list.append(env.log_g)\n",
    "    mu_list.append(env.mu_ij[0])\n",
    "    rew_list.append(env.profits[0])\n",
    "disc_rew = process_rewards(rew_list, 0.99)\n",
    "print(\n",
    "    \"Discounted Rewards\",\n",
    "    disc_rew,\n",
    "    \"\\n\" + \"cmu_stats:\",\n",
    "    [\n",
    "        np.max(mu_list),\n",
    "        np.min(mu_list),\n",
    "        np.mean(mu_list),\n",
    "        np.std(mu_list),\n",
    "    ],\n",
    "    \"\\n\" + \"reward_stats:\",\n",
    "    [np.max(rew_list), np.min(rew_list), np.mean(rew_list), np.std(rew_list)],\n",
    ")\n",
    "plt.plot(shock_list)\n",
    "plt.legend([\"shock\"])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1c99579f0a861f1ade1e1abc4784a15ca138f424327e789e2dba1116d0806699"
  },
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit ('marketsai-reVLCGV_-py3.8': poetry)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
