{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Imports\n",
    "from marketsai.economies.capital_mkts.capital_market import CapitalMarket\n",
    "import scipy.io as sio\n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "from marketsai.utils import encode\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import csv\n",
    "import json\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.tune.registry import register_env\n",
    "from ray import shutdown, init"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" GLOBAL CONFIGS \"\"\"\n",
    "# Script Options\n",
    "FOR_PUBLIC = True  # for publication\n",
    "SAVE_CSV = False  # save learning CSV\n",
    "PLOT_PROGRESS = True  # create plot with progress\n",
    "SIMUL_PERIODS = 10000\n",
    "# Input Directories\n",
    "# Rl experiment\n",
    "\"\"\" CHANGE HERE \"\"\"\n",
    "env_label = \"capital_market\"\n",
    "register_env(env_label, CapitalMarket)\n",
    "INPUT_PATH_EXPERS = \"/Users/matiascovarrubias/Dropbox/RL_macro/Experiments/expINFO_native_multi_hh_cap_market_run_Aug28_PPO.json\"\n",
    "# GDSGE policy\n",
    "dir_policy_folder = (\n",
    "    \"/Users/matiascovarrubias/Dropbox/RL_macro/Econ_algos/capital_market/Results/\"\n",
    ")\n",
    "\n",
    "# Output Directories\n",
    "if FOR_PUBLIC:\n",
    "    OUTPUT_PATH_EXPERS = \"/Users/matiascovarrubias/Dropbox/RL_macro/Experiments/\"\n",
    "    OUTPUT_PATH_FIGURES = \"/Users/matiascovarrubias/Dropbox/RL_macro/Documents/Figures/\"\n",
    "    OUTPUT_PATH_TABLES = \"/Users/matiascovarrubias/Dropbox/RL_macro/Documents/Tables/\"\n",
    "else:\n",
    "    OUTPUT_PATH_EXPERS = \"/Users/matiascovarrubias/Dropbox/RL_macro/Experiments/ALL/\"\n",
    "    OUTPUT_PATH_FIGURES = (\n",
    "        \"/Users/matiascovarrubias/Dropbox/RL_macro/Documents/Figures/ALL/\"\n",
    "    )\n",
    "    OUTPUT_PATH_TABLES = (\n",
    "        \"/Users/matiascovarrubias/Dropbox/RL_macro/Documents/Tables/ALL/\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Plot options\n",
    "sn.color_palette(\"Set2\")\n",
    "sn.set_style(\"ticks\")  # grid styling, \"dark\"\n",
    "# plt.figure(figure=(8, 4))\n",
    "# choose between \"paper\", \"talk\" or \"poster\"\n",
    "sn.set_context(\n",
    "    \"paper\",\n",
    "    font_scale=1.4,\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" Step 0: import experiment data and initalize empty output data \"\"\"\n",
    "with open(INPUT_PATH_EXPERS) as f:\n",
    "    exp_data_dict = json.load(f)\n",
    "\n",
    "# UNPACK USEFUL DATA\n",
    "n_agents_list = exp_data_dict[\"n_agents\"]\n",
    "exp_names = exp_data_dict[\"exp_names\"]\n",
    "checkpoints_dirs = exp_data_dict[\"checkpoints\"]\n",
    "progress_csv_dirs = exp_data_dict[\"progress_csv_dirs\"]\n",
    "# best_rewards = exp_data_dict[\"best_rewards\"]\n",
    "\n",
    "# Create output directory\n",
    "exp_data_analysis_dict = {\n",
    "    \"n_hh\": [],\n",
    "    \"max rewards\": [],\n",
    "    \"time to peak\": [],\n",
    "    \"Mean Agg. K\": [],\n",
    "    \"S.D. Agg. K\": [],\n",
    "    \"Max K\": [],\n",
    "    \"Min K\": [],\n",
    "    \"Mean Agg. s\": [],\n",
    "    \"S.D. Agg. s\": [],\n",
    "    \"Max s\": [],\n",
    "    \"Min s\": [],\n",
    "    \"Mean Price\": [],\n",
    "    \"S.D. Price\": [],\n",
    "}\n",
    "exp_data_analysis_econ_dict = {\n",
    "    \"n_hh\": [],\n",
    "    \"time to peak\": [],\n",
    "    \"Mean Agg. K\": [],\n",
    "    \"S.D. Agg. K\": [],\n",
    "    \"Max K\": [],\n",
    "    \"Min K\": [],\n",
    "    \"Mean Agg. s\": [],\n",
    "    \"S.D. Agg. s\": [],\n",
    "    \"Max s\": [],\n",
    "    \"Min s\": [],\n",
    "    \"Mean Price\": [],\n",
    "    \"S.D. Price\": [],\n",
    "}\n",
    "exp_data_simul_dict = {\n",
    "    \"n_hh\": [],\n",
    "    \"max rewards\": [],\n",
    "    \"time to peak\": [],\n",
    "    \"Mean Agg. K\": [],\n",
    "    \"S.D. Agg. K\": [],\n",
    "    \"Max K\": [],\n",
    "    \"Min K\": [],\n",
    "    \"Mean Agg. s\": [],\n",
    "    \"S.D. Agg. s\": [],\n",
    "    \"Max s\": [],\n",
    "    \"Min s\": [],\n",
    "    \"Mean Price\": [],\n",
    "    \"S.D. Price\": [],\n",
    "}\n",
    "exp_data_simul_econ_dict = {\n",
    "    \"n_hh\": [],\n",
    "    \"time to peak\": [],\n",
    "    \"Mean Agg. K\": [],\n",
    "    \"S.D. Agg. K\": [],\n",
    "    \"Max K\": [],\n",
    "    \"Min K\": [],\n",
    "    \"Mean Agg. s\": [],\n",
    "    \"S.D. Agg. s\": [],\n",
    "    \"Max s\": [],\n",
    "    \"Min s\": [],\n",
    "    \"Mean Price\": [],\n",
    "    \"S.D. Price\": [],\n",
    "}\n",
    "# init ray\n",
    "shutdown()\n",
    "init()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" Step 1: Plot progress during learning run \"\"\"\n",
    "\n",
    "if PLOT_PROGRESS == True:\n",
    "    # Big plot\n",
    "    for i in range(len(exp_names)):\n",
    "        data_progress_df = pd.read_csv(progress_csv_dirs[i])\n",
    "        max_rewards = data_progress_df[\n",
    "            \"evaluation/custom_metrics/discounted_rewards_mean\"\n",
    "        ].max()\n",
    "        exp_data_simul_dict[\"max rewards\"].append(max_rewards)\n",
    "        exp_data_simul_dict[\"time to peak\"].append(0)\n",
    "        exp_data_analysis_dict[\"max rewards\"].append(max_rewards)\n",
    "        exp_data_analysis_dict[\"time to peak\"].append(0)\n",
    "        data_progress_df[\"evaluation/custom_metrics/discounted_rewards_mean\"] = (\n",
    "            data_progress_df[\"evaluation/custom_metrics/discounted_rewards_mean\"]\n",
    "            / max_rewards\n",
    "        )\n",
    "        learning_plot_big = sn.lineplot(\n",
    "            data=data_progress_df,\n",
    "            y=\"evaluation/custom_metrics/discounted_rewards_mean\",\n",
    "            x=\"episodes_total\",\n",
    "        )\n",
    "\n",
    "    learning_plot_big = learning_plot_big.get_figure()\n",
    "    plt.ylabel(\"Discounted utility\")\n",
    "    plt.xlabel(\"Timesteps (thousands)\")\n",
    "    plt.xlim([0, 600])\n",
    "    plt.legend(labels=[f\"{i+1} household(s)\" for i in range(len(n_agents_list))])\n",
    "    learning_plot_big.savefig(\n",
    "        OUTPUT_PATH_FIGURES + \"progress_BIG_\" + exp_names[-1] + \".png\"\n",
    "    )\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # small plot\n",
    "    for i in range(len(exp_names)):\n",
    "        data_progress_df = pd.read_csv(progress_csv_dirs[i])\n",
    "        max_rewards = data_progress_df[\n",
    "            \"evaluation/custom_metrics/discounted_rewards_mean\"\n",
    "        ].max()\n",
    "        data_progress_df[\"evaluation/custom_metrics/discounted_rewards_mean\"] = (\n",
    "            data_progress_df[\"evaluation/custom_metrics/discounted_rewards_mean\"]\n",
    "            / max_rewards\n",
    "        )\n",
    "        learning_plot_small = sn.lineplot(\n",
    "            data=data_progress_df,\n",
    "            y=\"evaluation/custom_metrics/discounted_rewards_mean\",\n",
    "            x=\"episodes_total\",\n",
    "        )\n",
    "\n",
    "    learning_plot_small = learning_plot_small.get_figure()\n",
    "    plt.ylabel(\"Discounted utility\")\n",
    "    plt.xlabel(\"Timesteps (thousands)\")\n",
    "    plt.xlim([0, 100])\n",
    "    plt.legend(labels=[f\"{i+1} household(s)\" for i in range(len(n_agents_list))])\n",
    "    learning_plot_small.savefig(\n",
    "        OUTPUT_PATH_FIGURES + \"progress_SMALL_\" + exp_names[-1] + \".png\"\n",
    "    )\n",
    "    plt.show()\n",
    "    plt.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" Step 2: Congif env, Restore RL policy and then simualte analysis trajectory \"\"\"\n",
    "y_agg_list = [[] for i in n_agents_list]\n",
    "s_agg_list = [[] for i in n_agents_list]\n",
    "c_agg_list = [[] for i in n_agents_list]\n",
    "k_agg_list = [[] for i in n_agents_list]\n",
    "k_max_list = [[] for i in n_agents_list]\n",
    "k_min_list = [[] for i in n_agents_list]\n",
    "s_max_list = [[] for i in n_agents_list]\n",
    "s_min_list = [[] for i in n_agents_list]\n",
    "shock_agg_list = [[] for i in n_agents_list]\n",
    "p_list = [[] for i in n_agents_list]\n",
    "\n",
    "for ind, n_hh in enumerate(n_agents_list):\n",
    "    \"\"\"Step 2.0: replicate original environemnt and config\"\"\"\n",
    "    env_horizon = 1000\n",
    "    n_hh = n_hh\n",
    "    n_capital = 1\n",
    "    beta = 0.98\n",
    "    env_config_analysis = {\n",
    "        \"horizon\": 1000,\n",
    "        \"n_hh\": n_hh,\n",
    "        \"n_capital\": n_capital,\n",
    "        \"eval_mode\": False,\n",
    "        \"simul_mode\": False,\n",
    "        \"analysis_mode\": True,\n",
    "        \"max_savings\": 0.6,\n",
    "        \"bgt_penalty\": 1,\n",
    "        \"shock_idtc_values\": [0.9, 1.1],\n",
    "        \"shock_idtc_transition\": [[0.9, 0.1], [0.1, 0.9]],\n",
    "        \"shock_agg_values\": [0.8, 1.2],\n",
    "        \"shock_agg_transition\": [[0.95, 0.05], [0.05, 0.95]],\n",
    "        \"parameters\": {\"delta\": 0.04, \"alpha\": 0.3, \"phi\": 0.5, \"beta\": beta},\n",
    "    }\n",
    "\n",
    "    # We instantiate the environment to extract information.\n",
    "    \"\"\" CHANGE HERE \"\"\"\n",
    "    env = CapitalMarket(env_config_analysis)\n",
    "    config_analysis = {\n",
    "        \"gamma\": beta,\n",
    "        \"env\": env_label,\n",
    "        \"env_config\": env_config_analysis,\n",
    "        \"horizon\": env_horizon,\n",
    "        \"explore\": False,\n",
    "        \"framework\": \"torch\",\n",
    "        \"multiagent\": {\n",
    "            \"policies\": {\n",
    "                \"hh\": (\n",
    "                    None,\n",
    "                    env.observation_space[\"hh_0\"],\n",
    "                    env.action_space[\"hh_0\"],\n",
    "                    {},\n",
    "                ),\n",
    "            },\n",
    "            \"policy_mapping_fn\": (lambda agent_id: agent_id.split(\"_\")[0]),\n",
    "            \"replay_mode\": \"independent\",\n",
    "        },\n",
    "    }\n",
    "    \"\"\" Step 2.1: restore trainer \"\"\"\n",
    "\n",
    "    # restore the trainer\n",
    "    trained_trainer = PPOTrainer(env=env_label, config=config_analysis)\n",
    "    trained_trainer.restore(checkpoints_dirs[ind])\n",
    "\n",
    "    \"\"\" Step 2: Simulate an episode (MAX_steps timesteps) \"\"\"\n",
    "    shock_idtc_list = [[] for i in range(env.n_hh)]\n",
    "    y_list = [[] for i in range(env.n_hh)]\n",
    "    s_list = [[] for i in range(env.n_hh)]\n",
    "    c_list = [[] for i in range(env.n_hh)]\n",
    "    k_list = [[] for i in range(env.n_hh)]\n",
    "\n",
    "    # loop\n",
    "    obs = env.reset()\n",
    "    for t in range(env_horizon):\n",
    "        action = {}\n",
    "        for i in range(env.n_hh):\n",
    "            action[f\"hh_{i}\"] = trained_trainer.compute_action(\n",
    "                obs[f\"hh_{i}\"], policy_id=\"hh\"\n",
    "            )\n",
    "\n",
    "        obs, rew, done, info = env.step(action)\n",
    "        for i in range(env.n_hh):\n",
    "            shock_idtc_list[i].append(obs[\"hh_0\"][1][i])\n",
    "            y_list[i].append(info[\"hh_0\"][\"income\"][i])\n",
    "            s_list[i].append(info[\"hh_0\"][\"savings\"][i][0])\n",
    "            c_list[i].append(info[\"hh_0\"][\"consumption\"][i])\n",
    "            k_list[i].append(info[\"hh_0\"][\"capital\"][i][0])\n",
    "\n",
    "        # k_agg_list.append(np.sum([k_list[[j][t-1] for j in range(env_loop.n_hh)]))\n",
    "        shock_agg_list[ind].append(obs[\"hh_0\"][2])\n",
    "        y_agg_list[ind].append(np.sum([y_list[i][t] for i in range(env.n_hh)]))\n",
    "        s_agg_list[ind].append(\n",
    "            np.sum([s_list[i][t] * y_list[i][t] for i in range(env.n_hh)])\n",
    "            / y_agg_list[ind][t]\n",
    "        )\n",
    "        c_agg_list[ind].append(np.sum([y_list[i][t] for i in range(env.n_hh)]))\n",
    "        k_agg_list[ind].append(np.sum([k_list[i][t] for i in range(env.n_hh)]))\n",
    "        k_max_list[ind].append(np.max([k_list[i][t] for i in range(env.n_hh)]))\n",
    "        k_min_list[ind].append(np.min([k_list[i][t] for i in range(env.n_hh)]))\n",
    "        s_max_list[ind].append(np.max([s_list[i][t] for i in range(env.n_hh)]))\n",
    "        s_min_list[ind].append(np.min([s_list[i][t] for i in range(env.n_hh)]))\n",
    "        p_list[ind].append(info[\"hh_0\"][\"price\"][0])\n",
    "\n",
    "    \"\"\" Step 2.2: Calculate Statistics and save in table \"\"\"\n",
    "\n",
    "    exp_data_analysis_dict[\"n_hh\"].append(n_hh)\n",
    "    exp_data_analysis_dict[\"Mean Agg. K\"].append(np.mean(k_agg_list[ind]))\n",
    "    exp_data_analysis_dict[\"S.D. Agg. K\"].append(np.std(k_agg_list[ind]))\n",
    "    exp_data_analysis_dict[\"Max K\"].append(np.max(k_max_list[ind]))\n",
    "    exp_data_analysis_dict[\"Min K\"].append(np.min(k_min_list[ind]))\n",
    "    exp_data_analysis_dict[\"Mean Agg. s\"].append(np.mean(s_agg_list[ind]))\n",
    "    exp_data_analysis_dict[\"S.D. Agg. s\"].append(np.std(s_agg_list[ind]))\n",
    "    exp_data_analysis_dict[\"Max s\"].append(np.max(s_max_list[ind]))\n",
    "    exp_data_analysis_dict[\"Min s\"].append(np.min(s_min_list[ind]))\n",
    "    exp_data_analysis_dict[\"Mean Price\"].append(np.mean(p_list[ind]))\n",
    "    exp_data_analysis_dict[\"S.D. Price\"].append(np.std(p_list[ind]))\n",
    "\n",
    "    \"\"\" Step 2.3: Plot individual analysis trajectories \"\"\"\n",
    "\n",
    "    # Idiosyncratic trajectories\n",
    "    x = [i for i in range(100)]\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sn.lineplot(x, shock_agg_list[ind][:100], label=f\"household {i+1}\", legend=0)\n",
    "    plt.title(\"Shock\")\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    for i in range(env.n_hh):\n",
    "        sn.lineplot(x, s_list[i][:100], label=f\"household {i+1}\", legend=0)\n",
    "    plt.title(\"Savings Rate\")\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    for i in range(env.n_hh):\n",
    "        sn.lineplot(x, y_list[i][:100], label=f\"household {i+1}\", legend=0)\n",
    "    plt.title(\"Income\")\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    # plt.plot(k_agg_list[:100])\n",
    "    for i in range(env.n_hh):\n",
    "        sn.lineplot(x, k_list[i][:100], label=f\"household {i+1}\", legend=0)\n",
    "    plt.title(\"Capital\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    plt.legend(handles, labels, loc=\"lower right\", prop={\"size\": 6})\n",
    "    # plt.legend(labels=[f\"{i+1} households\" for i in range(env.n_hh)], loc='upper center', bbox_to_anchor=(0.5, 1.05))\n",
    "    plt.savefig(OUTPUT_PATH_FIGURES + \"SimInd_\" + exp_names[ind] + \".png\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "print(exp_data_analysis_dict)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" Step 3: Create aggregate trajectory plots \"\"\"\n",
    "\n",
    "x = [i for i in range(100)]\n",
    "plt.subplot(2, 2, 1)\n",
    "for i in range(len(n_agents_list)):\n",
    "    sn.lineplot(x, shock_agg_list[i][:100], label=f\"{i+1} household(s)\", legend=0)\n",
    "plt.title(\"Aggregate Shock\")\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "for i in range(len(n_agents_list)):\n",
    "    sn.lineplot(x, y_agg_list[i][:100], label=f\"{i+1} household(s)\", legend=0)\n",
    "plt.title(\"Aggregate Income\")\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "for i in range(len(n_agents_list)):\n",
    "    sn.lineplot(x, s_agg_list[i][:100], label=f\"{i+1} household(s)\", legend=0)\n",
    "plt.title(\"Aggregate Savings Rate\")\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "for i in range(len(n_agents_list)):\n",
    "    sn.lineplot(x, k_agg_list[i][:100], label=f\"{i+1} household(s)\", legend=0)\n",
    "plt.title(\"Aggregate Capital\")\n",
    "\n",
    "plt.tight_layout()\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "plt.legend(handles, labels, loc=\"lower right\", prop={\"size\": 6})\n",
    "plt.savefig(OUTPUT_PATH_FIGURES + \"SimAgg_\" + exp_names[-1] + \".png\")\n",
    "plt.show()\n",
    "plt.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" Step 4: Simulate the RL policy for SIMUL_PERIODS and get statistics \"\"\"\n",
    "\n",
    "y_agg_list = [[] for i in n_agents_list]\n",
    "s_agg_list = [[] for i in n_agents_list]\n",
    "c_agg_list = [[] for i in n_agents_list]\n",
    "k_agg_list = [[] for i in n_agents_list]\n",
    "k_max_list = [[] for i in n_agents_list]\n",
    "k_min_list = [[] for i in n_agents_list]\n",
    "s_max_list = [[] for i in n_agents_list]\n",
    "s_min_list = [[] for i in n_agents_list]\n",
    "shock_agg_list = [[] for i in n_agents_list]\n",
    "p_list = [[] for i in n_agents_list]\n",
    "\n",
    "for ind, n_hh in enumerate(n_agents_list):\n",
    "    \"\"\"Step 4.0: replicate original environemnt and config\"\"\"\n",
    "    env_horizon = 1000\n",
    "    n_hh = n_hh\n",
    "    n_capital = 1\n",
    "    beta = 0.98\n",
    "    env_config_simul = {\n",
    "        \"horizon\": 1000,\n",
    "        \"n_hh\": n_hh,\n",
    "        \"n_capital\": n_capital,\n",
    "        \"eval_mode\": False,\n",
    "        \"simul_mode\": True,\n",
    "        \"analysis_mode\": False,\n",
    "        \"max_savings\": 0.6,\n",
    "        \"bgt_penalty\": 1,\n",
    "        \"shock_idtc_values\": [0.9, 1.1],\n",
    "        \"shock_idtc_transition\": [[0.9, 0.1], [0.1, 0.9]],\n",
    "        \"shock_agg_values\": [0.8, 1.2],\n",
    "        \"shock_agg_transition\": [[0.95, 0.05], [0.05, 0.95]],\n",
    "        \"parameters\": {\"delta\": 0.04, \"alpha\": 0.3, \"phi\": 0.5, \"beta\": beta},\n",
    "    }\n",
    "\n",
    "    # We instantiate the environment to extract information.\n",
    "    \"\"\" CHANGE HERE \"\"\"\n",
    "    env = CapitalMarket(env_config_simul)\n",
    "    config_analysis = {\n",
    "        \"gamma\": beta,\n",
    "        \"env\": env_label,\n",
    "        \"env_config\": env_config_simul,\n",
    "        \"horizon\": env_horizon,\n",
    "        \"explore\": False,\n",
    "        \"framework\": \"torch\",\n",
    "        \"multiagent\": {\n",
    "            \"policies\": {\n",
    "                \"hh\": (\n",
    "                    None,\n",
    "                    env.observation_space[\"hh_0\"],\n",
    "                    env.action_space[\"hh_0\"],\n",
    "                    {},\n",
    "                ),\n",
    "            },\n",
    "            \"policy_mapping_fn\": (lambda agent_id: agent_id.split(\"_\")[0]),\n",
    "            \"replay_mode\": \"independent\",\n",
    "        },\n",
    "    }\n",
    "    \"\"\" Step 4.1: restore trainer \"\"\"\n",
    "\n",
    "    # restore the trainer\n",
    "    trained_trainer = PPOTrainer(env=env_label, config=config_analysis)\n",
    "    trained_trainer.restore(checkpoints_dirs[ind])\n",
    "\n",
    "    \"\"\" Simulate an episode (SIMUL_PERIODS timesteps) \"\"\"\n",
    "    shock_idtc_list = [[] for i in range(env.n_hh)]\n",
    "    y_list = [[] for i in range(env.n_hh)]\n",
    "    s_list = [[] for i in range(env.n_hh)]\n",
    "    c_list = [[] for i in range(env.n_hh)]\n",
    "    k_list = [[] for i in range(env.n_hh)]\n",
    "\n",
    "    # loop\n",
    "    obs = env.reset()\n",
    "    for t in range(SIMUL_PERIODS):\n",
    "        action = {}\n",
    "        if t % 1000 == 0:\n",
    "            obs = env.reset()\n",
    "        for i in range(env.n_hh):\n",
    "            action[f\"hh_{i}\"] = trained_trainer.compute_action(\n",
    "                obs[f\"hh_{i}\"], policy_id=\"hh\"\n",
    "            )\n",
    "\n",
    "        obs, rew, done, info = env.step(action)\n",
    "        for i in range(env.n_hh):\n",
    "            shock_idtc_list[i].append(obs[\"hh_0\"][1][i])\n",
    "            y_list[i].append(info[\"hh_0\"][\"income\"][i])\n",
    "            s_list[i].append(info[\"hh_0\"][\"savings\"][i][0])\n",
    "            c_list[i].append(info[\"hh_0\"][\"consumption\"][i])\n",
    "            k_list[i].append(info[\"hh_0\"][\"capital\"][i][0])\n",
    "\n",
    "        # k_agg_list.append(np.sum([k_list[[j][t-1] for j in range(env_loop.n_hh)]))\n",
    "        shock_agg_list[ind].append(obs[\"hh_0\"][2])\n",
    "        y_agg_list[ind].append(np.sum([y_list[i][t] for i in range(env.n_hh)]))\n",
    "        s_agg_list[ind].append(\n",
    "            np.sum([s_list[i][t] * y_list[i][t] for i in range(env.n_hh)])\n",
    "            / y_agg_list[ind][t]\n",
    "        )\n",
    "        c_agg_list[ind].append(np.sum([y_list[i][t] for i in range(env.n_hh)]))\n",
    "        k_agg_list[ind].append(np.sum([k_list[i][t] for i in range(env.n_hh)]))\n",
    "        k_max_list[ind].append(np.max([k_list[i][t] for i in range(env.n_hh)]))\n",
    "        k_min_list[ind].append(np.min([k_list[i][t] for i in range(env.n_hh)]))\n",
    "        s_max_list[ind].append(np.max([s_list[i][t] for i in range(env.n_hh)]))\n",
    "        s_min_list[ind].append(np.min([s_list[i][t] for i in range(env.n_hh)]))\n",
    "        p_list[ind].append(info[\"hh_0\"][\"price\"][0])\n",
    "\n",
    "    exp_data_simul_dict[\"n_hh\"].append(n_hh)\n",
    "    exp_data_simul_dict[\"Mean Agg. K\"].append(np.mean(k_agg_list[ind]))\n",
    "    exp_data_simul_dict[\"S.D. Agg. K\"].append(np.std(k_agg_list[ind]))\n",
    "    exp_data_simul_dict[\"Max K\"].append(np.max(k_max_list[ind]))\n",
    "    exp_data_simul_dict[\"Min K\"].append(np.min(k_min_list[ind]))\n",
    "    exp_data_simul_dict[\"Mean Agg. s\"].append(np.mean(s_agg_list[ind]))\n",
    "    exp_data_simul_dict[\"S.D. Agg. s\"].append(np.std(s_agg_list[ind]))\n",
    "    exp_data_simul_dict[\"Max s\"].append(np.max(s_max_list[ind]))\n",
    "    exp_data_simul_dict[\"Min s\"].append(np.min(s_min_list[ind]))\n",
    "    exp_data_simul_dict[\"Mean Price\"].append(np.mean(p_list[ind]))\n",
    "    exp_data_simul_dict[\"S.D. Price\"].append(np.std(p_list[ind]))\n",
    "\n",
    "print(exp_data_simul_dict)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" Step 5: config env, Restore PI (GDSGE) policy and simulate analysis trajectory \"\"\"\n",
    "y_agg_list = [[] for i in n_agents_list]\n",
    "s_agg_list = [[] for i in n_agents_list]\n",
    "c_agg_list = [[] for i in n_agents_list]\n",
    "k_agg_list = [[] for i in n_agents_list]\n",
    "k_max_list = [[] for i in n_agents_list]\n",
    "k_min_list = [[] for i in n_agents_list]\n",
    "s_max_list = [[] for i in n_agents_list]\n",
    "s_min_list = [[] for i in n_agents_list]\n",
    "shock_agg_list = [[] for i in n_agents_list]\n",
    "p_list = [[] for i in n_agents_list]\n",
    "\n",
    "for ind, n_hh in enumerate([1, 2, 3]):\n",
    "    # replicate environment\n",
    "    env_horizon = 1000\n",
    "    n_hh = n_hh\n",
    "    n_capital = 1\n",
    "    beta = 0.98\n",
    "    env_config_analysis = {\n",
    "        \"horizon\": 1000,\n",
    "        \"n_hh\": n_hh,\n",
    "        \"n_capital\": n_capital,\n",
    "        \"eval_mode\": False,\n",
    "        \"simul_mode\": False,\n",
    "        \"analysis_mode\": True,\n",
    "        \"max_savings\": 0.6,\n",
    "        \"bgt_penalty\": 1,\n",
    "        \"shock_idtc_values\": [0.9, 1.1],\n",
    "        \"shock_idtc_transition\": [[0.9, 0.1], [0.1, 0.9]],\n",
    "        \"shock_agg_values\": [0.8, 1.2],\n",
    "        \"shock_agg_transition\": [[0.95, 0.05], [0.05, 0.95]],\n",
    "        \"parameters\": {\"delta\": 0.04, \"alpha\": 0.3, \"phi\": 0.5, \"beta\": beta},\n",
    "    }\n",
    "\n",
    "    # We instantiate the environment to extract information.\n",
    "    \"\"\" CHANGE HERE \"\"\"\n",
    "    env = CapitalMarket(env_config_analysis)\n",
    "    config_analysis = {\n",
    "        \"gamma\": beta,\n",
    "        \"env\": env_label,\n",
    "        \"env_config\": env_config_analysis,\n",
    "        \"horizon\": env_horizon,\n",
    "        \"explore\": False,\n",
    "        \"framework\": \"torch\",\n",
    "        \"multiagent\": {\n",
    "            \"policies\": {\n",
    "                \"hh\": (\n",
    "                    None,\n",
    "                    env.observation_space[\"hh_0\"],\n",
    "                    env.action_space[\"hh_0\"],\n",
    "                    {},\n",
    "                ),\n",
    "            },\n",
    "            \"policy_mapping_fn\": (lambda agent_id: agent_id.split(\"_\")[0]),\n",
    "            \"replay_mode\": \"independent\",\n",
    "        },\n",
    "    }\n",
    "    \"\"\" Step 5.1: import matlab struct \"\"\"\n",
    "    \"\"\" CHANGE HERE \"\"\"\n",
    "    dir_model = f\"cap_market_{n_hh}hh_11pts\"\n",
    "    matlab_struct = sio.loadmat(dir_policy_folder + dir_model, simplify_cells=True)\n",
    "    exp_data_analysis_econ_dict[\"time to peak\"].append(\n",
    "        matlab_struct[\"IterRslt\"][\"timeElapsed\"]\n",
    "    )\n",
    "    exp_data_simul_econ_dict[\"time to peak\"].append(\n",
    "        matlab_struct[\"IterRslt\"][\"timeElapsed\"]\n",
    "    )\n",
    "    if n_hh == 1:\n",
    "        K_grid = [\n",
    "            np.array(matlab_struct[\"IterRslt\"][\"var_state\"][f\"K\"]) for i in range(n_hh)\n",
    "        ]\n",
    "    else:\n",
    "        K_grid = [\n",
    "            np.array(matlab_struct[\"IterRslt\"][\"var_state\"][f\"K_{i+1}\"])\n",
    "            for i in range(n_hh)\n",
    "        ]\n",
    "    shock_grid = np.array([i for i in range(matlab_struct[\"IterRslt\"][\"shock_num\"])])\n",
    "    if n_hh == 1:\n",
    "        s_on_grid = [matlab_struct[\"IterRslt\"][\"var_policy\"][\"s\"] for i in range(n_hh)]\n",
    "    else:\n",
    "        s_on_grid = [\n",
    "            matlab_struct[\"IterRslt\"][\"var_policy\"][f\"s_{i+1}\"] for i in range(n_hh)\n",
    "        ]\n",
    "\n",
    "    s_interp = [\n",
    "        RegularGridInterpolator((shock_grid,) + tuple(K_grid), s_on_grid[i])\n",
    "        for i in range(n_hh)\n",
    "    ]\n",
    "\n",
    "    def compute_action(obs, policy_list: list, max_action: float):\n",
    "        K = obs[0]\n",
    "        shock_raw = [obs[2]] + list(obs[1])\n",
    "        shock_id = encode(shock_raw, dims=[2 for i in range(env.n_hh + 1)])\n",
    "        s = [policy_list[i](np.array([shock_id] + K)) for i in range(env.n_hh)]\n",
    "        action = np.array([2 * s[i] / max_action - 1 for i in range(env.n_hh)])\n",
    "        return action\n",
    "\n",
    "    \"\"\" Step 5.2: Simulate an episode (MAX_steps timesteps) \"\"\"\n",
    "    shock_idtc_list = [[] for i in range(env.n_hh)]\n",
    "    y_list = [[] for i in range(env.n_hh)]\n",
    "    s_list = [[] for i in range(env.n_hh)]\n",
    "    c_list = [[] for i in range(env.n_hh)]\n",
    "    k_list = [[] for i in range(env.n_hh)]\n",
    "\n",
    "    # loop\n",
    "    obs = env.reset()\n",
    "    for t in range(env_horizon):\n",
    "        action = {}\n",
    "        for i in range(env.n_hh):\n",
    "            action[f\"hh_{i}\"] = compute_action(obs[\"hh_0\"], s_interp, env.max_s_ij)[i]\n",
    "\n",
    "        obs, rew, done, info = env.step(action)\n",
    "        for i in range(env.n_hh):\n",
    "            shock_idtc_list[i].append(obs[\"hh_0\"][1][i])\n",
    "            y_list[i].append(info[\"hh_0\"][\"income\"][i])\n",
    "            s_list[i].append(info[\"hh_0\"][\"savings\"][i][0])\n",
    "            c_list[i].append(info[\"hh_0\"][\"consumption\"][i])\n",
    "            k_list[i].append(info[\"hh_0\"][\"capital\"][i][0])\n",
    "\n",
    "        # k_agg_list.append(np.sum([k_list[[j][t-1] for j in range(env_loop.n_hh)]))\n",
    "        shock_agg_list[ind].append(obs[\"hh_0\"][2])\n",
    "        y_agg_list[ind].append(np.sum([y_list[i][t] for i in range(env.n_hh)]))\n",
    "        s_agg_list[ind].append(\n",
    "            np.sum([s_list[i][t] * y_list[i][t] for i in range(env.n_hh)])\n",
    "            / y_agg_list[ind][t]\n",
    "        )\n",
    "        c_agg_list[ind].append(np.sum([y_list[i][t] for i in range(env.n_hh)]))\n",
    "        k_agg_list[ind].append(np.sum([k_list[i][t] for i in range(env.n_hh)]))\n",
    "        k_max_list[ind].append(np.max([k_list[i][t] for i in range(env.n_hh)]))\n",
    "        k_min_list[ind].append(np.min([k_list[i][t] for i in range(env.n_hh)]))\n",
    "        s_max_list[ind].append(np.max([s_list[i][t] for i in range(env.n_hh)]))\n",
    "        s_min_list[ind].append(np.min([s_list[i][t] for i in range(env.n_hh)]))\n",
    "        p_list[ind].append(info[\"hh_0\"][\"price\"][0])\n",
    "\n",
    "    \"\"\" Step 5.3: Calculate Statistics and save in table \"\"\"\n",
    "\n",
    "    exp_data_analysis_econ_dict[\"n_hh\"].append(n_hh)\n",
    "    exp_data_analysis_econ_dict[\"Mean Agg. K\"].append(np.mean(k_agg_list[ind]))\n",
    "    exp_data_analysis_econ_dict[\"S.D. Agg. K\"].append(np.std(k_agg_list[ind]))\n",
    "    exp_data_analysis_econ_dict[\"Max K\"].append(np.max(k_max_list[ind]))\n",
    "    exp_data_analysis_econ_dict[\"Min K\"].append(np.min(k_min_list[ind]))\n",
    "    exp_data_analysis_econ_dict[\"Mean Agg. s\"].append(np.mean(s_agg_list[ind]))\n",
    "    exp_data_analysis_econ_dict[\"S.D. Agg. s\"].append(np.std(s_agg_list[ind]))\n",
    "    exp_data_analysis_econ_dict[\"Max s\"].append(np.max(s_max_list[ind]))\n",
    "    exp_data_analysis_econ_dict[\"Min s\"].append(np.min(s_min_list[ind]))\n",
    "    exp_data_analysis_econ_dict[\"Mean Price\"].append(np.mean(p_list[ind]))\n",
    "    exp_data_analysis_econ_dict[\"S.D. Price\"].append(np.std(p_list[ind]))\n",
    "\n",
    "    \"\"\" Step 5.4: Plot individual trajectories \"\"\"\n",
    "\n",
    "    # Idiosyncratic trajectories\n",
    "    x = [i for i in range(100)]\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sn.lineplot(x, shock_agg_list[ind][:100], label=f\"household {i+1}\", legend=0)\n",
    "    plt.title(\"Shock\")\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    for i in range(env.n_hh):\n",
    "        sn.lineplot(x, s_list[i][:100], label=f\"household {i+1}\", legend=0)\n",
    "    plt.title(\"Savings Rate\")\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    for i in range(env.n_hh):\n",
    "        sn.lineplot(x, y_list[i][:100], label=f\"household {i+1}\", legend=0)\n",
    "    plt.title(\"Income\")\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    # plt.plot(k_agg_list[:100])\n",
    "    for i in range(env.n_hh):\n",
    "        sn.lineplot(x, k_list[i][:100], label=f\"household {i+1}\", legend=0)\n",
    "    plt.title(\"Capital\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    plt.legend(handles, labels, loc=\"lower right\", prop={\"size\": 6})\n",
    "    # plt.legend(labels=[f\"{i+1} households\" for i in range(env.n_hh)], loc='upper center', bbox_to_anchor=(0.5, 1.05))\n",
    "    plt.savefig(OUTPUT_PATH_FIGURES + \"SimInd_\" + exp_names[ind] + \".png\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "print(exp_data_analysis_econ_dict)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" STEP 6: Get aggregate trakectory plots for PI (GDSE)  \"\"\"\n",
    "\n",
    "x = [i for i in range(100)]\n",
    "plt.subplot(2, 2, 1)\n",
    "for i in range(len([1, 2, 3])):\n",
    "    sn.lineplot(x, shock_agg_list[i][:100], label=f\"{i+1} household(s)\", legend=0)\n",
    "plt.title(\"Aggregate Shock\")\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "for i in range(len([1, 2, 3])):\n",
    "    sn.lineplot(x, y_agg_list[i][:100], label=f\"{i+1} household(s)\", legend=0)\n",
    "plt.title(\"Aggregate Income\")\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "for i in range(len([1, 2, 3])):\n",
    "    sn.lineplot(x, s_agg_list[i][:100], label=f\"{i+1} household(s)\", legend=0)\n",
    "plt.title(\"Aggregate Savings Rate\")\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "for i in range(len([1, 2, 3])):\n",
    "    sn.lineplot(x, k_agg_list[i][:100], label=f\"{i+1} household(s)\", legend=0)\n",
    "plt.title(\"Aggregate Capital\")\n",
    "\n",
    "plt.tight_layout()\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "plt.legend(handles, labels, loc=\"lower right\", prop={\"size\": 6})\n",
    "plt.savefig(OUTPUT_PATH_FIGURES + \"SimAgg_econ_\" + exp_names[-1] + \".png\")\n",
    "plt.clf\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" Step 7: Simulate the Plocy Iteration model and get statistics \"\"\"\n",
    "y_agg_list = [[] for i in n_agents_list]\n",
    "s_agg_list = [[] for i in n_agents_list]\n",
    "c_agg_list = [[] for i in n_agents_list]\n",
    "k_agg_list = [[] for i in n_agents_list]\n",
    "k_max_list = [[] for i in n_agents_list]\n",
    "k_min_list = [[] for i in n_agents_list]\n",
    "s_max_list = [[] for i in n_agents_list]\n",
    "s_min_list = [[] for i in n_agents_list]\n",
    "shock_agg_list = [[] for i in n_agents_list]\n",
    "p_list = [[] for i in n_agents_list]\n",
    "\n",
    "for ind, n_hh in enumerate([1, 2, 3]):\n",
    "    \"\"\"Step 7.0: replicate original environemnt and config\"\"\"\n",
    "    env_horizon = 1000\n",
    "    n_hh = n_hh\n",
    "    n_capital = 1\n",
    "    beta = 0.98\n",
    "    env_config_simul = {\n",
    "        \"horizon\": 1000,\n",
    "        \"n_hh\": n_hh,\n",
    "        \"n_capital\": n_capital,\n",
    "        \"eval_mode\": False,\n",
    "        \"simul_mode\": True,\n",
    "        \"analysis_mode\": False,\n",
    "        \"max_savings\": 0.6,\n",
    "        \"bgt_penalty\": 1,\n",
    "        \"shock_idtc_values\": [0.9, 1.1],\n",
    "        \"shock_idtc_transition\": [[0.9, 0.1], [0.1, 0.9]],\n",
    "        \"shock_agg_values\": [0.8, 1.2],\n",
    "        \"shock_agg_transition\": [[0.95, 0.05], [0.05, 0.95]],\n",
    "        \"parameters\": {\"delta\": 0.04, \"alpha\": 0.3, \"phi\": 0.5, \"beta\": beta},\n",
    "    }\n",
    "\n",
    "    # We instantiate the environment to extract information.\n",
    "    env = CapitalMarket(env_config_simul)\n",
    "    config_analysis = {\n",
    "        \"gamma\": beta,\n",
    "        \"env\": env_label,\n",
    "        \"env_config\": env_config_simul,\n",
    "        \"horizon\": env_horizon,\n",
    "        \"explore\": False,\n",
    "        \"framework\": \"torch\",\n",
    "        \"multiagent\": {\n",
    "            \"policies\": {\n",
    "                \"hh\": (\n",
    "                    None,\n",
    "                    env.observation_space[\"hh_0\"],\n",
    "                    env.action_space[\"hh_0\"],\n",
    "                    {},\n",
    "                ),\n",
    "            },\n",
    "            \"policy_mapping_fn\": (lambda agent_id: agent_id.split(\"_\")[0]),\n",
    "            \"replay_mode\": \"independent\",\n",
    "        },\n",
    "    }\n",
    "    \"\"\" Step 7.1: restore trainer \"\"\"\n",
    "\n",
    "    dir_model = f\"cap_market_{n_hh}hh_11pts\"\n",
    "    matlab_struct = sio.loadmat(dir_policy_folder + dir_model, simplify_cells=True)\n",
    "    if n_hh == 1:\n",
    "        K_grid = [\n",
    "            np.array(matlab_struct[\"IterRslt\"][\"var_state\"][f\"K\"]) for i in range(n_hh)\n",
    "        ]\n",
    "    else:\n",
    "        K_grid = [\n",
    "            np.array(matlab_struct[\"IterRslt\"][\"var_state\"][f\"K_{i+1}\"])\n",
    "            for i in range(n_hh)\n",
    "        ]\n",
    "    shock_grid = np.array([i for i in range(matlab_struct[\"IterRslt\"][\"shock_num\"])])\n",
    "    if n_hh == 1:\n",
    "        s_on_grid = [matlab_struct[\"IterRslt\"][\"var_policy\"][\"s\"] for i in range(n_hh)]\n",
    "    else:\n",
    "        s_on_grid = [\n",
    "            matlab_struct[\"IterRslt\"][\"var_policy\"][f\"s_{i+1}\"] for i in range(n_hh)\n",
    "        ]\n",
    "\n",
    "    s_interp = [\n",
    "        RegularGridInterpolator((shock_grid,) + tuple(K_grid), s_on_grid[i])\n",
    "        for i in range(n_hh)\n",
    "    ]\n",
    "\n",
    "    def compute_action(obs, policy_list: list, max_action: float, K_grid: list):\n",
    "        K = [min(max(obs[0][i], min(K_grid[i])), max(K_grid[i])) for i in range(n_hh)]\n",
    "        shock_raw = [obs[2]] + list(obs[1])\n",
    "        shock_id = encode(shock_raw, dims=[2 for i in range(env.n_hh + 1)])\n",
    "        s = [policy_list[i](np.array([shock_id] + K)) for i in range(env.n_hh)]\n",
    "        action = np.array([2 * s[i] / max_action - 1 for i in range(env.n_hh)])\n",
    "        return action\n",
    "\n",
    "    \"\"\" Simulate an episode (SIMUL_PERIODS timesteps) \"\"\"\n",
    "    shock_idtc_list = [[] for i in range(env.n_hh)]\n",
    "    y_list = [[] for i in range(env.n_hh)]\n",
    "    s_list = [[] for i in range(env.n_hh)]\n",
    "    c_list = [[] for i in range(env.n_hh)]\n",
    "    k_list = [[] for i in range(env.n_hh)]\n",
    "\n",
    "    # loop\n",
    "    obs = env.reset()\n",
    "    for t in range(SIMUL_PERIODS):\n",
    "        action = {}\n",
    "        if t % 1000 == 0:\n",
    "            obs = env.reset()\n",
    "        for i in range(env.n_hh):\n",
    "            action[f\"hh_{i}\"] = compute_action(\n",
    "                obs[\"hh_0\"], s_interp, env.max_s_ij, K_grid\n",
    "            )[i]\n",
    "\n",
    "        obs, rew, done, info = env.step(action)\n",
    "        for i in range(env.n_hh):\n",
    "            shock_idtc_list[i].append(obs[\"hh_0\"][1][i])\n",
    "            y_list[i].append(info[\"hh_0\"][\"income\"][i])\n",
    "            s_list[i].append(info[\"hh_0\"][\"savings\"][i][0])\n",
    "            c_list[i].append(info[\"hh_0\"][\"consumption\"][i])\n",
    "            k_list[i].append(info[\"hh_0\"][\"capital\"][i][0])\n",
    "\n",
    "        # k_agg_list.append(np.sum([k_list[[j][t-1] for j in range(env_loop.n_hh)]))\n",
    "        shock_agg_list[ind].append(obs[\"hh_0\"][2])\n",
    "        y_agg_list[ind].append(np.sum([y_list[i][t] for i in range(env.n_hh)]))\n",
    "        s_agg_list[ind].append(\n",
    "            np.sum([s_list[i][t] * y_list[i][t] for i in range(env.n_hh)])\n",
    "            / y_agg_list[ind][t]\n",
    "        )\n",
    "        c_agg_list[ind].append(np.sum([y_list[i][t] for i in range(env.n_hh)]))\n",
    "        k_agg_list[ind].append(np.sum([k_list[i][t] for i in range(env.n_hh)]))\n",
    "        k_max_list[ind].append(np.max([k_list[i][t] for i in range(env.n_hh)]))\n",
    "        k_min_list[ind].append(np.min([k_list[i][t] for i in range(env.n_hh)]))\n",
    "        s_max_list[ind].append(np.max([s_list[i][t] for i in range(env.n_hh)]))\n",
    "        s_min_list[ind].append(np.min([s_list[i][t] for i in range(env.n_hh)]))\n",
    "        p_list[ind].append(info[\"hh_0\"][\"price\"][0])\n",
    "\n",
    "    exp_data_simul_econ_dict[\"n_hh\"].append(n_hh)\n",
    "    exp_data_simul_econ_dict[\"Mean Agg. K\"].append(np.mean(k_agg_list[ind]))\n",
    "    exp_data_simul_econ_dict[\"S.D. Agg. K\"].append(np.std(k_agg_list[ind]))\n",
    "    exp_data_simul_econ_dict[\"Max K\"].append(np.max(k_max_list[ind]))\n",
    "    exp_data_simul_econ_dict[\"Min K\"].append(np.min(k_min_list[ind]))\n",
    "    exp_data_simul_econ_dict[\"Mean Agg. s\"].append(np.mean(s_agg_list[ind]))\n",
    "    exp_data_simul_econ_dict[\"S.D. Agg. s\"].append(np.std(s_agg_list[ind]))\n",
    "    exp_data_simul_econ_dict[\"Max s\"].append(np.max(s_max_list[ind]))\n",
    "    exp_data_simul_econ_dict[\"Min s\"].append(np.min(s_min_list[ind]))\n",
    "    exp_data_simul_econ_dict[\"Mean Price\"].append(np.mean(p_list[ind]))\n",
    "    exp_data_simul_econ_dict[\"S.D. Price\"].append(np.std(p_list[ind]))\n",
    "\n",
    "print(exp_data_simul_econ_dict)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" Step 8 and final: Table with moments \"\"\"\n",
    "exp_table_df = pd.DataFrame.from_dict(exp_data_simul_dict)\n",
    "with open(OUTPUT_PATH_TABLES + exp_names[-1] + \"_BIG_TABLE.tex\", \"w\") as tf:\n",
    "    tf.write(exp_table_df.to_latex())\n",
    "\n",
    "exp_table_small_df = exp_table_df[\n",
    "    [\n",
    "        \"n_hh\",\n",
    "        \"max rewards\",\n",
    "        \"time to peak\",\n",
    "        \"Mean Agg. K\",\n",
    "        \"S.D. Agg. K\",\n",
    "        \"Mean Price\",\n",
    "        \"S.D. Price\",\n",
    "    ]\n",
    "]\n",
    "with open(OUTPUT_PATH_TABLES + exp_names[-1] + \"_SMALL_TABLE.tex\", \"w\") as tf:\n",
    "    tf.write(exp_table_small_df.to_latex())\n",
    "\n",
    "exp_table_econ_df = pd.DataFrame.from_dict(exp_data_simul_econ_dict)\n",
    "with open(OUTPUT_PATH_TABLES + exp_names[-1] + \"_econ_BIG_TABLE.tex\", \"w\") as tf:\n",
    "    tf.write(exp_table_econ_df.to_latex())\n",
    "\n",
    "exp_table_small_econ_df = exp_table_df[\n",
    "    [\n",
    "        \"n_hh\",\n",
    "        \"max rewards\",\n",
    "        \"time to peak\",\n",
    "        \"Mean Agg. K\",\n",
    "        \"S.D. Agg. K\",\n",
    "        \"Mean Price\",\n",
    "        \"S.D. Price\",\n",
    "    ]\n",
    "]\n",
    "with open(OUTPUT_PATH_TABLES + exp_names[-1] + \"_econ_SMALL_TABLE.tex\", \"w\") as tf:\n",
    "    tf.write(exp_table_small_econ_df.to_latex())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "shutdown()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.7 64-bit ('marketsai-reVLCGV_-py3.8': poetry)"
  },
  "interpreter": {
   "hash": "1c99579f0a861f1ade1e1abc4784a15ca138f424327e789e2dba1116d0806699"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}