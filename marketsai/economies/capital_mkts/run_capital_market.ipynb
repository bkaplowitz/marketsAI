{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# import environment\n",
    "from marketsai.economies.capital_mkts.capital_market import CapitalMarket\n",
    "\n",
    "# import ray\n",
    "from ray import tune, shutdown, init\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "# from ray.tune.integration.mlflow import MLflowLoggerCallback\n",
    "\n",
    "# For custom metrics (Callbacks)\n",
    "from ray.rllib.agents.callbacks import DefaultCallbacks\n",
    "from ray.rllib.env import BaseEnv\n",
    "from ray.rllib.evaluation import MultiAgentEpisode, RolloutWorker\n",
    "from ray.rllib.policy import Policy\n",
    "\n",
    "# common imports\n",
    "from typing import Dict\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "# import logging\n",
    "# import random\n",
    "# import math\n",
    "\n",
    "\"\"\" STEP 0: Experiment configs \"\"\"\n",
    "\n",
    "# global configs\n",
    "DATE = \"Aug28_\"\n",
    "TEST = False\n",
    "SAVE_EXP_INFO = True\n",
    "PLOT_PROGRESS = True\n",
    "sn.color_palette(\"Set2\")\n",
    "SAVE_PROGRESS_CSV = True\n",
    "\n",
    "if TEST:\n",
    "    OUTPUT_PATH_EXPERS = \"/Users/matiascovarrubias/Dropbox/RL_macro/Tests/\"\n",
    "    OUTPUT_PATH_FIGURES = \"/Users/matiascovarrubias/Dropbox/RL_macro/Tests/\"\n",
    "else:\n",
    "    OUTPUT_PATH_EXPERS = \"/Users/matiascovarrubias/Dropbox/RL_macro/Experiments/\"\n",
    "    OUTPUT_PATH_FIGURES = \"/Users/matiascovarrubias/Dropbox/RL_macro/Documents/Figures/\"\n",
    "\n",
    "ALGO = \"PPO\"  # either PPO\" or \"SAC\"\n",
    "DEVICE = \"native\"  # either \"native\" or \"server\"\n",
    "N_HH_LIST = [1, 2, 3, 4, 5]  # number of agents more generally\n",
    "ITERS_TEST = 2\n",
    "ITERS_RUN = 300\n",
    "# Define environment, which should be imported from a class\n",
    "ENV_LABEL = \"cap_market\"\n",
    "register_env(ENV_LABEL, CapitalMarket)\n",
    "\n",
    "# Other economic Hiperparameteres.\n",
    "ENV_HORIZON = 1000\n",
    "N_CAPITAL = 1\n",
    "BETA = 0.98  # discount parameter"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" STEP 1: Paralleliztion and batch options\"\"\"\n",
    "# Parallelization options\n",
    "NUM_CPUS = 12\n",
    "NUM_CPUS_DRIVER = 1\n",
    "NUM_TRIALS = 2\n",
    "NUM_ROLLOUT = ENV_HORIZON * 1\n",
    "NUM_ENV_PW = 1  # num_env_per_worker\n",
    "NUM_GPUS = 0\n",
    "BATCH_ROLLOUT = 1\n",
    "NUM_MINI_BATCH = NUM_CPUS_DRIVER\n",
    "\n",
    "N_WORKERS = (NUM_CPUS - NUM_TRIALS * NUM_CPUS_DRIVER) // NUM_TRIALS\n",
    "BATCH_SIZE = NUM_ROLLOUT * (max(N_WORKERS, 1)) * NUM_ENV_PW * BATCH_ROLLOUT\n",
    "\n",
    "print(N_WORKERS, BATCH_SIZE)\n",
    "\n",
    "# define length of experiment (MAX_STEPS) and experiment name\n",
    "if TEST == True:\n",
    "    MAX_STEPS = ITERS_TEST * BATCH_SIZE\n",
    "else:\n",
    "    MAX_STEPS = ITERS_RUN * BATCH_SIZE\n",
    "\n",
    "CHKPT_FREQ = 2\n",
    "\n",
    "stop = {\"timesteps_total\": MAX_STEPS}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" STEP 2: set custom metrics such as discounted rewards to keep track of through leraning\"\"\"\n",
    "# Define custom metrics using the Callbacks class\n",
    "# See rllib documentation on Callbacks. They are a way of inserting code in different parts of the pipeline.\n",
    "\n",
    "# function to get discounted rewards for analysys\n",
    "def process_rewards(r):\n",
    "    \"\"\"Compute discounted reward from a vector of rewards.\"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, len(r))):\n",
    "        running_add = running_add * BETA + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r[0]\n",
    "\n",
    "\n",
    "class MyCallbacks(DefaultCallbacks):\n",
    "    def on_episode_start(\n",
    "        self,\n",
    "        *,\n",
    "        worker: RolloutWorker,\n",
    "        base_env: BaseEnv,\n",
    "        policies: Dict[str, Policy],\n",
    "        episode: MultiAgentEpisode,\n",
    "        env_index: int,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # Make sure this episode has just been started (only initial obs\n",
    "        # logged so far).\n",
    "\n",
    "        assert episode.length == 0, (\n",
    "            \"ERROR: `on_episode_start()` callback should be called right \"\n",
    "            \"after env reset!\"\n",
    "        )\n",
    "        episode.user_data[\"rewards\"] = []\n",
    "        # episode.user_data[\"bgt_penalty\"] = []\n",
    "\n",
    "    def on_episode_step(\n",
    "        self,\n",
    "        *,\n",
    "        worker: RolloutWorker,\n",
    "        base_env: BaseEnv,\n",
    "        episode: MultiAgentEpisode,\n",
    "        env_index: int,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        if episode.length > 1:  # at t=0, previous rewards are not defined\n",
    "            rewards = episode.prev_reward_for(\"hh_0\")\n",
    "            # bgt_penalty = episode.last_info_for(\"hh_0\")[\"bgt_penalty\"]\n",
    "            episode.user_data[\"rewards\"].append(rewards)\n",
    "            # episode.user_data[\"bgt_penalty\"].append(bgt_penalty)\n",
    "\n",
    "    def on_episode_end(\n",
    "        self,\n",
    "        *,\n",
    "        worker: RolloutWorker,\n",
    "        base_env: BaseEnv,\n",
    "        policies: Dict[str, Policy],\n",
    "        episode: MultiAgentEpisode,\n",
    "        env_index: int,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        discounted_rewards = process_rewards(episode.user_data[\"rewards\"])\n",
    "        episode.custom_metrics[\"discounted_rewards\"] = discounted_rewards\n",
    "        # episode.custom_metrics[\"bgt_penalty\"] = np.mean(\n",
    "        #    episode.user_data[\"bgt_penalty\"][0]\n",
    "        # )\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" STEP 3: Environment and Algorithm configuration \"\"\"\n",
    "\n",
    "\n",
    "# environment config including evaluation environment (without exploration)\n",
    "env_config = {\n",
    "    \"horizon\": 1000,\n",
    "    \"n_capital\": N_CAPITAL,\n",
    "    \"eval_mode\": False,\n",
    "    \"max_savings\": 0.6,\n",
    "    \"bgt_penalty\": 1,\n",
    "    \"shock_idtc_values\": [0.9, 1.1],\n",
    "    \"shock_idtc_transition\": [[0.9, 0.1], [0.1, 0.9]],\n",
    "    \"shock_agg_values\": [0.8, 1.2],\n",
    "    \"shock_agg_transition\": [[0.95, 0.05], [0.05, 0.95]],\n",
    "    \"parameters\": {\"delta\": 0.04, \"alpha\": 0.3, \"phi\": 0.5, \"beta\": BETA},\n",
    "}\n",
    "\n",
    "env_config_eval = env_config.copy()\n",
    "env_config_eval[\"eval_mode\"] = True\n",
    "\n",
    "# we instantiate the environment to extrac relevant info\n",
    "env = CapitalMarket(env_config)\n",
    "\n",
    "# common configuration\n",
    "\n",
    "\"\"\"\n",
    "NOTE: in order to do hyperparameter optimization, you can select a range of values \n",
    "with tune.choice([0.05,1] for random choice or tune.grid_search([0.05,1]) for fix search.\n",
    "# see https://docs.ray.io/en/master/tune/key-concepts.html#search-spaces for spaces and their definition.\n",
    "# se at the bottom (Annex_env_hyp) for an explanation how to do the same with environment parameters.\n",
    "\"\"\"\n",
    "common_config = {\n",
    "    # CUSTOM METRICS\n",
    "    \"callbacks\": MyCallbacks,\n",
    "    # ENVIRONMENT\n",
    "    \"gamma\": BETA,\n",
    "    \"env\": ENV_LABEL,\n",
    "    \"env_config\": env_config,\n",
    "    \"horizon\": ENV_HORIZON,\n",
    "    # MODEL\n",
    "    \"framework\": \"torch\",\n",
    "    # \"model\": tune.grid_search([{\"use_lstm\": True}, {\"use_lstm\": False}]),\n",
    "    # TRAINING CONFIG\n",
    "    \"num_workers\": N_WORKERS,\n",
    "    \"create_env_on_driver\": False,\n",
    "    \"num_gpus\": NUM_GPUS / NUM_TRIALS,\n",
    "    \"num_envs_per_worker\": NUM_ENV_PW,\n",
    "    \"num_cpus_for_driver\": NUM_CPUS_DRIVER,\n",
    "    \"rollout_fragment_length\": NUM_ROLLOUT,\n",
    "    \"train_batch_size\": BATCH_SIZE,\n",
    "    # EVALUATION\n",
    "    \"evaluation_interval\": 1,\n",
    "    \"evaluation_num_episodes\": 1,\n",
    "    \"evaluation_config\": {\n",
    "        \"explore\": False,\n",
    "        \"env_config\": env_config_eval,\n",
    "    },\n",
    "    # MULTIAGENT,\n",
    "    \"multiagent\": {\n",
    "        \"policies\": {\n",
    "            \"hh\": (\n",
    "                None,\n",
    "                env.observation_space[\"hh_0\"],\n",
    "                env.action_space[\"hh_0\"],\n",
    "                {},\n",
    "            ),\n",
    "        },\n",
    "        \"policy_mapping_fn\": (lambda agent_id: agent_id.split(\"_\")[0]),\n",
    "        \"replay_mode\": \"independent\",  # you can change to \"lockstep\".\n",
    "    },\n",
    "}\n",
    "\n",
    "# Configs specific to the chosel algorithms, INCLUDING THE LEARNING RATE\n",
    "ppo_config = {\n",
    "    \"lr\": 0.0005,\n",
    "    # \"lr_schedule\": [[0, 0.00005], [MAX_STEPS * 1 / 2, 0.00001]],\n",
    "    \"sgd_minibatch_size\": BATCH_SIZE // NUM_MINI_BATCH,\n",
    "    \"num_sgd_iter\": 1,\n",
    "    \"batch_mode\": \"complete_episodes\",\n",
    "    \"lambda\": 0.98,\n",
    "    \"entropy_coeff\": 0,\n",
    "    \"kl_coeff\": 0.2,\n",
    "    # \"vf_loss_coeff\": 0.5,\n",
    "    # \"vf_clip_param\": tune.choice([5, 10, 20]),\n",
    "    # \"entropy_coeff_schedule\": [[0, 0.01], [5120 * 1000, 0]],\n",
    "    \"clip_param\": 0.2,\n",
    "    \"clip_actions\": True,\n",
    "}\n",
    "\n",
    "sac_config = {\n",
    "    \"prioritized_replay\": True,\n",
    "}\n",
    "\n",
    "if ALGO == \"PPO\":\n",
    "    training_config = {**common_config, **ppo_config}\n",
    "elif ALGO == \"SAC\":\n",
    "    training_config = {**common_config, **sac_config}\n",
    "else:\n",
    "    training_config = common_config"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" STEP 4: run experiment \"\"\"\n",
    "\n",
    "exp_names = []\n",
    "exp_dirs = []\n",
    "checkpoints = []\n",
    "best_rewards = []\n",
    "best_configs = []\n",
    "learning_dta = []\n",
    "\n",
    "# Initialize ray\n",
    "shutdown()\n",
    "init(\n",
    "    num_cpus=NUM_CPUS,\n",
    "    num_gpus=NUM_GPUS,\n",
    "    # logging_level=logging.ERROR,\n",
    ")\n",
    "\n",
    "# RUN TRAINER\n",
    "for n_hh in N_HH_LIST:\n",
    "    EXP_LABEL = DEVICE + f\"_{n_hh}hh_\"\n",
    "    if TEST == True:\n",
    "        EXP_NAME = EXP_LABEL + ENV_LABEL + \"_test_\" + DATE + ALGO\n",
    "    else:\n",
    "        EXP_NAME = EXP_LABEL + ENV_LABEL + \"_run_\" + DATE + ALGO\n",
    "\n",
    "    env_config[\"n_hh\"] = n_hh\n",
    "    env_config_eval[\"n_hh\"] = n_hh\n",
    "    env = CapitalMarket(env_config)\n",
    "    training_config[\"env_config\"] = env_config\n",
    "    training_config[\"evaluation_config\"][\"env_config\"] = env_config_eval\n",
    "    training_config[\"multiagent\"] = {\n",
    "        \"policies\": {\n",
    "            \"hh\": (\n",
    "                None,\n",
    "                env.observation_space[\"hh_0\"],\n",
    "                env.action_space[\"hh_0\"],\n",
    "                {},\n",
    "            ),\n",
    "        },\n",
    "        \"policy_mapping_fn\": (lambda agent_id: agent_id.split(\"_\")[0]),\n",
    "        \"replay_mode\": \"independent\",  # you can change to \"lockstep\".\n",
    "    }\n",
    "\n",
    "    analysis = tune.run(\n",
    "        ALGO,\n",
    "        name=EXP_NAME,\n",
    "        config=training_config,\n",
    "        stop=stop,\n",
    "        checkpoint_freq=CHKPT_FREQ,\n",
    "        checkpoint_at_end=True,\n",
    "        metric=\"evaluation/custom_metrics/discounted_rewards_mean\",\n",
    "        mode=\"max\",\n",
    "        num_samples=2*NUM_TRIALS,\n",
    "        # resources_per_trial={\"gpu\": 0.5},\n",
    "    )\n",
    "\n",
    "    exp_names.append(EXP_NAME)\n",
    "    checkpoints.append(analysis.best_checkpoint)\n",
    "    best_rewards.append(\n",
    "        analysis.best_result[\"evaluation\"][\"custom_metrics\"][\"discounted_rewards_mean\"]\n",
    "    )\n",
    "    best_configs.append(analysis.best_config)\n",
    "    exp_dirs.append(analysis.best_logdir)\n",
    "    learning_dta.append(\n",
    "        analysis.best_dataframe[\n",
    "            [\"episodes_total\", \"evaluation/custom_metrics/discounted_rewards_mean\"]\n",
    "        ]\n",
    "    )\n",
    "    learning_dta[n_hh - 1].columns = [\"episodes_total\", f\"{n_hh} households\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" STEP 5 (optional): Organize and Plot \"\"\"\n",
    "\n",
    "# global experiment name\n",
    "if len(exp_names) > 1:\n",
    "    EXP_LABEL = DEVICE + f\"_multi_hh_\"\n",
    "    if TEST == True:\n",
    "        EXP_NAME = EXP_LABEL + ENV_LABEL + \"_test_\" + DATE + ALGO\n",
    "    else:\n",
    "        EXP_NAME = EXP_LABEL + ENV_LABEL + \"_run_\" + DATE + ALGO\n",
    "\n",
    "\n",
    "# create CSV with information on each experiment\n",
    "if SAVE_EXP_INFO:\n",
    "    progress_csv_dirs = [exp_dirs[i] + \"/progress.csv\" for i in range(len(exp_dirs))]\n",
    "\n",
    "    # Create CSV with economy level\n",
    "    exp_dict = {\n",
    "        \"n_agents\": N_HH_LIST,\n",
    "        \"exp_names\": exp_names,\n",
    "        \"exp_dirs\": exp_dirs,\n",
    "        \"progress_csv_dirs\": progress_csv_dirs,\n",
    "        \"best_rewards\": best_rewards,\n",
    "        \"checkpoints\": checkpoints,\n",
    "        # \"best_config\": best_configs,\n",
    "    }\n",
    "    # for i in range(len(exp_dict.values())):\n",
    "    #     print(type(exp_dict.values()[i]))\n",
    "    print(\n",
    "        \"exp_names =\",\n",
    "        exp_names,\n",
    "        \"\\n\" \"exp_dirs =\",\n",
    "        exp_dirs,\n",
    "        \"\\n\" \"progress_csv_dirs =\",\n",
    "        progress_csv_dirs,\n",
    "        \"\\n\" \"best_rewards =\",\n",
    "        best_rewards,\n",
    "        \"\\n\" \"checkpoints =\",\n",
    "        checkpoints,\n",
    "        # \"\\n\" \"best_config =\",\n",
    "        # best_configs,\n",
    "    )\n",
    "\n",
    "    with open(OUTPUT_PATH_EXPERS + \"expINFO_\" + EXP_NAME + \".json\", \"w+\") as f:\n",
    "        json.dump(exp_dict, f)\n",
    "\n",
    "    # exp_df = pd.DataFrame(exp_dict)\n",
    "    # exp_df.to_csv(OUTPUT_PATH_EXPERS + \"exp_info\" + EXP_NAME + \".csv\")\n",
    "    print(OUTPUT_PATH_EXPERS + \"expINFO_\" + EXP_NAME + \".json\")\n",
    "\n",
    "# Plot and save progress\n",
    "if PLOT_PROGRESS:\n",
    "    for i in range(len(exp_names)):\n",
    "        learning_plot = sn.lineplot(\n",
    "            data=learning_dta[i],\n",
    "            y=f\"{i+1} households\",\n",
    "            x=\"episodes_total\",\n",
    "        )\n",
    "    learning_plot = learning_plot.get_figure()\n",
    "    plt.ylabel(\"Discounted utility\")\n",
    "    plt.xlabel(\"Timesteps (thousands)\")\n",
    "    plt.legend(labels=[f\"{i+1} households\" for i in range(len(learning_dta))])\n",
    "    learning_plot.savefig(OUTPUT_PATH_FIGURES + \"progress_\" + EXP_NAME + \".png\")\n",
    "\n",
    "# Save progress as CSV\n",
    "if SAVE_PROGRESS_CSV:\n",
    "    # merge data\n",
    "    learning_dta = [df.set_index(\"episodes_total\") for df in learning_dta]\n",
    "    learning_dta_merged = pd.concat(learning_dta, axis=1)\n",
    "    learning_dta_merged.to_csv(OUTPUT_PATH_EXPERS + \"progress_\" + EXP_NAME + \".csv\")\n",
    "\n",
    "\"\"\" Annex_env_hyp: For Environment hyperparameter tuning\"\"\"\n",
    "\n",
    "# # We create a list that contain the main config + altered copies.\n",
    "env_configs = [env_config]\n",
    "for i in range(1, 15):\n",
    "    env_configs.append(env_config.copy())\n",
    "    env_configs[i][\"parameteres\"] = (\n",
    "        {\n",
    "            \"depreciation\": np.random.choice([0.02, 0.04, 0.06, 0.08]),\n",
    "            \"alpha\": 0.3,\n",
    "            \"phi\": 0.3,\n",
    "            \"beta\": 0.98,\n",
    "        },\n",
    "    )\n",
    "    env_configs[i][\"bgt_penalty\"] = np.random.choice([1, 5, 10, 50])"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.7 64-bit ('marketsai-reVLCGV_-py3.8': poetry)"
  },
  "interpreter": {
   "hash": "1c99579f0a861f1ade1e1abc4784a15ca138f424327e789e2dba1116d0806699"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}