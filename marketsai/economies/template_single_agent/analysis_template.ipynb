{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Imports\n",
    "from marketsai.economies.template_single_agent.env_template_sa import TemplateSA\n",
    "import scipy.io as sio\n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "from marketsai.utils import encode\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import csv\n",
    "import json\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.tune.registry import register_env\n",
    "from ray import shutdown, init\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "\"\"\" GLOBAL CONFIGS \"\"\"\n",
    "# Script Options\n",
    "FOR_PUBLIC = True  # for publication\n",
    "SAVE_CSV = False  # save learning CSV\n",
    "PLOT_PROGRESS = True  # create plot with progress\n",
    "SIMUL_PERIODS = 5000\n",
    "# Input Directories\n",
    "# Rl experiment\n",
    "\"\"\" CHANGE HERE \"\"\"\n",
    "INPUT_PATH_EXPERS = \"/Users/matiascovarrubias/Dropbox/RL_macro/Experiments/expINFO_native_template_sa_Sep19_PPO_run.json\"\n",
    "# GDSGE policy\n",
    "dir_policy_folder = (\n",
    "    \"/Users/matiascovarrubias/Dropbox/RL_macro/Econ_algos/TemplateSA/Results/\"\n",
    ")\n",
    "\n",
    "# Output Directories\n",
    "if FOR_PUBLIC:\n",
    "    OUTPUT_PATH_EXPERS = \"/Users/matiascovarrubias/Dropbox/RL_macro/Experiments/\"\n",
    "    OUTPUT_PATH_FIGURES = \"/Users/matiascovarrubias/Dropbox/RL_macro/Documents/Figures/\"\n",
    "    OUTPUT_PATH_TABLES = \"/Users/matiascovarrubias/Dropbox/RL_macro/Documents/Tables/\"\n",
    "else:\n",
    "    OUTPUT_PATH_EXPERS = \"/Users/matiascovarrubias/Dropbox/RL_macro/Experiments/ALL/\"\n",
    "    OUTPUT_PATH_FIGURES = (\n",
    "        \"/Users/matiascovarrubias/Dropbox/RL_macro/Documents/Figures/ALL/\"\n",
    "    )\n",
    "    OUTPUT_PATH_TABLES = \"/Users/matiascovarrubias/Dropbox/RL_macro/Documents/Tables/ALL/\"\n",
    "    \n",
    "\n",
    "# Plot options\n",
    "sn.color_palette(\"Set2\")\n",
    "sn.set_style(\"ticks\")  # grid styling, \"dark\"\n",
    "# plt.figure(figure=(8, 4))\n",
    "# choose between \"paper\", \"talk\" or \"poster\"\n",
    "sn.set_context(\n",
    "    \"paper\",\n",
    "    font_scale=1.4,\n",
    ")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "\"\"\" Step 0: import experiment data and initalize empty output data \"\"\"\n",
    "with open(INPUT_PATH_EXPERS) as f:\n",
    "    exp_data_dict = json.load(f)\n",
    "print(exp_data_dict)\n",
    "\n",
    "# UNPACK USEFUL DATA\n",
    "exp_names = exp_data_dict[\"exp_names\"]\n",
    "checkpoints_dirs = exp_data_dict[\"checkpoints\"]\n",
    "progress_csv_dirs = exp_data_dict[\"progress_csv_dirs\"]\n",
    "#best_rewards = exp_data_dict[\"best_rewards\"]\n",
    "\n",
    "\n",
    "#Create output directory\n",
    "exp_data_analysis_dict =  {\"n_firms\": [], \"max rewards\": [] , \"time to peak\": [], \"Mean Agg. K\": [], \"S.D. Agg. K\": [], \"Mean Avge. K\":[], \"S.D. Avge. K\": [], \"S.D. Agg. K\": [], \"Max K\":[], \"Min K\": [], \n",
    "    \"Discounted Rewards\":[], \"Mean Price\": [], \"S.D. Price\": [], \"Max Price\": [], \"Min Price\": [], \"Discounted Rewards\": [], \"Mean Agg. s\": [], \"S.D. Agg. s\": [], \"Max s\":[], \"Min s\":[]}\n",
    "exp_data_analysis_econ_dict =  {\"n_firms\": [], \"max rewards\": [] , \"time to peak\": [], \"Mean Agg. K\": [], \"S.D. Agg. K\": [], \"Mean Avge. K\":[], \"S.D. Avge. K\": [], \"S.D. Agg. K\": [], \"Max K\":[], \"Min K\": [], \n",
    "    \"Discounted Rewards\":[], \"Mean Price\": [], \"S.D. Price\": [], \"Max Price\": [], \"Min Price\": [], \"Discounted Rewards\": [], \"Mean Agg. s\": [], \"S.D. Agg. s\": [], \"Max s\":[], \"Min s\":[]}\n",
    "exp_data_simul_dict =  {\"n_firms\": [], \"max rewards\": [] , \"time to peak\": [], \"Mean Agg. K\": [], \"S.D. Agg. K\": [], \"Mean Avge. K\":[], \"S.D. Avge. K\": [], \"S.D. Agg. K\": [], \"Max K\":[], \"Min K\": [], \n",
    "    \"Discounted Rewards\":[], \"Mean Price\": [], \"S.D. Price\": [], \"Max Price\": [], \"Min Price\": [], \"Discounted Rewards\": [], \"Mean Agg. s\": [], \"S.D. Agg. s\": [], \"Max s\":[], \"Min s\":[]}\n",
    "exp_data_simul_econ_dict =  {\"n_firms\": [], \"max rewards\": [] , \"time to peak\": [], \"Mean Agg. K\": [], \"S.D. Agg. K\": [], \"Mean Avge. K\":[], \"S.D. Avge. K\": [], \"S.D. Agg. K\": [], \"Max K\":[], \"Min K\": [], \n",
    "    \"Discounted Rewards\":[], \"Mean Price\": [], \"S.D. Price\": [], \"Max Price\": [], \"Min Price\": [], \"Discounted Rewards\": [], \"Mean Agg. s\": [], \"S.D. Agg. s\": [], \"Max s\":[], \"Min s\":[]}\n",
    "# init ray\n",
    "shutdown()\n",
    "init()\n",
    "\n",
    "#useful functions\n",
    "def process_rewards(r, BETA):\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, len(r))):\n",
    "        running_add = running_add * BETA + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r[0]\n",
    "\n",
    "#register environment\n",
    "env_label = \"template_sa\"\n",
    "register_env(env_label, TemplateSA)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'exp_names': ['native_template_sa_Sep19_PPO_run'], 'exp_dirs': ['/Users/jasonli/ray_results/native_template_sa_Sep19_PPO_run/PPO_template_sa_86c1b_00000_0_2021-09-20_09-27-39'], 'progress_csv_dirs': ['/Users/jasonli/ray_results/native_template_sa_Sep19_PPO_run/PPO_template_sa_86c1b_00000_0_2021-09-20_09-27-39/progress.csv'], 'best_rewards': [59.57886537930697], 'checkpoints': ['/Users/jasonli/ray_results/native_template_sa_Sep19_PPO_run/PPO_template_sa_86c1b_00000_0_2021-09-20_09-27-39/checkpoint_000800/checkpoint-800']}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-20 19:05:45,348\tINFO services.py:1267 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "\"\"\" Step 1: Plot progress during learning run \"\"\"\n",
    "\n",
    "if PLOT_PROGRESS == True:\n",
    "    #Big plot\n",
    "\n",
    "    data_progress_df = pd.read_csv(progress_csv_dirs[0])\n",
    "    max_rewards = abs(data_progress_df[\n",
    "        \"evaluation/custom_metrics/discounted_rewards_mean\"\n",
    "    ].max())\n",
    "    print()\n",
    "    exp_data_simul_dict[\"max rewards\"].append(max_rewards)\n",
    "    exp_data_simul_dict[\"time to peak\"].append(0)\n",
    "    exp_data_analysis_dict[\"max rewards\"].append(max_rewards)\n",
    "    exp_data_analysis_dict[\"time to peak\"].append(0)\n",
    "    data_progress_df[\"evaluation/custom_metrics/discounted_rewards_mean\"] = data_progress_df[\"evaluation/custom_metrics/discounted_rewards_mean\"] / max_rewards + 2\n",
    "    \n",
    "    learning_plot_big = sn.lineplot(\n",
    "        data=data_progress_df,\n",
    "        y=\"evaluation/custom_metrics/discounted_rewards_mean\",\n",
    "        x=\"episodes_total\",\n",
    "    )\n",
    "\n",
    "\n",
    "    learning_plot_big = learning_plot_big.get_figure()\n",
    "    plt.ylabel(\"Discounted utility\")\n",
    "    plt.xlabel(\"Timesteps (thousands)\")\n",
    "    plt.xlim([0, 6000])\n",
    "    learning_plot_big.savefig(OUTPUT_PATH_FIGURES + \"progress_BIG_\" + exp_names[-1] + \".png\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # small plot\n",
    "    for i in range(len(exp_names)):\n",
    "        data_progress_df = pd.read_csv(progress_csv_dirs)\n",
    "        max_rewards = data_progress_df[\n",
    "            \"evaluation/custom_metrics/discounted_rewards_mean\"\n",
    "        ].max()\n",
    "        data_progress_df[\"evaluation/custom_metrics/discounted_rewards_mean\"] = (\n",
    "            data_progress_df[\"evaluation/custom_metrics/discounted_rewards_mean\"]\n",
    "            / abs(max_rewards) +2\n",
    "        )\n",
    "        learning_plot_small = sn.lineplot(\n",
    "            data=data_progress_df,\n",
    "            y=\"evaluation/custom_metrics/discounted_rewards_mean\",\n",
    "            x=\"episodes_total\",\n",
    "        )\n",
    "\n",
    "\n",
    "    learning_plot_small = learning_plot_small.get_figure()\n",
    "    plt.ylabel(\"Discounted utility\")\n",
    "    plt.xlabel(\"Timesteps (thousands)\")\n",
    "    plt.xlim([0, 200])\n",
    "    learning_plot_small.savefig(OUTPUT_PATH_FIGURES + \"progress_SMALL_\" + exp_names[-1] + \".png\")\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/jasonli/ray_results/native_template_sa_Sep19_PPO_run/PPO_template_sa_86c1b_00000_0_2021-09-20_09-27-39/progress.csv'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6b0fe4a6b235>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#Big plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdata_progress_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprogress_csv_dirs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     max_rewards = abs(data_progress_df[\n\u001b[1;32m      8\u001b[0m         \u001b[0;34m\"evaluation/custom_metrics/discounted_rewards_mean\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \"\"\"\n\u001b[0;32m-> 1362\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/jasonli/ray_results/native_template_sa_Sep19_PPO_run/PPO_template_sa_86c1b_00000_0_2021-09-20_09-27-39/progress.csv'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" Step 2: Congif env, Restore RL policy and then simualte analysis trajectory \"\"\"\n",
    "\n",
    "\n",
    "\"\"\" Step 2.0: replicate original environemnt and config \"\"\"\n",
    "env_horizon = 200\n",
    "beta = 0.98\n",
    "# environment config\n",
    "env_config_analysis = {\n",
    "    \"horizon\": 200,\n",
    "    \"eval_mode\": False,\n",
    "    \"analysis_mode\": True,\n",
    "    \"simul_mode\": False,\n",
    "    \"max_action\": 0.5,\n",
    "    \"rew_mean\": 0,\n",
    "    \"rew_std\": 1,\n",
    "    \"parameters\": {\n",
    "        \"alpha\": 0.5,\n",
    "        \"delta\": 0.04,\n",
    "        \"beta\": 0.99,\n",
    "    },\n",
    "}\n",
    "\n",
    "# We instantiate the environment to extract information.\n",
    "\"\"\" CHANGE HERE \"\"\"\n",
    "env = TemplateSA(env_config_analysis)\n",
    "config_algo = {\n",
    "    \"gamma\": beta,\n",
    "    \"env\": env_label,\n",
    "    \"env_config\": env_config_analysis,\n",
    "    \"horizon\": env_horizon,\n",
    "    \"explore\": False,\n",
    "    \"framework\": \"torch\",\n",
    "}\n",
    "\"\"\" Step 2.1: restore trainer \"\"\"\n",
    "\n",
    "# restore the trainer\n",
    "trained_trainer = PPOTrainer(env=env_label, config=config_algo)\n",
    "trained_trainer.restore(checkpoints_dirs)\n",
    "\n",
    "\"\"\" Step 2: Simulate an episode (MAX_steps timesteps) \"\"\"\n",
    "#shock_idtc_list = [[] for i in range(env.n_firms)]\n",
    "y_list = []\n",
    "s_list = []\n",
    "k_list = []\n",
    "rew_list = []\n",
    "\n",
    "# loop\n",
    "obs = env.reset()\n",
    "for t in range(env_horizon):\n",
    "    action = trained_trainer.compute_action(\n",
    "        obs[f\"firm_{i}\"], policy_id=\"firm\"\n",
    "    )\n",
    "\n",
    "    obs, rew, done, info = env.step(action)\n",
    "    y_list.append(info[\"income\"])\n",
    "    s_list.append(info[\"savings\"])\n",
    "    k_list.append(info[\"capital\"])\n",
    "    rew_list.append(info[\"rewards\"])\n",
    "\n",
    "rew_disc.append(np.mean(process_rewards(rew_list,0.98)))\n",
    "\n",
    "\n",
    "\"\"\" Step 2.2: Plot analysis trajectories \"\"\"\n",
    "\n",
    "# Idiosyncratic trajectories\n",
    "x = [i for i in range(100)]\n",
    "plt.subplot(2, 2, 1)\n",
    "sn.lineplot(x,y_list[:100])\n",
    "plt.title(\"Income\")\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "sn.lineplot(x,s_list[:100])\n",
    "plt.title(\"Savings Rate\")\n",
    "plt.subplot(2, 2, 3)\n",
    "\n",
    "sn.lineplot(x,y_list[:100])\n",
    "plt.title(\"Income\")\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "sn.lineplot(x,k_list[:100])\n",
    "plt.title(\"Capital\")\n",
    "\n",
    "plt.tight_layout()\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "plt.legend(handles, labels, loc='lower right', prop={'size': 6})\n",
    "#plt.legend(labels=[f\"{i+1} firms\" for i in range(env.n_firms)], loc='upper center', bbox_to_anchor=(0.5, 1.05))\n",
    "plt.savefig(OUTPUT_PATH_FIGURES + \"SimInds_\" + exp_names + \".png\")\n",
    "plt.show()\n",
    "plt.close()\n",
    " \n",
    "print(exp_data_analysis_dict)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" Step 4: Simulate the RL policy for SIMUL_PERIODS and get statistics \"\"\"\n",
    "\n",
    "\n",
    "\"\"\" Step 4.0: replicate original environemnt and config \"\"\"\n",
    "env_horizon = 200\n",
    "beta = 0.98\n",
    "env_config_simul = {\n",
    "    \"horizon\": 200,\n",
    "    \"eval_mode\": False,\n",
    "    \"analysis_mode\": False,\n",
    "    \"simul_mode\": True,\n",
    "    \"max_action\": 0.5,\n",
    "    \"rew_mean\": 0,\n",
    "    \"rew_std\": 1,\n",
    "    \"parameters\": {\n",
    "        \"alpha\": 0.5,\n",
    "        \"delta\": 0.04,\n",
    "        \"beta\": 0.99,\n",
    "    },\n",
    "}\n",
    "\n",
    "# We instantiate the environment to extract information.\n",
    "\"\"\" CHANGE HERE \"\"\"\n",
    "env = TemplateSA(env_config_simul)\n",
    "config_analysis = {\n",
    "    \"gamma\": beta,\n",
    "    \"env\": env_label,\n",
    "    \"env_config\": env_config_simul,\n",
    "    \"horizon\": env_horizon,\n",
    "    \"explore\": False,\n",
    "    \"framework\": \"torch\",\n",
    "}\n",
    "\"\"\" Step 4.1: restore trainer \"\"\"\n",
    "\n",
    "# restore the trainer\n",
    "trained_trainer = PPOTrainer(env=env_label, config=config_analysis)\n",
    "trained_trainer.restore(checkpoints_dirs)\n",
    "\n",
    "\"\"\" Simulate an episode (SIMUL_PERIODS timesteps) \"\"\"\n",
    "\n",
    "y_list = []\n",
    "s_list = []\n",
    "k_list = []\n",
    "rew_list = []\n",
    "\n",
    "# loop\n",
    "obs = env.reset()\n",
    "for t in range(SIMUL_PERIODS):\n",
    "    action = {}\n",
    "    if t%env.horizon == 0:\n",
    "        obs=env.reset()\n",
    "    for i in range(env.n_agents):\n",
    "        action[f\"firm_{i}\"] = trained_trainer.compute_action(\n",
    "            obs[f\"firm_{i}\"], policy_id=\"firm\"\n",
    "        )\n",
    "\n",
    "    obs, rew, done, info = env.step(action)\n",
    "    for i in range(env.n_agents):\n",
    "        y_list.append(info[\"income\"])\n",
    "        s_list.append(info[\"savings\"])\n",
    "        k_list.append(info[\"capital\"])\n",
    "        rew_list.append(info[\"rewards\"])\n",
    "\n",
    "rew_disc=process_rewards(rew_list,0.98)\n",
    "\n",
    "\n",
    "\n",
    " "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" Step 5: config env, Restore PI (GDSGE) policy and simulate analysis trajectory \"\"\"\n",
    "\n",
    "# replicate environment\n",
    "env_horizon = 1000\n",
    "n_capital = 1\n",
    "beta = 0.98\n",
    "env_config_analysis = {\n",
    "    \"horizon\": 200,\n",
    "    \"eval_mode\": False,\n",
    "    \"analysis_mode\": True,\n",
    "    \"simul_mode\": False,\n",
    "    \"max_action\": 0.5,\n",
    "    \"rew_mean\": 0,\n",
    "    \"rew_std\": 1,\n",
    "    \"parameters\": {\n",
    "        \"alpha\": 0.5,\n",
    "        \"delta\": 0.04,\n",
    "        \"beta\": 0.99,\n",
    "    },\n",
    "}\n",
    "# We instantiate the environment to extract information.\n",
    "\"\"\" CHANGE HERE \"\"\"\n",
    "env = TemplateSA(env_config_analysis)\n",
    "config_analysis = {\n",
    "    \"gamma\": beta,\n",
    "    \"env\": env_label,\n",
    "    \"env_config\": env_config_analysis,\n",
    "    \"horizon\": env_horizon,\n",
    "    \"explore\": False,\n",
    "    \"framework\": \"torch\",\n",
    "}\n",
    "\"\"\" Step 5.1: import matlab struct \"\"\"\n",
    "\"\"\" CHANGE HERE \"\"\"\n",
    "dir_model = f\"rbc_savings_11pts\"\n",
    "matlab_struct = sio.loadmat(dir_policy_folder + dir_model, simplify_cells=True)\n",
    "exp_data_analysis_econ_dict[\"time to peak\"].append(\n",
    "    matlab_struct[\"IterRslt\"][\"timeElapsed\"]\n",
    ")\n",
    "exp_data_simul_econ_dict[\"time to peak\"].append(\n",
    "    matlab_struct[\"IterRslt\"][\"timeElapsed\"]\n",
    ")\n",
    "\n",
    "K_grid = [\n",
    "    np.array(matlab_struct[\"IterRslt\"][\"var_state\"][\"K\"]) \n",
    "]\n",
    "\n",
    "shock_grid = np.array([i for i in range(matlab_struct[\"IterRslt\"][\"shock_num\"])])\n",
    "s_on_grid = [matlab_struct[\"IterRslt\"][\"var_policy\"][\"s\"]]\n",
    "\n",
    "\n",
    "s_interp = [RegularGridInterpolator((shock_grid,) + tuple(K_grid), s_on_grid)]\n",
    "\n",
    "def compute_action(obs, policy_list: list, max_action: float):\n",
    "    K = obs[0]\n",
    "    shock_raw = [obs[2]] + list(obs[1])\n",
    "    shock_id = encode(shock_raw, dims=[2])\n",
    "    s = [policy_list(np.array([shock_id] + K)) for i in range(env.n_firms)]\n",
    "    action = np.array([2 * s / max_action - 1 for i in range(env.n_firms)])\n",
    "    return action\n",
    "\n",
    "\"\"\" Step 5.2: Simulate an episode (MAX_steps timesteps) \"\"\"\n",
    "shock_idtc_list = [[] for i in range(env.n_firms)]\n",
    "y_list = [[] for i in range(env.n_firms)]\n",
    "s_list = [[] for i in range(env.n_firms)]\n",
    "c_list = [[] for i in range(env.n_firms)]\n",
    "k_list = [[] for i in range(env.n_firms)]\n",
    "\n",
    "# loop\n",
    "obs = env.reset()\n",
    "for t in range(env_horizon):\n",
    "    action = compute_action(obs, s_interp, env.max_action)\n",
    "\n",
    "    obs, rew, done, info = env.step(action)\n",
    "    shock_idtc_list.append(obs[1])\n",
    "    y_list.append(info[\"income\"])\n",
    "    s_list.append(info[\"savings\"])\n",
    "    c_list.append(info[\"consumption\"])\n",
    "    k_list.append(info[\"capital\"])\n",
    "\n",
    "    # k_agg_list.append(np.sum([k_list[[j][t-1] for j in range(env_loop.n_firms)]))\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" Step 5.4: Plot individual trajectories \"\"\"\n",
    "\n",
    "# Idiosyncratic trajectories\n",
    "x = [i for i in range(100)]\n",
    "plt.subplot(2, 2, 1)\n",
    "sn.lineplot(x, y_list[:100], legend=0)\n",
    "plt.title(\"Shock\")\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "sn.lineplot(x, s_list[:100], legend=0)\n",
    "plt.title(\"Savings Rate\")\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "sn.lineplot(x, y_list[:100], legend=0)\n",
    "plt.title(\"Income\")\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "sn.lineplot(x, k_list[:100], legend=0)\n",
    "plt.title(\"Capital\")\n",
    "\n",
    "plt.tight_layout()\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "plt.legend(handles, labels, loc=\"lower right\", prop={\"size\": 6})\n",
    "# plt.legend(labels=[f\"{i+1} firms\" for i in range(env.n_firms)], loc='upper center', bbox_to_anchor=(0.5, 1.05))\n",
    "plt.savefig(OUTPUT_PATH_FIGURES + \"SimInd_\" + exp_names + \".png\")\n",
    "plt.show()\n",
    "plt.close()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" Step 7: Simulate the Plocy Iteration model and get statistics \"\"\"\n",
    "\n",
    "\"\"\" Step 7.0: replicate original environemnt and config \"\"\"\n",
    "env_horizon = 1000\n",
    "\n",
    "n_capital = 1\n",
    "beta = 0.98\n",
    "env_config_analysis = {\n",
    "    \"horizon\": 200,\n",
    "    \"eval_mode\": False,\n",
    "    \"analysis_mode\": False,\n",
    "    \"simul_mode\": True,\n",
    "    \"max_action\": 0.5,\n",
    "    \"rew_mean\": 0,\n",
    "    \"rew_std\": 1,\n",
    "    \"parameters\": {\n",
    "        \"alpha\": 0.5,\n",
    "        \"delta\": 0.04,\n",
    "        \"beta\": 0.99,\n",
    "    },\n",
    "}\n",
    "\n",
    "# We instantiate the environment to extract information.\n",
    "env = TemplateSA(env_config_simul)\n",
    "config_analysis = {\n",
    "    \"gamma\": beta,\n",
    "    \"env\": env_label,\n",
    "    \"env_config\": env_config_simul,\n",
    "    \"horizon\": env_horizon,\n",
    "    \"explore\": False,\n",
    "    \"framework\": \"torch\",\n",
    "}\n",
    "\"\"\" Step 7.1: restore trainer \"\"\"\n",
    "\n",
    "dir_model = \"rbc_savings_firm_11pts\"\n",
    "matlab_struct = sio.loadmat(dir_policy_folder + dir_model, simplify_cells=True)\n",
    "\n",
    "K_grid = [np.array(matlab_struct[\"IterRslt\"][\"var_state\"][f\"K\"])]\n",
    "\n",
    "shock_grid = np.array([i for i in range(matlab_struct[\"IterRslt\"][\"shock_num\"])])\n",
    "\n",
    "s_on_grid = [matlab_struct[\"IterRslt\"][\"var_policy\"][\"s\"]]\n",
    "\n",
    "s_interp = [RegularGridInterpolator((shock_grid,) + tuple(K_grid), s_on_grid)]\n",
    "\n",
    "def compute_action(obs, policy_list: list, max_action: float, K_grid: list):\n",
    "    K = [min(max(obs[0],min(K_grid)), max(K_grid))]\n",
    "    shock_raw = [obs[2]] + list(obs[1])\n",
    "    shock_id = encode(shock_raw, dims=[2])\n",
    "    s = [policy_list(np.array([shock_id] + K)) for i in range(env.n_firms)]\n",
    "    action = np.array([2 * s / max_action - 1 for i in range(env.n_firms)])\n",
    "    return action\n",
    "\n",
    "\"\"\" Simulate an episode (SIMUL_PERIODS timesteps) \"\"\"\n",
    "shock_idtc_list = [[] for i in range(env.n_firms)]\n",
    "y_list = [[] for i in range(env.n_firms)]\n",
    "s_list = [[] for i in range(env.n_firms)]\n",
    "c_list = [[] for i in range(env.n_firms)]\n",
    "k_list = [[] for i in range(env.n_firms)]\n",
    "\n",
    "# loop\n",
    "obs = env.reset()\n",
    "for t in range(SIMUL_PERIODS):\n",
    "\n",
    "    if t%1000 == 0:\n",
    "        obs=env.reset()\n",
    "    action = compute_action(obs, s_interp, env.max_s_ij, K_grid)\n",
    "    obs, rew, done, info = env.step(action)\n",
    "    #shock_idtc_list.append(obs[1])\n",
    "    y_list.append(info[\"income\"])\n",
    "    s_list.append(info[\"savings\"])\n",
    "    c_list.append(info[\"consumption\"])\n",
    "    k_list.append(info[\"capital\"])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" Step 8 and final: Table with moments \"\"\"\n",
    "\n",
    "exp_table_df = pd.DataFrame.from_dict(exp_data_simul_dict)\n",
    "with open(OUTPUT_PATH_TABLES + exp_names[-1]+'_BIG_TABLE.tex','w') as tf:\n",
    "    tf.write(exp_table_df.to_latex())\n",
    "\n",
    "exp_table_small_df=exp_table_df[[\"n_firms\",\"max rewards\", \"time to peak\", \"Mean Agg. K\", \"S.D. Agg. K\", \"Mean Price\", \"S.D. Price\"]]\n",
    "with open(OUTPUT_PATH_TABLES + exp_names[-1]+'_SMALL_TABLE.tex','w') as tf:\n",
    "    tf.write(exp_table_small_df.to_latex())\n",
    "\n",
    "# exp_table_econ_df = pd.DataFrame.from_dict(exp_data_simul_econ_dict)\n",
    "# with open(OUTPUT_PATH_TABLES + exp_names[-1]+'_econ_BIG_TABLE.tex','w') as tf:\n",
    "#     tf.write(exp_table_econ_df.to_latex())\n",
    "\n",
    "# exp_table_small_econ_df=exp_table_df[[\"n_firms\",\"max rewards\", \"time to peak\", \"Mean Agg. K\", \"S.D. Agg. K\", \"Mean Price\", \"S.D. Price\"]]\n",
    "# with open(OUTPUT_PATH_TABLES + exp_names[-1]+'_econ_SMALL_TABLE.tex','w') as tf:\n",
    "#     tf.write(exp_table_small_econ_df.to_latex())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "shutdown()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.7 64-bit ('marketsai-reVLCGV_-py3.8': poetry)"
  },
  "interpreter": {
   "hash": "1c99579f0a861f1ade1e1abc4784a15ca138f424327e789e2dba1116d0806699"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}