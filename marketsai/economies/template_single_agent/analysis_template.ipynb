{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Imports\n",
    "from marketsai.economies.template_single_agent.env_template_sa import TemplateSA\n",
    "import scipy.io as sio\n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "from marketsai.utils import encode\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import csv\n",
    "import json\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.tune.registry import register_env\n",
    "from ray import shutdown, init\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" GLOBAL CONFIGS \"\"\"\n",
    "# Script Options\n",
    "FOR_PUBLIC = True  # for publication\n",
    "SAVE_CSV = False  # save learning CSV\n",
    "PLOT_PROGRESS = True  # create plot with progress\n",
    "SIMUL_PERIODS = 5000\n",
    "# Input Directories\n",
    "# Rl experiment\n",
    "\"\"\" CHANGE HERE \"\"\"\n",
    "# INPUT_PATH_EXPERS = \"/Users/matiascovarrubias/Dropbox/RL_macro/Experiments/expINFO_native__multi_firm_TemplateSA_run_Sep6_PPO.json\"\n",
    "INPUT_PATH_EXPERS = \"/Users/matiascovarrubias/Dropbox/RL_macro/Experiments/expINFO_native__multi_firm_TemplateSA_run_Sep7_PPO_mod.json\"\n",
    "# GDSGE policy\n",
    "dir_policy_folder = (\n",
    "    \"/Users/matiascovarrubias/Dropbox/RL_macro/Econ_algos/TemplateSA/Results/\"\n",
    ")\n",
    "\n",
    "# Output Directories\n",
    "if FOR_PUBLIC:\n",
    "    OUTPUT_PATH_EXPERS = \"/Users/matiascovarrubias/Dropbox/RL_macro/Experiments/\"\n",
    "    OUTPUT_PATH_FIGURES = \"/Users/matiascovarrubias/Dropbox/RL_macro/Documents/Figures/\"\n",
    "    OUTPUT_PATH_TABLES = \"/Users/matiascovarrubias/Dropbox/RL_macro/Documents/Tables/\"\n",
    "else:\n",
    "    OUTPUT_PATH_EXPERS = \"/Users/matiascovarrubias/Dropbox/RL_macro/Experiments/ALL/\"\n",
    "    OUTPUT_PATH_FIGURES = (\n",
    "        \"/Users/matiascovarrubias/Dropbox/RL_macro/Documents/Figures/ALL/\"\n",
    "    )\n",
    "    OUTPUT_PATH_TABLES = \"/Users/matiascovarrubias/Dropbox/RL_macro/Documents/Tables/ALL/\"\n",
    "    \n",
    "\n",
    "# Plot options\n",
    "sn.color_palette(\"Set2\")\n",
    "sn.set_style(\"ticks\")  # grid styling, \"dark\"\n",
    "# plt.figure(figure=(8, 4))\n",
    "# choose between \"paper\", \"talk\" or \"poster\"\n",
    "sn.set_context(\n",
    "    \"paper\",\n",
    "    font_scale=1.4,\n",
    ")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" Step 0: import experiment data and initalize empty output data \"\"\"\n",
    "with open(INPUT_PATH_EXPERS) as f:\n",
    "    exp_data_dict = json.load(f)\n",
    "print(exp_data_dict)\n",
    "\n",
    "# UNPACK USEFUL DATA\n",
    "n_agents_list = exp_data_dict[\"n_agents\"]\n",
    "exp_names = exp_data_dict[\"exp_names\"]\n",
    "checkpoints_dirs = exp_data_dict[\"checkpoints\"]\n",
    "progress_csv_dirs = exp_data_dict[\"progress_csv_dirs\"]\n",
    "#best_rewards = exp_data_dict[\"best_rewards\"]\n",
    "\n",
    "\n",
    "#Create output directory\n",
    "exp_data_analysis_dict =  {\"n_firms\": [], \"max rewards\": [] , \"time to peak\": [], \"Mean Agg. K\": [], \"S.D. Agg. K\": [], \"Mean Avge. K\":[], \"S.D. Avge. K\": [], \"S.D. Agg. K\": [], \"Max K\":[], \"Min K\": [], \n",
    "    \"Discounted Rewards\":[], \"Mean Price\": [], \"S.D. Price\": [], \"Max Price\": [], \"Min Price\": [], \"Discounted Rewards\": [], \"Mean Agg. s\": [], \"S.D. Agg. s\": [], \"Max s\":[], \"Min s\":[]}\n",
    "exp_data_analysis_econ_dict =  {\"n_firms\": [], \"max rewards\": [] , \"time to peak\": [], \"Mean Agg. K\": [], \"S.D. Agg. K\": [], \"Mean Avge. K\":[], \"S.D. Avge. K\": [], \"S.D. Agg. K\": [], \"Max K\":[], \"Min K\": [], \n",
    "    \"Discounted Rewards\":[], \"Mean Price\": [], \"S.D. Price\": [], \"Max Price\": [], \"Min Price\": [], \"Discounted Rewards\": [], \"Mean Agg. s\": [], \"S.D. Agg. s\": [], \"Max s\":[], \"Min s\":[]}\n",
    "exp_data_simul_dict =  {\"n_firms\": [], \"max rewards\": [] , \"time to peak\": [], \"Mean Agg. K\": [], \"S.D. Agg. K\": [], \"Mean Avge. K\":[], \"S.D. Avge. K\": [], \"S.D. Agg. K\": [], \"Max K\":[], \"Min K\": [], \n",
    "    \"Discounted Rewards\":[], \"Mean Price\": [], \"S.D. Price\": [], \"Max Price\": [], \"Min Price\": [], \"Discounted Rewards\": [], \"Mean Agg. s\": [], \"S.D. Agg. s\": [], \"Max s\":[], \"Min s\":[]}\n",
    "exp_data_simul_econ_dict =  {\"n_firms\": [], \"max rewards\": [] , \"time to peak\": [], \"Mean Agg. K\": [], \"S.D. Agg. K\": [], \"Mean Avge. K\":[], \"S.D. Avge. K\": [], \"S.D. Agg. K\": [], \"Max K\":[], \"Min K\": [], \n",
    "    \"Discounted Rewards\":[], \"Mean Price\": [], \"S.D. Price\": [], \"Max Price\": [], \"Min Price\": [], \"Discounted Rewards\": [], \"Mean Agg. s\": [], \"S.D. Agg. s\": [], \"Max s\":[], \"Min s\":[]}\n",
    "# init ray\n",
    "shutdown()\n",
    "init()\n",
    "\n",
    "#useful functions\n",
    "def process_rewards(r, BETA):\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, len(r))):\n",
    "        running_add = running_add * BETA + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r[0]\n",
    "\n",
    "#register environment\n",
    "env_label = \"template_sa\"\n",
    "register_env(env_label, TemplateSA)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" Step 1: Plot progress during learning run \"\"\"\n",
    "\n",
    "if PLOT_PROGRESS == True:\n",
    "    #Big plot\n",
    "\n",
    "    data_progress_df = pd.read_csv(progress_csv_dirs[0])\n",
    "    max_rewards = abs(data_progress_df[\n",
    "        \"evaluation/custom_metrics/discounted_rewards_mean\"\n",
    "    ].max())\n",
    "    print()\n",
    "    exp_data_simul_dict[\"max rewards\"].append(max_rewards)\n",
    "    exp_data_simul_dict[\"time to peak\"].append(0)\n",
    "    exp_data_analysis_dict[\"max rewards\"].append(max_rewards)\n",
    "    exp_data_analysis_dict[\"time to peak\"].append(0)\n",
    "    data_progress_df[\"evaluation/custom_metrics/discounted_rewards_mean\"] = data_progress_df[\"evaluation/custom_metrics/discounted_rewards_mean\"] / max_rewards + 2\n",
    "    \n",
    "    learning_plot_big = sn.lineplot(\n",
    "        data=data_progress_df,\n",
    "        y=\"evaluation/custom_metrics/discounted_rewards_mean\",\n",
    "        x=\"episodes_total\",\n",
    "    )\n",
    "\n",
    "\n",
    "    learning_plot_big = learning_plot_big.get_figure()\n",
    "    plt.ylabel(\"Discounted utility\")\n",
    "    plt.xlabel(\"Timesteps (thousands)\")\n",
    "    plt.xlim([0, 6000])\n",
    "    plt.legend(labels=[f\"{n} firm(s)\" for n in n_agents_list])\n",
    "    learning_plot_big.savefig(OUTPUT_PATH_FIGURES + \"progress_BIG_\" + exp_names[-1] + \".png\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # small plot\n",
    "    for i in range(len(exp_names)):\n",
    "        data_progress_df = pd.read_csv(progress_csv_dirs)\n",
    "        max_rewards = data_progress_df[\n",
    "            \"evaluation/custom_metrics/discounted_rewards_mean\"\n",
    "        ].max()\n",
    "        data_progress_df[\"evaluation/custom_metrics/discounted_rewards_mean\"] = (\n",
    "            data_progress_df[\"evaluation/custom_metrics/discounted_rewards_mean\"]\n",
    "            / abs(max_rewards) +2\n",
    "        )\n",
    "        learning_plot_small = sn.lineplot(\n",
    "            data=data_progress_df,\n",
    "            y=\"evaluation/custom_metrics/discounted_rewards_mean\",\n",
    "            x=\"episodes_total\",\n",
    "        )\n",
    "\n",
    "\n",
    "    learning_plot_small = learning_plot_small.get_figure()\n",
    "    plt.ylabel(\"Discounted utility\")\n",
    "    plt.xlabel(\"Timesteps (thousands)\")\n",
    "    plt.xlim([0, 200])\n",
    "    learning_plot_small.savefig(OUTPUT_PATH_FIGURES + \"progress_SMALL_\" + exp_names[-1] + \".png\")\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" Step 2: Congif env, Restore RL policy and then simualte analysis trajectory \"\"\"\n",
    "\n",
    "\n",
    "\"\"\" Step 2.0: replicate original environemnt and config \"\"\"\n",
    "env_horizon = 200\n",
    "beta = 0.98\n",
    "# environment config\n",
    "env_config_analysis = {\n",
    "    \"horizon\": 200,\n",
    "    \"eval_mode\": False,\n",
    "    \"analysis_mode\": True,\n",
    "    \"simul_mode\": False,\n",
    "    \"max_action\": 0.5,\n",
    "    \"rew_mean\": 0,\n",
    "    \"rew_std\": 1,\n",
    "    \"parameters\": {\n",
    "        \"alpha\": 0.5,\n",
    "        \"delta\": 0.04,\n",
    "        \"beta\": 0.99,\n",
    "    },\n",
    "}\n",
    "\n",
    "# We instantiate the environment to extract information.\n",
    "\"\"\" CHANGE HERE \"\"\"\n",
    "env = TemplateSA(env_config_analysis)\n",
    "config_algo = {\n",
    "    \"gamma\": beta,\n",
    "    \"env\": env_label,\n",
    "    \"env_config\": env_config_analysis,\n",
    "    \"horizon\": env_horizon,\n",
    "    \"explore\": False,\n",
    "    \"framework\": \"torch\",\n",
    "}\n",
    "\"\"\" Step 2.1: restore trainer \"\"\"\n",
    "\n",
    "# restore the trainer\n",
    "trained_trainer = PPOTrainer(env=env_label, config=config_algo)\n",
    "trained_trainer.restore(checkpoints_dirs)\n",
    "\n",
    "\"\"\" Step 2: Simulate an episode (MAX_steps timesteps) \"\"\"\n",
    "#shock_idtc_list = [[] for i in range(env.n_firms)]\n",
    "y_list = []\n",
    "s_list = []\n",
    "k_list = []\n",
    "rew_list = []\n",
    "\n",
    "# loop\n",
    "obs = env.reset()\n",
    "for t in range(env_horizon):\n",
    "    action = trained_trainer.compute_action(\n",
    "        obs[f\"firm_{i}\"], policy_id=\"firm\"\n",
    "    )\n",
    "\n",
    "    obs, rew, done, info = env.step(action)\n",
    "    y_list.append(info[\"income\"])\n",
    "    s_list.append(info[\"savings\"])\n",
    "    k_list.append(info[\"capital\"])\n",
    "    rew_list.append(info[\"rewards\"])\n",
    "\n",
    "rew_disc.append(np.mean(process_rewards(rew_list,0.98)))\n",
    "\n",
    "\n",
    "\"\"\" Step 2.2: Plot analysis trajectories \"\"\"\n",
    "\n",
    "# Idiosyncratic trajectories\n",
    "x = [i for i in range(100)]\n",
    "plt.subplot(2, 2, 1)\n",
    "sn.lineplot(x,y_list[:100])\n",
    "plt.title(\"Income\")\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "sn.lineplot(x,s_list[:100])\n",
    "plt.title(\"Savings Rate\")\n",
    "plt.subplot(2, 2, 3)\n",
    "\n",
    "sn.lineplot(x,y_list[:100])\n",
    "plt.title(\"Income\")\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "sn.lineplot(x,k_list[:100])\n",
    "plt.title(\"Capital\")\n",
    "\n",
    "plt.tight_layout()\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "plt.legend(handles, labels, loc='lower right', prop={'size': 6})\n",
    "#plt.legend(labels=[f\"{i+1} firms\" for i in range(env.n_firms)], loc='upper center', bbox_to_anchor=(0.5, 1.05))\n",
    "plt.savefig(OUTPUT_PATH_FIGURES + \"SimInds_\" + exp_names + \".png\")\n",
    "plt.show()\n",
    "plt.close()\n",
    " \n",
    "print(exp_data_analysis_dict)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" Step 4: Simulate the RL policy for SIMUL_PERIODS and get statistics \"\"\"\n",
    "\n",
    "\n",
    "\"\"\" Step 4.0: replicate original environemnt and config \"\"\"\n",
    "env_horizon = 200\n",
    "beta = 0.98\n",
    "env_config_simul = {\n",
    "    \"horizon\": 200,\n",
    "    \"eval_mode\": False,\n",
    "    \"analysis_mode\": False,\n",
    "    \"simul_mode\": True,\n",
    "    \"max_action\": 0.5,\n",
    "    \"rew_mean\": 0,\n",
    "    \"rew_std\": 1,\n",
    "    \"parameters\": {\n",
    "        \"alpha\": 0.5,\n",
    "        \"delta\": 0.04,\n",
    "        \"beta\": 0.99,\n",
    "    },\n",
    "}\n",
    "\n",
    "# We instantiate the environment to extract information.\n",
    "\"\"\" CHANGE HERE \"\"\"\n",
    "env = TemplateSA(env_config_simul)\n",
    "config_analysis = {\n",
    "    \"gamma\": beta,\n",
    "    \"env\": env_label,\n",
    "    \"env_config\": env_config_simul,\n",
    "    \"horizon\": env_horizon,\n",
    "    \"explore\": False,\n",
    "    \"framework\": \"torch\",\n",
    "}\n",
    "\"\"\" Step 4.1: restore trainer \"\"\"\n",
    "\n",
    "# restore the trainer\n",
    "trained_trainer = PPOTrainer(env=env_label, config=config_analysis)\n",
    "trained_trainer.restore(checkpoints_dirs)\n",
    "\n",
    "\"\"\" Simulate an episode (SIMUL_PERIODS timesteps) \"\"\"\n",
    "\n",
    "y_list = []\n",
    "s_list = []\n",
    "k_list = []\n",
    "rew_list = []\n",
    "\n",
    "# loop\n",
    "obs = env.reset()\n",
    "for t in range(SIMUL_PERIODS):\n",
    "    action = {}\n",
    "    if t%env.horizon == 0:\n",
    "        obs=env.reset()\n",
    "    for i in range(env.n_agents):\n",
    "        action[f\"firm_{i}\"] = trained_trainer.compute_action(\n",
    "            obs[f\"firm_{i}\"], policy_id=\"firm\"\n",
    "        )\n",
    "\n",
    "    obs, rew, done, info = env.step(action)\n",
    "    for i in range(env.n_agents):\n",
    "        y_list.append(info[\"income\"])\n",
    "        s_list.append(info[\"savings\"])\n",
    "        k_list.append(info[\"capital\"])\n",
    "        rew_list.append(info[\"rewards\"])\n",
    "\n",
    "rew_disc=process_rewards(rew_list,0.98)\n",
    "\n",
    "\n",
    "\n",
    " "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" Step 5: config env, Restore PI (GDSGE) policy and simulate analysis trajectory \"\"\"\n",
    "\n",
    "# replicate environment\n",
    "env_horizon = 1000\n",
    "n_capital = 1\n",
    "beta = 0.98\n",
    "env_config_analysis = {\n",
    "    \"horizon\": 200,\n",
    "    \"eval_mode\": False,\n",
    "    \"analysis_mode\": True,\n",
    "    \"simul_mode\": False,\n",
    "    \"max_action\": 0.5,\n",
    "    \"rew_mean\": 0,\n",
    "    \"rew_std\": 1,\n",
    "    \"parameters\": {\n",
    "        \"alpha\": 0.5,\n",
    "        \"delta\": 0.04,\n",
    "        \"beta\": 0.99,\n",
    "    },\n",
    "}\n",
    "# We instantiate the environment to extract information.\n",
    "\"\"\" CHANGE HERE \"\"\"\n",
    "env = TemplateSA(env_config_analysis)\n",
    "config_analysis = {\n",
    "    \"gamma\": beta,\n",
    "    \"env\": env_label,\n",
    "    \"env_config\": env_config_analysis,\n",
    "    \"horizon\": env_horizon,\n",
    "    \"explore\": False,\n",
    "    \"framework\": \"torch\",\n",
    "}\n",
    "\"\"\" Step 5.1: import matlab struct \"\"\"\n",
    "\"\"\" CHANGE HERE \"\"\"\n",
    "dir_model = f\"rbc_savings_11pts\"\n",
    "matlab_struct = sio.loadmat(dir_policy_folder + dir_model, simplify_cells=True)\n",
    "exp_data_analysis_econ_dict[\"time to peak\"].append(\n",
    "    matlab_struct[\"IterRslt\"][\"timeElapsed\"]\n",
    ")\n",
    "exp_data_simul_econ_dict[\"time to peak\"].append(\n",
    "    matlab_struct[\"IterRslt\"][\"timeElapsed\"]\n",
    ")\n",
    "\n",
    "K_grid = [\n",
    "    np.array(matlab_struct[\"IterRslt\"][\"var_state\"][\"K\"]) \n",
    "]\n",
    "\n",
    "shock_grid = np.array([i for i in range(matlab_struct[\"IterRslt\"][\"shock_num\"])])\n",
    "s_on_grid = [matlab_struct[\"IterRslt\"][\"var_policy\"][\"s\"]]\n",
    "\n",
    "\n",
    "s_interp = [RegularGridInterpolator((shock_grid,) + tuple(K_grid), s_on_grid)]\n",
    "\n",
    "def compute_action(obs, policy_list: list, max_action: float):\n",
    "    K = obs[0]\n",
    "    shock_raw = [obs[2]] + list(obs[1])\n",
    "    shock_id = encode(shock_raw, dims=[2])\n",
    "    s = [policy_list(np.array([shock_id] + K)) for i in range(env.n_firms)]\n",
    "    action = np.array([2 * s / max_action - 1 for i in range(env.n_firms)])\n",
    "    return action\n",
    "\n",
    "\"\"\" Step 5.2: Simulate an episode (MAX_steps timesteps) \"\"\"\n",
    "shock_idtc_list = [[] for i in range(env.n_firms)]\n",
    "y_list = [[] for i in range(env.n_firms)]\n",
    "s_list = [[] for i in range(env.n_firms)]\n",
    "c_list = [[] for i in range(env.n_firms)]\n",
    "k_list = [[] for i in range(env.n_firms)]\n",
    "\n",
    "# loop\n",
    "obs = env.reset()\n",
    "for t in range(env_horizon):\n",
    "    action = compute_action(obs, s_interp, env.max_action)\n",
    "\n",
    "    obs, rew, done, info = env.step(action)\n",
    "    shock_idtc_list.append(obs[1])\n",
    "    y_list.append(info[\"income\"])\n",
    "    s_list.append(info[\"savings\"])\n",
    "    c_list.append(info[\"consumption\"])\n",
    "    k_list.append(info[\"capital\"])\n",
    "\n",
    "    # k_agg_list.append(np.sum([k_list[[j][t-1] for j in range(env_loop.n_firms)]))\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" Step 5.4: Plot individual trajectories \"\"\"\n",
    "\n",
    "# Idiosyncratic trajectories\n",
    "x = [i for i in range(100)]\n",
    "plt.subplot(2, 2, 1)\n",
    "sn.lineplot(x, y_list[:100], legend=0)\n",
    "plt.title(\"Shock\")\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "sn.lineplot(x, s_list[:100], legend=0)\n",
    "plt.title(\"Savings Rate\")\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "sn.lineplot(x, y_list[:100], legend=0)\n",
    "plt.title(\"Income\")\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "sn.lineplot(x, k_list[:100], legend=0)\n",
    "plt.title(\"Capital\")\n",
    "\n",
    "plt.tight_layout()\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "plt.legend(handles, labels, loc=\"lower right\", prop={\"size\": 6})\n",
    "# plt.legend(labels=[f\"{i+1} firms\" for i in range(env.n_firms)], loc='upper center', bbox_to_anchor=(0.5, 1.05))\n",
    "plt.savefig(OUTPUT_PATH_FIGURES + \"SimInd_\" + exp_names + \".png\")\n",
    "plt.show()\n",
    "plt.close()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" Step 7: Simulate the Plocy Iteration model and get statistics \"\"\"\n",
    "\n",
    "\"\"\" Step 7.0: replicate original environemnt and config \"\"\"\n",
    "env_horizon = 1000\n",
    "\n",
    "n_capital = 1\n",
    "beta = 0.98\n",
    "env_config_analysis = {\n",
    "    \"horizon\": 200,\n",
    "    \"eval_mode\": False,\n",
    "    \"analysis_mode\": False,\n",
    "    \"simul_mode\": True,\n",
    "    \"max_action\": 0.5,\n",
    "    \"rew_mean\": 0,\n",
    "    \"rew_std\": 1,\n",
    "    \"parameters\": {\n",
    "        \"alpha\": 0.5,\n",
    "        \"delta\": 0.04,\n",
    "        \"beta\": 0.99,\n",
    "    },\n",
    "}\n",
    "\n",
    "# We instantiate the environment to extract information.\n",
    "env = TemplateSA(env_config_simul)\n",
    "config_analysis = {\n",
    "    \"gamma\": beta,\n",
    "    \"env\": env_label,\n",
    "    \"env_config\": env_config_simul,\n",
    "    \"horizon\": env_horizon,\n",
    "    \"explore\": False,\n",
    "    \"framework\": \"torch\",\n",
    "}\n",
    "\"\"\" Step 7.1: restore trainer \"\"\"\n",
    "\n",
    "dir_model = \"rbc_savings_firm_11pts\"\n",
    "matlab_struct = sio.loadmat(dir_policy_folder + dir_model, simplify_cells=True)\n",
    "\n",
    "K_grid = [np.array(matlab_struct[\"IterRslt\"][\"var_state\"][f\"K\"])]\n",
    "\n",
    "shock_grid = np.array([i for i in range(matlab_struct[\"IterRslt\"][\"shock_num\"])])\n",
    "\n",
    "s_on_grid = [matlab_struct[\"IterRslt\"][\"var_policy\"][\"s\"]]\n",
    "\n",
    "s_interp = [RegularGridInterpolator((shock_grid,) + tuple(K_grid), s_on_grid)]\n",
    "\n",
    "def compute_action(obs, policy_list: list, max_action: float, K_grid: list):\n",
    "    K = [min(max(obs[0],min(K_grid)), max(K_grid))]\n",
    "    shock_raw = [obs[2]] + list(obs[1])\n",
    "    shock_id = encode(shock_raw, dims=[2])\n",
    "    s = [policy_list(np.array([shock_id] + K)) for i in range(env.n_firms)]\n",
    "    action = np.array([2 * s / max_action - 1 for i in range(env.n_firms)])\n",
    "    return action\n",
    "\n",
    "\"\"\" Simulate an episode (SIMUL_PERIODS timesteps) \"\"\"\n",
    "shock_idtc_list = [[] for i in range(env.n_firms)]\n",
    "y_list = [[] for i in range(env.n_firms)]\n",
    "s_list = [[] for i in range(env.n_firms)]\n",
    "c_list = [[] for i in range(env.n_firms)]\n",
    "k_list = [[] for i in range(env.n_firms)]\n",
    "\n",
    "# loop\n",
    "obs = env.reset()\n",
    "for t in range(SIMUL_PERIODS):\n",
    "\n",
    "    if t%1000 == 0:\n",
    "        obs=env.reset()\n",
    "    action = compute_action(obs, s_interp, env.max_s_ij, K_grid)\n",
    "    obs, rew, done, info = env.step(action)\n",
    "    #shock_idtc_list.append(obs[1])\n",
    "    y_list.append(info[\"income\"])\n",
    "    s_list.append(info[\"savings\"])\n",
    "    c_list.append(info[\"consumption\"])\n",
    "    k_list.append(info[\"capital\"])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" Step 8 and final: Table with moments \"\"\"\n",
    "\n",
    "exp_table_df = pd.DataFrame.from_dict(exp_data_simul_dict)\n",
    "with open(OUTPUT_PATH_TABLES + exp_names[-1]+'_BIG_TABLE.tex','w') as tf:\n",
    "    tf.write(exp_table_df.to_latex())\n",
    "\n",
    "exp_table_small_df=exp_table_df[[\"n_firms\",\"max rewards\", \"time to peak\", \"Mean Agg. K\", \"S.D. Agg. K\", \"Mean Price\", \"S.D. Price\"]]\n",
    "with open(OUTPUT_PATH_TABLES + exp_names[-1]+'_SMALL_TABLE.tex','w') as tf:\n",
    "    tf.write(exp_table_small_df.to_latex())\n",
    "\n",
    "# exp_table_econ_df = pd.DataFrame.from_dict(exp_data_simul_econ_dict)\n",
    "# with open(OUTPUT_PATH_TABLES + exp_names[-1]+'_econ_BIG_TABLE.tex','w') as tf:\n",
    "#     tf.write(exp_table_econ_df.to_latex())\n",
    "\n",
    "# exp_table_small_econ_df=exp_table_df[[\"n_firms\",\"max rewards\", \"time to peak\", \"Mean Agg. K\", \"S.D. Agg. K\", \"Mean Price\", \"S.D. Price\"]]\n",
    "# with open(OUTPUT_PATH_TABLES + exp_names[-1]+'_econ_SMALL_TABLE.tex','w') as tf:\n",
    "#     tf.write(exp_table_small_econ_df.to_latex())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "shutdown()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.7 64-bit ('marketsai-reVLCGV_-py3.8': poetry)"
  },
  "interpreter": {
   "hash": "1c99579f0a861f1ade1e1abc4784a15ca138f424327e789e2dba1116d0806699"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}