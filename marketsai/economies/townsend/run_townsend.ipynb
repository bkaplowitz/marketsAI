{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# import environment\n",
    "from marketsai.economies.townsend.townsend import Townsend\n",
    "\n",
    "# import ray\n",
    "from ray import tune, shutdown, init\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "# from ray.tune.integration.mlflow import MLflowLoggerCallback\n",
    "\n",
    "# For custom metrics (Callbacks)\n",
    "from ray.rllib.agents.callbacks import DefaultCallbacks\n",
    "from ray.rllib.env import BaseEnv\n",
    "from ray.rllib.evaluation import MultiAgentEpisode, RolloutWorker\n",
    "from ray.rllib.policy import Policy\n",
    "\n",
    "# common imports\n",
    "from typing import Dict\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "# import logging\n",
    "# import random\n",
    "# import math\n",
    "\n",
    "\"\"\" STEP 0: Experiment configs \"\"\"\n",
    "\n",
    "# global configs\n",
    "DATE = \"Sep3_\"\n",
    "TEST = False\n",
    "SAVE_EXP_INFO = True\n",
    "PLOT_PROGRESS = True\n",
    "sn.color_palette(\"Set2\")\n",
    "SAVE_PROGRESS_CSV = True\n",
    "\n",
    "if TEST:\n",
    "    OUTPUT_PATH_EXPERS = \"/Users/matiascovarrubias/Dropbox/RL_macro/Tests/\"\n",
    "    OUTPUT_PATH_FIGURES = \"/Users/matiascovarrubias/Dropbox/RL_macro/Tests/\"\n",
    "else:\n",
    "    OUTPUT_PATH_EXPERS = \"/Users/matiascovarrubias/Dropbox/RL_macro/Experiments/\"\n",
    "    OUTPUT_PATH_FIGURES = \"/Users/matiascovarrubias/Dropbox/RL_macro/Documents/Figures/\"\n",
    "\n",
    "ALGO = \"PPO\"  # either PPO\" or \"SAC\"\n",
    "DEVICE = \"native\"  # either \"native\" or \"server\"\n",
    "n_firms_LIST = [2, 3, 4, 5]  # list with number of agents for each run\n",
    "ITERS_TEST = 2 # number of iteration for test\n",
    "ITERS_RUN = 1000 # number of iteration for fullrun\n",
    "\n",
    "\n",
    "# Other economic Hiperparameteres.\n",
    "ENV_HORIZON = 1000\n",
    "N_CAPITAL = 1\n",
    "BETA = 0.98  # discount parameter"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "\"\"\" STEP 1: Paralleliztion and batch options\"\"\"\n",
    "# Parallelization options\n",
    "NUM_CPUS = 4\n",
    "NUM_CPUS_DRIVER = 1\n",
    "NUM_TRIALS = 6\n",
    "NUM_ROLLOUT = ENV_HORIZON * 1\n",
    "NUM_ENV_PW = 1  # num_env_per_worker\n",
    "NUM_GPUS = 0\n",
    "BATCH_ROLLOUT = 1\n",
    "NUM_MINI_BATCH = NUM_CPUS_DRIVER\n",
    "\n",
    "N_WORKERS = (NUM_CPUS - NUM_TRIALS * NUM_CPUS_DRIVER) // NUM_TRIALS\n",
    "BATCH_SIZE = NUM_ROLLOUT * (max(N_WORKERS, 1)) * NUM_ENV_PW * BATCH_ROLLOUT\n",
    "\n",
    "print(N_WORKERS, BATCH_SIZE)\n",
    "\n",
    "# define length of experiment (MAX_STEPS) and experiment name\n",
    "if TEST == True:\n",
    "    MAX_STEPS = ITERS_TEST * BATCH_SIZE\n",
    "else:\n",
    "    MAX_STEPS = ITERS_RUN * BATCH_SIZE\n",
    "\n",
    "CHKPT_FREQ = 10\n",
    "\n",
    "stop = {\"timesteps_total\": MAX_STEPS}\n",
    "# Initialize ray\n",
    "shutdown()\n",
    "init(\n",
    "    num_cpus=NUM_CPUS,\n",
    "    num_gpus=NUM_GPUS,\n",
    "    # logging_level=logging.ERROR,\n",
    ")\n",
    "\n",
    "# Define environment, which should be imported from a class\n",
    "ENV_LABEL = \"townsend\"\n",
    "register_env(ENV_LABEL, Townsend)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-1 1000\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-03 20:28:38,266\tINFO services.py:1267 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "\"\"\" STEP 2: set custom metrics such as discounted rewards to keep track of through leraning\"\"\"\n",
    "# Define custom metrics using the Callbacks class\n",
    "# See rllib documentation on Callbacks. They are a way of inserting code in different parts of the pipeline.\n",
    "\n",
    "# function to get discounted rewards for analysys\n",
    "def process_rewards(r):\n",
    "    \"\"\"Compute discounted reward from a vector of rewards.\"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, len(r))):\n",
    "        running_add = running_add * BETA + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r[0]\n",
    "\n",
    "\n",
    "class MyCallbacks(DefaultCallbacks):\n",
    "    def on_episode_start(\n",
    "        self,\n",
    "        *,\n",
    "        worker: RolloutWorker,\n",
    "        base_env: BaseEnv,\n",
    "        policies: Dict[str, Policy],\n",
    "        episode: MultiAgentEpisode,\n",
    "        env_index: int,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # Make sure this episode has just been started (only initial obs\n",
    "        # logged so far).\n",
    "\n",
    "        assert episode.length == 0, (\n",
    "            \"ERROR: `on_episode_start()` callback should be called right \"\n",
    "            \"after env reset!\"\n",
    "        )\n",
    "        episode.user_data[\"rewards\"] = []\n",
    "\n",
    "    def on_episode_step(\n",
    "        self,\n",
    "        *,\n",
    "        worker: RolloutWorker,\n",
    "        base_env: BaseEnv,\n",
    "        episode: MultiAgentEpisode,\n",
    "        env_index: int,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        if episode.length > 1:  # at t=0, previous rewards are not defined\n",
    "            rewards = episode.prev_reward_for(\"firm_0\")\n",
    "            episode.user_data[\"rewards\"].append(rewards)\n",
    "\n",
    "    def on_episode_end(\n",
    "        self,\n",
    "        *,\n",
    "        worker: RolloutWorker,\n",
    "        base_env: BaseEnv,\n",
    "        policies: Dict[str, Policy],\n",
    "        episode: MultiAgentEpisode,\n",
    "        env_index: int,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        discounted_rewards = process_rewards(episode.user_data[\"rewards\"])\n",
    "        episode.custom_metrics[\"discounted_rewards\"] = discounted_rewards\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "\"\"\" STEP 3: Environment and Algorithm configuration \"\"\"\n",
    "\n",
    "\n",
    "# environment config including evaluation environment (without exploration)\n",
    "env_config={\n",
    "    \"horizon\": ENV_HORIZON,\n",
    "    \"N_firms\": 2,\n",
    "    \"eval_mode\": False,\n",
    "    \"analysis_mode\": False,\n",
    "    \"simul_mode\": False,\n",
    "    \"max_savings\": 0.6,\n",
    "    \"parameters\": {\n",
    "        \"delta\": 0,\n",
    "        \"alpha\": 1,\n",
    "        \"beta\": 0.98,\n",
    "        \"phi\": 0.5,\n",
    "        \"A\": 1,\n",
    "        \"tfp\": 0.1,\n",
    "        \"rho\": 0.9,\n",
    "        \"theta_0\": 10,\n",
    "        \"mean_w\": 0,\n",
    "        \"var_w\": 1,\n",
    "        \"var_epsilon\": 1,\n",
    "        \"var_theta\": 2,\n",
    "    },\n",
    "}\n",
    "\n",
    "env_config_eval = env_config.copy()\n",
    "env_config_eval[\"eval_mode\"] = True\n",
    "\n",
    "# we instantiate the environment to extrac relevant info\n",
    "\" CHANGE HERE \"\n",
    "env = Townsend(env_config)\n",
    "\n",
    "# common configuration\n",
    "\n",
    "\"\"\"\n",
    "NOTE: in order to do hyperparameter optimization, you can select a range of values \n",
    "with tune.choice([0.05,1] for random choice or tune.grid_search([0.05,1]) for fix search.\n",
    "# see https://docs.ray.io/en/master/tune/key-concepts.html#search-spaces for spaces and their definition.\n",
    "# se at the bottom (Annex_env_hyp) for an explanation how to do the same with environment parameters.\n",
    "\"\"\"\n",
    "common_config = {\n",
    "    # CUSTOM METRICS\n",
    "    \"callbacks\": MyCallbacks,\n",
    "    # ENVIRONMENT\n",
    "    \"gamma\": BETA,\n",
    "    \"env\": ENV_LABEL,\n",
    "    \"env_config\": env_config,\n",
    "    \"horizon\": ENV_HORIZON,\n",
    "    # MODEL\n",
    "    \"framework\": \"torch\",\n",
    "    # \"model\": tune.grid_search([{\"use_lstm\": True}, {\"use_lstm\": False}]),\n",
    "    # TRAINING CONFIG\n",
    "    \"num_workers\": N_WORKERS,\n",
    "    \"create_env_on_driver\": False,\n",
    "    \"num_gpus\": NUM_GPUS / NUM_TRIALS,\n",
    "    \"num_envs_per_worker\": NUM_ENV_PW,\n",
    "    \"num_cpus_for_driver\": NUM_CPUS_DRIVER,\n",
    "    \"rollout_fragment_length\": NUM_ROLLOUT,\n",
    "    \"train_batch_size\": BATCH_SIZE,\n",
    "    # EVALUATION\n",
    "    \"evaluation_interval\": 1,\n",
    "    \"evaluation_num_episodes\": 1,\n",
    "    \"evaluation_config\": {\n",
    "        \"explore\": False,\n",
    "        \"env_config\": env_config_eval,\n",
    "    },\n",
    "    # MULTIAGENT,\n",
    "    \"multiagent\": {\n",
    "        \"policies\": {\n",
    "            \"firm\": (\n",
    "                None,\n",
    "                env.observation_space[\"firm_0\"],\n",
    "                env.action_space[\"firm_0\"],\n",
    "                {},\n",
    "            ),\n",
    "        },\n",
    "        \"policy_mapping_fn\": (lambda agent_id: agent_id.split(\"_\")[0]),\n",
    "        \"replay_mode\": \"independent\",  # you can change to \"lockstep\".\n",
    "    },\n",
    "}\n",
    "\n",
    "# Configs specific to the chosel algorithms, INCLUDING THE LEARNING RATE\n",
    "ppo_config = {\n",
    "    \"lr\": 0.0005,\n",
    "    # \"lr_schedule\": [[0, 0.00005], [MAX_STEPS * 1 / 2, 0.00001]],\n",
    "    \"sgd_minibatch_size\": BATCH_SIZE // NUM_MINI_BATCH,\n",
    "    \"num_sgd_iter\": 1,\n",
    "    \"batch_mode\": \"complete_episodes\",\n",
    "    \"lambda\": 0.98,\n",
    "    \"entropy_coeff\": 0,\n",
    "    \"kl_coeff\": 0.2,\n",
    "    # \"vf_loss_coeff\": 0.5,\n",
    "    # \"vf_clip_param\": tune.choice([5, 10, 20]),\n",
    "    # \"entropy_coeff_schedule\": [[0, 0.01], [5120 * 1000, 0]],\n",
    "    \"clip_param\": 0.2,\n",
    "    \"clip_actions\": True,\n",
    "}\n",
    "\n",
    "sac_config = {\n",
    "    \"prioritized_replay\": True,\n",
    "}\n",
    "\n",
    "if ALGO == \"PPO\":\n",
    "    training_config = {**common_config, **ppo_config}\n",
    "elif ALGO == \"SAC\":\n",
    "    training_config = {**common_config, **sac_config}\n",
    "else:\n",
    "    training_config = common_config"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "\"\"\" STEP 4: run experiment \"\"\"\n",
    "\n",
    "exp_names = []\n",
    "exp_dirs = []\n",
    "checkpoints = []\n",
    "best_rewards = []\n",
    "best_configs = []\n",
    "learning_dta = []\n",
    "\n",
    "\n",
    "\n",
    "# RUN TRAINER\n",
    "for n_firms in n_firms_LIST:\n",
    "    EXP_LABEL = DEVICE + f\"_{n_firms}_firms_\"\n",
    "    if TEST == True:\n",
    "        EXP_NAME = EXP_LABEL + ENV_LABEL + \"_test_\" + DATE + ALGO\n",
    "    else:\n",
    "        EXP_NAME = EXP_LABEL + ENV_LABEL + \"_run_\" + DATE + ALGO\n",
    "\n",
    "    env_config[\"N_firms\"] = n_firms\n",
    "    env_config_eval[\"N_firms\"] = n_firms\n",
    "    \"\"\" CHANGE HERE \"\"\"\n",
    "    env = Townsend(env_config)\n",
    "    training_config[\"env_config\"] = env_config\n",
    "    training_config[\"evaluation_config\"][\"env_config\"] = env_config_eval\n",
    "    training_config[\"multiagent\"] = {\n",
    "        \"policies\": {\n",
    "            \"firm\": (\n",
    "                None,\n",
    "                env.observation_space[\"firm_0\"],\n",
    "                env.action_space[\"firm_0\"],\n",
    "                {},\n",
    "            ),\n",
    "        },\n",
    "        \"policy_mapping_fn\": (lambda agent_id: agent_id.split(\"_\")[0]),\n",
    "        \"replay_mode\": \"independent\",  # you can change to \"lockstep\".\n",
    "    }\n",
    "\n",
    "    analysis = tune.run(\n",
    "        ALGO,\n",
    "        name=EXP_NAME,\n",
    "        config=training_config,\n",
    "        stop=stop,\n",
    "        checkpoint_freq=CHKPT_FREQ,\n",
    "        checkpoint_at_end=True,\n",
    "        metric=\"evaluation/custom_metrics/discounted_rewards_mean\",\n",
    "        mode=\"max\",\n",
    "        num_samples=NUM_TRIALS,\n",
    "        # resources_per_trial={\"gpu\": 0.5},\n",
    "    )\n",
    "\n",
    "    exp_names.append(EXP_NAME)\n",
    "    checkpoints.append(analysis.best_checkpoint)\n",
    "    best_rewards.append(\n",
    "        analysis.best_result[\"evaluation\"][\"custom_metrics\"][\"discounted_rewards_mean\"]\n",
    "    )\n",
    "    best_configs.append(analysis.best_config)\n",
    "    exp_dirs.append(analysis.best_logdir)\n",
    "    learning_dta.append(\n",
    "        analysis.best_dataframe[\n",
    "            [\"episodes_total\", \"evaluation/custom_metrics/discounted_rewards_mean\"]\n",
    "        ]\n",
    "    )\n",
    "    learning_dta[n_firms - 1].columns = [\"episodes_total\", f\"{n_firms} households\"]"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.7/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/4 CPUs, 0/0 GPUs, 0.0/10.89 GiB heap, 0.0/5.44 GiB objects<br>Result logdir: /Users/matiascovarrubias/ray_results/native_1_firms_townsend_test_Sep3_PPO<br>Number of trials: 6/6 (5 PENDING, 1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_townsend_0e084_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_0e084_00001</td><td>PENDING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_0e084_00002</td><td>PENDING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_0e084_00003</td><td>PENDING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_0e084_00004</td><td>PENDING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_0e084_00005</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=31897)\u001b[0m WARNING:tensorflow:From /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=31897)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=31897)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=31899)\u001b[0m WARNING:tensorflow:From /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=31899)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=31899)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=31898)\u001b[0m WARNING:tensorflow:From /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=31898)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=31898)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=31896)\u001b[0m WARNING:tensorflow:From /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=31896)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=31896)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=31897)\u001b[0m 2021-09-03 20:28:45,609\tINFO trainer.py:694 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=31899)\u001b[0m 2021-09-03 20:28:45,742\tINFO trainer.py:694 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=31898)\u001b[0m 2021-09-03 20:28:45,763\tINFO trainer.py:694 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=31897)\u001b[0m 2021-09-03 20:28:45,774\tWARNING deprecation.py:33 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=31896)\u001b[0m 2021-09-03 20:28:45,772\tINFO trainer.py:694 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=31899)\u001b[0m 2021-09-03 20:28:45,922\tWARNING deprecation.py:33 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=31898)\u001b[0m 2021-09-03 20:28:45,939\tWARNING deprecation.py:33 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=31896)\u001b[0m 2021-09-03 20:28:45,954\tWARNING deprecation.py:33 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.2/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/4 CPUs, 0/0 GPUs, 0.0/10.89 GiB heap, 0.0/5.44 GiB objects<br>Result logdir: /Users/matiascovarrubias/ray_results/native_1_firms_townsend_test_Sep3_PPO<br>Number of trials: 6/6 (2 PENDING, 4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_townsend_0e084_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_0e084_00001</td><td>RUNNING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_0e084_00002</td><td>RUNNING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_0e084_00003</td><td>RUNNING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_0e084_00004</td><td>PENDING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_0e084_00005</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=31897)\u001b[0m 2021-09-03 20:28:47,770\tWARNING deprecation.py:33 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=31899)\u001b[0m 2021-09-03 20:28:47,906\tWARNING deprecation.py:33 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=31898)\u001b[0m 2021-09-03 20:28:47,939\tWARNING deprecation.py:33 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=31896)\u001b[0m 2021-09-03 20:28:47,986\tWARNING deprecation.py:33 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_townsend_0e084_00000:\n",
      "  agent_timesteps_total: 1000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: -108.46254754923437\n",
      "    discounted_rewards_mean: -108.46254754923437\n",
      "    discounted_rewards_min: -108.46254754923437\n",
      "  date: 2021-09-03_20-28-49\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - -2888.092825951124\n",
      "  episode_reward_mean: -2888.092825951124\n",
      "  episode_reward_min:\n",
      "  - -2888.092825951124\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: -104.47070180024892\n",
      "      discounted_rewards_mean: -104.47070180024892\n",
      "      discounted_rewards_min: -104.47070180024892\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -1535.0452699891769\n",
      "    episode_reward_mean: -1535.0452699891769\n",
      "    episode_reward_min:\n",
      "    - -1535.0452699891769\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -1535.0452699891769\n",
      "      policy_firm_reward:\n",
      "      - - -1535.0452699891769\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -1535.0452699891769\n",
      "    policy_reward_mean:\n",
      "      firm: -1535.0452699891769\n",
      "    policy_reward_min:\n",
      "      firm: -1535.0452699891769\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11593287998622472\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.1328732226635669\n",
      "      mean_inference_ms: 1.3837352261081204\n",
      "      mean_raw_obs_processing_ms: 0.16131315317068187\n",
      "  experiment_id: 4fea6012a56b4458b429f97fba7072f1\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.414589762687683\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.4158454328744475e-16\n",
      "          policy_loss: 1.5901184724498307e-07\n",
      "          total_loss: 12347.927734375\n",
      "          vf_explained_var: -7.212161978742415e-09\n",
      "          vf_loss: 12347.9287109375\n",
      "    num_agent_steps_sampled: 1000\n",
      "    num_agent_steps_trained: 1000\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.38333333333333\n",
      "    ram_util_percent: 50.86666666666667\n",
      "  pid: 31897\n",
      "  policy_reward_max:\n",
      "    firm: -2888.092825951124\n",
      "  policy_reward_mean:\n",
      "    firm: -2888.092825951124\n",
      "  policy_reward_min:\n",
      "    firm: -2888.092825951124\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1199295470764587\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1353888840346665\n",
      "    mean_inference_ms: 1.4413131938709485\n",
      "    mean_raw_obs_processing_ms: 0.18815822772808247\n",
      "  time_since_restore: 3.925259828567505\n",
      "  time_this_iter_s: 3.925259828567505\n",
      "  time_total_s: 3.925259828567505\n",
      "  timers:\n",
      "    learn_throughput: 5254.446\n",
      "    learn_time_ms: 190.315\n",
      "    sample_throughput: 525.412\n",
      "    sample_time_ms: 1903.267\n",
      "  timestamp: 1630715329\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: 0e084_00000\n",
      "  \n",
      "Result for PPO_townsend_0e084_00001:\n",
      "  agent_timesteps_total: 1000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: 76.34083265898316\n",
      "    discounted_rewards_mean: 76.34083265898316\n",
      "    discounted_rewards_min: 76.34083265898316\n",
      "  date: 2021-09-03_20-28-49\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - -309.7096719668825\n",
      "  episode_reward_mean: -309.7096719668825\n",
      "  episode_reward_min:\n",
      "  - -309.7096719668825\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: 72.00287618309524\n",
      "      discounted_rewards_mean: 72.00287618309524\n",
      "      discounted_rewards_min: 72.00287618309524\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -297.0505407034221\n",
      "    episode_reward_mean: -297.0505407034221\n",
      "    episode_reward_min:\n",
      "    - -297.0505407034221\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -297.0505407034221\n",
      "      policy_firm_reward:\n",
      "      - - -297.0505407034221\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -297.0505407034221\n",
      "    policy_reward_mean:\n",
      "      firm: -297.0505407034221\n",
      "    policy_reward_min:\n",
      "      firm: -297.0505407034221\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1117747265856702\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.1300907992459201\n",
      "      mean_inference_ms: 1.3341160563679486\n",
      "      mean_raw_obs_processing_ms: 0.1657976137174593\n",
      "  experiment_id: c2151c8a72cb4145830c6c514a4111c1\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.424928903579712\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.563194010201891e-16\n",
      "          policy_loss: -2.4475097504250698e-08\n",
      "          total_loss: 2668.054443359375\n",
      "          vf_explained_var: 2.38418573772492e-09\n",
      "          vf_loss: 2668.0546875\n",
      "    num_agent_steps_sampled: 1000\n",
      "    num_agent_steps_trained: 1000\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.016666666666666\n",
      "    ram_util_percent: 50.86666666666667\n",
      "  pid: 31899\n",
      "  policy_reward_max:\n",
      "    firm: -309.7096719668825\n",
      "  policy_reward_mean:\n",
      "    firm: -309.7096719668825\n",
      "  policy_reward_min:\n",
      "    firm: -309.7096719668825\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11972542647477036\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1389203848062338\n",
      "    mean_inference_ms: 1.4258257516256936\n",
      "    mean_raw_obs_processing_ms: 0.18633066952883542\n",
      "  time_since_restore: 3.8600897789001465\n",
      "  time_this_iter_s: 3.8600897789001465\n",
      "  time_total_s: 3.8600897789001465\n",
      "  timers:\n",
      "    learn_throughput: 5245.47\n",
      "    learn_time_ms: 190.641\n",
      "    sample_throughput: 529.538\n",
      "    sample_time_ms: 1888.44\n",
      "  timestamp: 1630715329\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: 0e084_00001\n",
      "  \n",
      "Result for PPO_townsend_0e084_00002:\n",
      "  agent_timesteps_total: 1000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: 79.34344088562108\n",
      "    discounted_rewards_mean: 79.34344088562108\n",
      "    discounted_rewards_min: 79.34344088562108\n",
      "  date: 2021-09-03_20-28-49\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - -277.823201555947\n",
      "  episode_reward_mean: -277.823201555947\n",
      "  episode_reward_min:\n",
      "  - -277.823201555947\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: 80.47969163145366\n",
      "      discounted_rewards_mean: 80.47969163145366\n",
      "      discounted_rewards_min: 80.47969163145366\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -1356.029169142978\n",
      "    episode_reward_mean: -1356.029169142978\n",
      "    episode_reward_min:\n",
      "    - -1356.029169142978\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -1356.029169142978\n",
      "      policy_firm_reward:\n",
      "      - - -1356.029169142978\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -1356.029169142978\n",
      "    policy_reward_mean:\n",
      "      firm: -1356.029169142978\n",
      "    policy_reward_min:\n",
      "      firm: -1356.029169142978\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11130765482381388\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.13024418742268473\n",
      "      mean_inference_ms: 1.3393560251394114\n",
      "      mean_raw_obs_processing_ms: 0.16679225506244244\n",
      "  experiment_id: 18673948113642e39edb6a5b2d5c7458\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.4138479232788086\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.847411151092743e-16\n",
      "          policy_loss: -3.784179725130343e-08\n",
      "          total_loss: 2383.176025390625\n",
      "          vf_explained_var: 9.536743617033494e-10\n",
      "          vf_loss: 2383.176025390625\n",
      "    num_agent_steps_sampled: 1000\n",
      "    num_agent_steps_trained: 1000\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.06666666666666\n",
      "    ram_util_percent: 50.86666666666667\n",
      "  pid: 31898\n",
      "  policy_reward_max:\n",
      "    firm: -277.823201555947\n",
      "  policy_reward_mean:\n",
      "    firm: -277.823201555947\n",
      "  policy_reward_min:\n",
      "    firm: -277.823201555947\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12105971306830376\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1387191223693299\n",
      "    mean_inference_ms: 1.4380193971372865\n",
      "    mean_raw_obs_processing_ms: 0.1867384343714147\n",
      "  time_since_restore: 3.8835628032684326\n",
      "  time_this_iter_s: 3.8835628032684326\n",
      "  time_total_s: 3.8835628032684326\n",
      "  timers:\n",
      "    learn_throughput: 5129.838\n",
      "    learn_time_ms: 194.938\n",
      "    sample_throughput: 525.615\n",
      "    sample_time_ms: 1902.532\n",
      "  timestamp: 1630715329\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: 0e084_00002\n",
      "  \n",
      "Result for PPO_townsend_0e084_00003:\n",
      "  agent_timesteps_total: 1000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: -47.915400885789595\n",
      "    discounted_rewards_mean: -47.915400885789595\n",
      "    discounted_rewards_min: -47.915400885789595\n",
      "  date: 2021-09-03_20-28-49\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - -1571.0439993311713\n",
      "  episode_reward_mean: -1571.0439993311713\n",
      "  episode_reward_min:\n",
      "  - -1571.0439993311713\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: 94.70991349610146\n",
      "      discounted_rewards_mean: 94.70991349610146\n",
      "      discounted_rewards_min: 94.70991349610146\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -1157.4278679498689\n",
      "    episode_reward_mean: -1157.4278679498689\n",
      "    episode_reward_min:\n",
      "    - -1157.4278679498689\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -1157.4278679498689\n",
      "      policy_firm_reward:\n",
      "      - - -1157.4278679498689\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -1157.4278679498689\n",
      "    policy_reward_mean:\n",
      "      firm: -1157.4278679498689\n",
      "    policy_reward_min:\n",
      "      firm: -1157.4278679498689\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11439756913618608\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.13264623674360307\n",
      "      mean_inference_ms: 1.3547115154437848\n",
      "      mean_raw_obs_processing_ms: 0.17530220252769693\n",
      "  experiment_id: 216e687858cf48ba8b27271946a00c91\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.40986967086792\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.5579537386220775e-16\n",
      "          policy_loss: 7.0037842192505195e-09\n",
      "          total_loss: 5044.86767578125\n",
      "          vf_explained_var: -7.867813067718998e-09\n",
      "          vf_loss: 5044.86767578125\n",
      "    num_agent_steps_sampled: 1000\n",
      "    num_agent_steps_trained: 1000\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.1\n",
      "    ram_util_percent: 50.849999999999994\n",
      "  pid: 31896\n",
      "  policy_reward_max:\n",
      "    firm: -1571.0439993311713\n",
      "  policy_reward_mean:\n",
      "    firm: -1571.0439993311713\n",
      "  policy_reward_min:\n",
      "    firm: -1571.0439993311713\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12204101631095955\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1424194930435775\n",
      "    mean_inference_ms: 1.4574754011857285\n",
      "    mean_raw_obs_processing_ms: 0.1907089016177914\n",
      "  time_since_restore: 3.9337239265441895\n",
      "  time_this_iter_s: 3.9337239265441895\n",
      "  time_total_s: 3.9337239265441895\n",
      "  timers:\n",
      "    learn_throughput: 5389.093\n",
      "    learn_time_ms: 185.56\n",
      "    sample_throughput: 517.681\n",
      "    sample_time_ms: 1931.69\n",
      "  timestamp: 1630715329\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: 0e084_00003\n",
      "  \n",
      "Result for PPO_townsend_0e084_00000:\n",
      "  agent_timesteps_total: 2000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: -103.82824384264018\n",
      "    discounted_rewards_mean: -106.14539569593728\n",
      "    discounted_rewards_min: -108.46254754923437\n",
      "  date: 2021-09-03_20-28-53\n",
      "  done: true\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - -1105.762135405807\n",
      "  episode_reward_mean: -1996.9274806784654\n",
      "  episode_reward_min:\n",
      "  - -2888.092825951124\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: 42.68414519730039\n",
      "      discounted_rewards_mean: 42.68414519730039\n",
      "      discounted_rewards_min: 42.68414519730039\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -963.8626119401334\n",
      "    episode_reward_mean: -963.8626119401334\n",
      "    episode_reward_min:\n",
      "    - -963.8626119401334\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -963.8626119401334\n",
      "      policy_firm_reward:\n",
      "      - - -963.8626119401334\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -963.8626119401334\n",
      "    policy_reward_mean:\n",
      "      firm: -963.8626119401334\n",
      "    policy_reward_min:\n",
      "      firm: -963.8626119401334\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11004345944855941\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.126307872103072\n",
      "      mean_inference_ms: 1.318790029728788\n",
      "      mean_raw_obs_processing_ms: 0.15897514938057095\n",
      "  experiment_id: 4fea6012a56b4458b429f97fba7072f1\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.1\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.3535147905349731\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.8369308726309123e-16\n",
      "          policy_loss: 9.808349688000817e-08\n",
      "          total_loss: 5960.703125\n",
      "          vf_explained_var: -8.344650526126429e-10\n",
      "          vf_loss: 5960.703125\n",
      "    num_agent_steps_sampled: 2000\n",
      "    num_agent_steps_trained: 2000\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.220000000000006\n",
      "    ram_util_percent: 51.32000000000001\n",
      "  pid: 31897\n",
      "  policy_reward_max:\n",
      "    firm: -1105.762135405807\n",
      "  policy_reward_mean:\n",
      "    firm: -1996.9274806784654\n",
      "  policy_reward_min:\n",
      "    firm: -2888.092825951124\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11675790207085635\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1318482228997192\n",
      "    mean_inference_ms: 1.4111780449669515\n",
      "    mean_raw_obs_processing_ms: 0.18015924226831442\n",
      "  time_since_restore: 7.501527786254883\n",
      "  time_this_iter_s: 3.576267957687378\n",
      "  time_total_s: 7.501527786254883\n",
      "  timers:\n",
      "    learn_throughput: 5183.778\n",
      "    learn_time_ms: 192.909\n",
      "    sample_throughput: 552.972\n",
      "    sample_time_ms: 1808.411\n",
      "  timestamp: 1630715333\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: 0e084_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.5/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/4 CPUs, 0/0 GPUs, 0.0/10.89 GiB heap, 0.0/5.44 GiB objects<br>Result logdir: /Users/matiascovarrubias/ray_results/native_1_firms_townsend_test_Sep3_PPO<br>Number of trials: 6/6 (2 PENDING, 4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_len_mean</th><th style=\"text-align: right;\">  episodes_this_iter</th><th style=\"text-align: right;\">  num_healthy_workers</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_townsend_0e084_00000</td><td>RUNNING </td><td>192.168.1.202:31897</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         7.50153</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">-1996.93 </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_0e084_00001</td><td>RUNNING </td><td>192.168.1.202:31899</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.86009</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\"> -309.71 </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_0e084_00002</td><td>RUNNING </td><td>192.168.1.202:31898</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.88356</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\"> -277.823</td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_0e084_00003</td><td>RUNNING </td><td>192.168.1.202:31896</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.93372</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">-1571.04 </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_0e084_00004</td><td>PENDING </td><td>                   </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                     </td></tr>\n",
       "<tr><td>PPO_townsend_0e084_00005</td><td>PENDING </td><td>                   </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_townsend_0e084_00002:\n",
      "  agent_timesteps_total: 2000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: 127.81453123238353\n",
      "    discounted_rewards_mean: 103.5789860590023\n",
      "    discounted_rewards_min: 79.34344088562108\n",
      "  date: 2021-09-03_20-28-53\n",
      "  done: true\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - -277.823201555947\n",
      "  episode_reward_mean: -546.7310632033934\n",
      "  episode_reward_min:\n",
      "  - -815.6389248508398\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: 55.29194996937671\n",
      "      discounted_rewards_mean: 55.29194996937671\n",
      "      discounted_rewards_min: 55.29194996937671\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -1102.8945176273116\n",
      "    episode_reward_mean: -1102.8945176273116\n",
      "    episode_reward_min:\n",
      "    - -1102.8945176273116\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -1102.8945176273116\n",
      "      policy_firm_reward:\n",
      "      - - -1102.8945176273116\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -1102.8945176273116\n",
      "    policy_reward_mean:\n",
      "      firm: -1102.8945176273116\n",
      "    policy_reward_min:\n",
      "      firm: -1102.8945176273116\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10694282642309216\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.12509230671376959\n",
      "      mean_inference_ms: 1.28572014556534\n",
      "      mean_raw_obs_processing_ms: 0.16152257028071657\n",
      "  experiment_id: 18673948113642e39edb6a5b2d5c7458\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.1\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.4567830562591553\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.547473724858043e-16\n",
      "          policy_loss: -5.0216673486147556e-08\n",
      "          total_loss: 2638.904296875\n",
      "          vf_explained_var: -4.768371808516747e-10\n",
      "          vf_loss: 2638.904296875\n",
      "    num_agent_steps_sampled: 2000\n",
      "    num_agent_steps_trained: 2000\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.419999999999998\n",
      "    ram_util_percent: 51.36\n",
      "  pid: 31898\n",
      "  policy_reward_max:\n",
      "    firm: -277.823201555947\n",
      "  policy_reward_mean:\n",
      "    firm: -546.7310632033934\n",
      "  policy_reward_min:\n",
      "    firm: -815.6389248508398\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11754764185383335\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.13533394970756657\n",
      "    mean_inference_ms: 1.406492650150866\n",
      "    mean_raw_obs_processing_ms: 0.18038026231088036\n",
      "  time_since_restore: 7.4214186668396\n",
      "  time_this_iter_s: 3.537855863571167\n",
      "  time_total_s: 7.4214186668396\n",
      "  timers:\n",
      "    learn_throughput: 5306.911\n",
      "    learn_time_ms: 188.434\n",
      "    sample_throughput: 553.125\n",
      "    sample_time_ms: 1807.91\n",
      "  timestamp: 1630715333\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: 0e084_00002\n",
      "  \n",
      "Result for PPO_townsend_0e084_00001:\n",
      "  agent_timesteps_total: 2000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: 76.34083265898316\n",
      "    discounted_rewards_mean: 49.38384651794036\n",
      "    discounted_rewards_min: 22.42686037689756\n",
      "  date: 2021-09-03_20-28-53\n",
      "  done: true\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - -309.7096719668825\n",
      "  episode_reward_mean: -485.7974733481526\n",
      "  episode_reward_min:\n",
      "  - -661.8852747294227\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: -9.283219967288593\n",
      "      discounted_rewards_mean: -9.283219967288593\n",
      "      discounted_rewards_min: -9.283219967288593\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -1317.8651947919834\n",
      "    episode_reward_mean: -1317.8651947919834\n",
      "    episode_reward_min:\n",
      "    - -1317.8651947919834\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -1317.8651947919834\n",
      "      policy_firm_reward:\n",
      "      - - -1317.8651947919834\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -1317.8651947919834\n",
      "    policy_reward_mean:\n",
      "      firm: -1317.8651947919834\n",
      "    policy_reward_min:\n",
      "      firm: -1317.8651947919834\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10863308427573323\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.1261953947724014\n",
      "      mean_inference_ms: 1.2829569445318845\n",
      "      mean_raw_obs_processing_ms: 0.1614187908792186\n",
      "  experiment_id: c2151c8a72cb4145830c6c514a4111c1\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.1\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.4426182508468628\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.9790394430763384e-16\n",
      "          policy_loss: 3.7567136956795366e-08\n",
      "          total_loss: 2343.2529296875\n",
      "          vf_explained_var: -3.6358833721550354e-09\n",
      "          vf_loss: 2343.2529296875\n",
      "    num_agent_steps_sampled: 2000\n",
      "    num_agent_steps_trained: 2000\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.54\n",
      "    ram_util_percent: 51.36\n",
      "  pid: 31899\n",
      "  policy_reward_max:\n",
      "    firm: -309.7096719668825\n",
      "  policy_reward_mean:\n",
      "    firm: -485.7974733481526\n",
      "  policy_reward_min:\n",
      "    firm: -661.8852747294227\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11666078648317196\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.13531602695989858\n",
      "    mean_inference_ms: 1.3949192894837579\n",
      "    mean_raw_obs_processing_ms: 0.1803558789347764\n",
      "  time_since_restore: 7.409777641296387\n",
      "  time_this_iter_s: 3.5496878623962402\n",
      "  time_total_s: 7.409777641296387\n",
      "  timers:\n",
      "    learn_throughput: 5205.676\n",
      "    learn_time_ms: 192.098\n",
      "    sample_throughput: 556.538\n",
      "    sample_time_ms: 1796.822\n",
      "  timestamp: 1630715333\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: 0e084_00001\n",
      "  \n",
      "Result for PPO_townsend_0e084_00003:\n",
      "  agent_timesteps_total: 2000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: 41.89223942816827\n",
      "    discounted_rewards_mean: -3.0115807288106637\n",
      "    discounted_rewards_min: -47.915400885789595\n",
      "  date: 2021-09-03_20-28-53\n",
      "  done: true\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - -375.31152835574534\n",
      "  episode_reward_mean: -973.1777638434584\n",
      "  episode_reward_min:\n",
      "  - -1571.0439993311713\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: -40.16090730190343\n",
      "      discounted_rewards_mean: -40.16090730190343\n",
      "      discounted_rewards_min: -40.16090730190343\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -1272.2185578010394\n",
      "    episode_reward_mean: -1272.2185578010394\n",
      "    episode_reward_min:\n",
      "    - -1272.2185578010394\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -1272.2185578010394\n",
      "      policy_firm_reward:\n",
      "      - - -1272.2185578010394\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -1272.2185578010394\n",
      "    policy_reward_mean:\n",
      "      firm: -1272.2185578010394\n",
      "    policy_reward_min:\n",
      "      firm: -1272.2185578010394\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10939480840176835\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.12698726377625397\n",
      "      mean_inference_ms: 1.295998595703369\n",
      "      mean_raw_obs_processing_ms: 0.16699964436574438\n",
      "  experiment_id: 216e687858cf48ba8b27271946a00c91\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.1\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.392898440361023\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.694822302185486e-16\n",
      "          policy_loss: 1.563262941317589e-08\n",
      "          total_loss: 5308.03271484375\n",
      "          vf_explained_var: -4.768371808516747e-10\n",
      "          vf_loss: 5308.03271484375\n",
      "    num_agent_steps_sampled: 2000\n",
      "    num_agent_steps_trained: 2000\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.7\n",
      "    ram_util_percent: 51.36\n",
      "  pid: 31896\n",
      "  policy_reward_max:\n",
      "    firm: -375.31152835574534\n",
      "  policy_reward_mean:\n",
      "    firm: -973.1777638434584\n",
      "  policy_reward_min:\n",
      "    firm: -1571.0439993311713\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11880371124235187\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.13834304474465609\n",
      "    mean_inference_ms: 1.4225108634880401\n",
      "    mean_raw_obs_processing_ms: 0.18523557426160414\n",
      "  time_since_restore: 7.494019031524658\n",
      "  time_this_iter_s: 3.5602951049804688\n",
      "  time_total_s: 7.494019031524658\n",
      "  timers:\n",
      "    learn_throughput: 5508.974\n",
      "    learn_time_ms: 181.522\n",
      "    sample_throughput: 546.082\n",
      "    sample_time_ms: 1831.227\n",
      "  timestamp: 1630715333\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: 0e084_00003\n",
      "  \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(pid=31949)\u001b[0m WARNING:tensorflow:From /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=31949)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=31949)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=31951)\u001b[0m WARNING:tensorflow:From /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=31951)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=31951)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=31949)\u001b[0m 2021-09-03 20:28:57,964\tINFO trainer.py:694 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=31951)\u001b[0m 2021-09-03 20:28:58,046\tINFO trainer.py:694 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=31949)\u001b[0m 2021-09-03 20:28:58,107\tWARNING deprecation.py:33 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=31951)\u001b[0m 2021-09-03 20:28:58,194\tWARNING deprecation.py:33 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=31949)\u001b[0m 2021-09-03 20:28:59,717\tWARNING deprecation.py:33 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=31951)\u001b[0m 2021-09-03 20:28:59,803\tWARNING deprecation.py:33 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_townsend_0e084_00005:\n",
      "  agent_timesteps_total: 1000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: 53.290731408213716\n",
      "    discounted_rewards_mean: 53.290731408213716\n",
      "    discounted_rewards_min: 53.290731408213716\n",
      "  date: 2021-09-03_20-29-01\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - 46.72199987287698\n",
      "  episode_reward_mean: 46.72199987287698\n",
      "  episode_reward_min:\n",
      "  - 46.72199987287698\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: -53.95663440873671\n",
      "      discounted_rewards_mean: -53.95663440873671\n",
      "      discounted_rewards_min: -53.95663440873671\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -941.9638593581832\n",
      "    episode_reward_mean: -941.9638593581832\n",
      "    episode_reward_min:\n",
      "    - -941.9638593581832\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -941.9638593581832\n",
      "      policy_firm_reward:\n",
      "      - - -941.9638593581832\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -941.9638593581832\n",
      "    policy_reward_mean:\n",
      "      firm: -941.9638593581832\n",
      "    policy_reward_min:\n",
      "      firm: -941.9638593581832\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.08923571545641859\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10248807284024569\n",
      "      mean_inference_ms: 1.0481797731839695\n",
      "      mean_raw_obs_processing_ms: 0.13613510322380257\n",
      "  experiment_id: d827d2ca00bf4e0b8fafd562c0424864\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.4182109832763672\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.5579537386220775e-16\n",
      "          policy_loss: 2.24304201879022e-08\n",
      "          total_loss: 2208.241943359375\n",
      "          vf_explained_var: -7.152557435219364e-10\n",
      "          vf_loss: 2208.241943359375\n",
      "    num_agent_steps_sampled: 1000\n",
      "    num_agent_steps_trained: 1000\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.900000000000002\n",
      "    ram_util_percent: 49.94\n",
      "  pid: 31949\n",
      "  policy_reward_max:\n",
      "    firm: 46.72199987287698\n",
      "  policy_reward_mean:\n",
      "    firm: 46.72199987287698\n",
      "  policy_reward_min:\n",
      "    firm: 46.72199987287698\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09675673790625879\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.11055262295992582\n",
      "    mean_inference_ms: 1.1611215837232836\n",
      "    mean_raw_obs_processing_ms: 0.1497730746731296\n",
      "  time_since_restore: 3.0597951412200928\n",
      "  time_this_iter_s: 3.0597951412200928\n",
      "  time_total_s: 3.0597951412200928\n",
      "  timers:\n",
      "    learn_throughput: 8138.354\n",
      "    learn_time_ms: 122.875\n",
      "    sample_throughput: 652.907\n",
      "    sample_time_ms: 1531.611\n",
      "  timestamp: 1630715341\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: 0e084_00005\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.0/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/0 GPUs, 0.0/10.89 GiB heap, 0.0/5.44 GiB objects<br>Result logdir: /Users/matiascovarrubias/ray_results/native_1_firms_townsend_test_Sep3_PPO<br>Number of trials: 6/6 (2 RUNNING, 4 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_len_mean</th><th style=\"text-align: right;\">  episodes_this_iter</th><th style=\"text-align: right;\">  num_healthy_workers</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_townsend_0e084_00004</td><td>RUNNING   </td><td>                   </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                     </td></tr>\n",
       "<tr><td>PPO_townsend_0e084_00005</td><td>RUNNING   </td><td>192.168.1.202:31949</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.0598 </td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">   46.722</td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_0e084_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         7.50153</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">-1996.93 </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_0e084_00001</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         7.40978</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\"> -485.797</td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_0e084_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         7.42142</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\"> -546.731</td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_0e084_00003</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         7.49402</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\"> -973.178</td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_townsend_0e084_00004:\n",
      "  agent_timesteps_total: 1000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: -74.82892231461972\n",
      "    discounted_rewards_mean: -74.82892231461972\n",
      "    discounted_rewards_min: -74.82892231461972\n",
      "  date: 2021-09-03_20-29-01\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - -1631.4598996105299\n",
      "  episode_reward_mean: -1631.4598996105299\n",
      "  episode_reward_min:\n",
      "  - -1631.4598996105299\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: 5.86236113684236\n",
      "      discounted_rewards_mean: 5.86236113684236\n",
      "      discounted_rewards_min: 5.86236113684236\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -869.7624682867555\n",
      "    episode_reward_mean: -869.7624682867555\n",
      "    episode_reward_min:\n",
      "    - -869.7624682867555\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -869.7624682867555\n",
      "      policy_firm_reward:\n",
      "      - - -869.7624682867555\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -869.7624682867555\n",
      "    policy_reward_mean:\n",
      "      firm: -869.7624682867555\n",
      "    policy_reward_min:\n",
      "      firm: -869.7624682867555\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.08936528559331294\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10204410457706356\n",
      "      mean_inference_ms: 1.0584577337487953\n",
      "      mean_raw_obs_processing_ms: 0.13552845774830638\n",
      "  experiment_id: 8b201cde48e840459bb333c64cb3bf3d\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.4124112129211426\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.4158454328744475e-16\n",
      "          policy_loss: 1.763915946639827e-08\n",
      "          total_loss: 7317.63818359375\n",
      "          vf_explained_var: -9.357929542375132e-09\n",
      "          vf_loss: 7317.63818359375\n",
      "    num_agent_steps_sampled: 1000\n",
      "    num_agent_steps_trained: 1000\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.86\n",
      "    ram_util_percent: 49.94\n",
      "  pid: 31951\n",
      "  policy_reward_max:\n",
      "    firm: -1631.4598996105299\n",
      "  policy_reward_mean:\n",
      "    firm: -1631.4598996105299\n",
      "  policy_reward_min:\n",
      "    firm: -1631.4598996105299\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09718927351030318\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.11001481161965476\n",
      "    mean_inference_ms: 1.155057510772309\n",
      "    mean_raw_obs_processing_ms: 0.15196695432558166\n",
      "  time_since_restore: 3.090830087661743\n",
      "  time_this_iter_s: 3.090830087661743\n",
      "  time_total_s: 3.090830087661743\n",
      "  timers:\n",
      "    learn_throughput: 6649.836\n",
      "    learn_time_ms: 150.38\n",
      "    sample_throughput: 654.385\n",
      "    sample_time_ms: 1528.153\n",
      "  timestamp: 1630715341\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: 0e084_00004\n",
      "  \n",
      "Result for PPO_townsend_0e084_00005:\n",
      "  agent_timesteps_total: 2000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: 53.290731408213716\n",
      "    discounted_rewards_mean: 48.619033291816734\n",
      "    discounted_rewards_min: 43.94733517541976\n",
      "  date: 2021-09-03_20-29-04\n",
      "  done: true\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - 185.66314887579682\n",
      "  episode_reward_mean: 116.1925743743369\n",
      "  episode_reward_min:\n",
      "  - 46.72199987287698\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: 2.7165407666299055\n",
      "      discounted_rewards_mean: 2.7165407666299055\n",
      "      discounted_rewards_min: 2.7165407666299055\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -1658.7377095337379\n",
      "    episode_reward_mean: -1658.7377095337379\n",
      "    episode_reward_min:\n",
      "    - -1658.7377095337379\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -1658.7377095337379\n",
      "      policy_firm_reward:\n",
      "      - - -1658.7377095337379\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -1658.7377095337379\n",
      "    policy_reward_mean:\n",
      "      firm: -1658.7377095337379\n",
      "    policy_reward_min:\n",
      "      firm: -1658.7377095337379\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09150292979425814\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10494742615112122\n",
      "      mean_inference_ms: 1.0752913833915563\n",
      "      mean_raw_obs_processing_ms: 0.13776268737426942\n",
      "  experiment_id: d827d2ca00bf4e0b8fafd562c0424864\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.1\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.4667085409164429\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 5.684341759025859e-16\n",
      "          policy_loss: -9.841919279551803e-09\n",
      "          total_loss: 1108.947998046875\n",
      "          vf_explained_var: -1.1920929521291868e-10\n",
      "          vf_loss: 1108.947998046875\n",
      "    num_agent_steps_sampled: 2000\n",
      "    num_agent_steps_trained: 2000\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.275\n",
      "    ram_util_percent: 50.25\n",
      "  pid: 31949\n",
      "  policy_reward_max:\n",
      "    firm: 185.66314887579682\n",
      "  policy_reward_mean:\n",
      "    firm: 116.1925743743369\n",
      "  policy_reward_min:\n",
      "    firm: 46.72199987287698\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09549785129951113\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10891387896864899\n",
      "    mean_inference_ms: 1.149672195537093\n",
      "    mean_raw_obs_processing_ms: 0.14850825242277782\n",
      "  time_since_restore: 6.1445839405059814\n",
      "  time_this_iter_s: 3.0847887992858887\n",
      "  time_total_s: 6.1445839405059814\n",
      "  timers:\n",
      "    learn_throughput: 7266.702\n",
      "    learn_time_ms: 137.614\n",
      "    sample_throughput: 668.012\n",
      "    sample_time_ms: 1496.978\n",
      "  timestamp: 1630715344\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: 0e084_00005\n",
      "  \n",
      "Result for PPO_townsend_0e084_00004:\n",
      "  agent_timesteps_total: 2000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: 84.08414562670596\n",
      "    discounted_rewards_mean: 4.627611656043122\n",
      "    discounted_rewards_min: -74.82892231461972\n",
      "  date: 2021-09-03_20-29-04\n",
      "  done: true\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - -716.29336488843\n",
      "  episode_reward_mean: -1173.8766322494798\n",
      "  episode_reward_min:\n",
      "  - -1631.4598996105299\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: -23.18849060492807\n",
      "      discounted_rewards_mean: -23.18849060492807\n",
      "      discounted_rewards_min: -23.18849060492807\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -1578.6629130078347\n",
      "    episode_reward_mean: -1578.6629130078347\n",
      "    episode_reward_min:\n",
      "    - -1578.6629130078347\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -1578.6629130078347\n",
      "      policy_firm_reward:\n",
      "      - - -1578.6629130078347\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -1578.6629130078347\n",
      "    policy_reward_mean:\n",
      "      firm: -1578.6629130078347\n",
      "    policy_reward_min:\n",
      "      firm: -1578.6629130078347\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.0911245103003918\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10439206933093512\n",
      "      mean_inference_ms: 1.0824589536286544\n",
      "      mean_raw_obs_processing_ms: 0.13906916399588293\n",
      "  experiment_id: 8b201cde48e840459bb333c64cb3bf3d\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.1\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.379226565361023\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.410605161294634e-16\n",
      "          policy_loss: 8.480834878810128e-08\n",
      "          total_loss: 3865.283935546875\n",
      "          vf_explained_var: -7.3909758313561724e-09\n",
      "          vf_loss: 3865.2841796875\n",
      "    num_agent_steps_sampled: 2000\n",
      "    num_agent_steps_trained: 2000\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.450000000000003\n",
      "    ram_util_percent: 50.25\n",
      "  pid: 31951\n",
      "  policy_reward_max:\n",
      "    firm: -716.29336488843\n",
      "  policy_reward_mean:\n",
      "    firm: -1173.8766322494798\n",
      "  policy_reward_min:\n",
      "    firm: -1631.4598996105299\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09601020614254652\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10880874458135958\n",
      "    mean_inference_ms: 1.1482411737779412\n",
      "    mean_raw_obs_processing_ms: 0.15022614098712866\n",
      "  time_since_restore: 6.191161155700684\n",
      "  time_this_iter_s: 3.1003310680389404\n",
      "  time_total_s: 6.191161155700684\n",
      "  timers:\n",
      "    learn_throughput: 6775.815\n",
      "    learn_time_ms: 147.584\n",
      "    sample_throughput: 665.418\n",
      "    sample_time_ms: 1502.814\n",
      "  timestamp: 1630715344\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: 0e084_00004\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.1/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/4 CPUs, 0/0 GPUs, 0.0/10.89 GiB heap, 0.0/5.44 GiB objects<br>Result logdir: /Users/matiascovarrubias/ray_results/native_1_firms_townsend_test_Sep3_PPO<br>Number of trials: 6/6 (6 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_len_mean</th><th style=\"text-align: right;\">  episodes_this_iter</th><th style=\"text-align: right;\">  num_healthy_workers</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_townsend_0e084_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         7.50153</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">-1996.93 </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_0e084_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         7.40978</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\"> -485.797</td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_0e084_00002</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         7.42142</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\"> -546.731</td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_0e084_00003</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         7.49402</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\"> -973.178</td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_0e084_00004</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         6.19116</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">-1173.88 </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_0e084_00005</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         6.14458</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">  116.193</td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=31951)\u001b[0m 2021-09-03 20:29:05,432\tERROR worker.py:382 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=31951)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=31951)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=31951)\u001b[0m   File \"python/ray/_raylet.pyx\", line 495, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=31951)\u001b[0m   File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=31951)\u001b[0m   File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=31951)\u001b[0m   File \"/Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=31951)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=31951)\u001b[0m   File \"/Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/actor.py\", line 1001, in __ray_terminate__\n",
      "\u001b[2m\u001b[36m(pid=31951)\u001b[0m     ray.actor.exit_actor()\n",
      "\u001b[2m\u001b[36m(pid=31951)\u001b[0m   File \"/Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/actor.py\", line 1077, in exit_actor\n",
      "\u001b[2m\u001b[36m(pid=31951)\u001b[0m     raise exit\n",
      "\u001b[2m\u001b[36m(pid=31951)\u001b[0m SystemExit: 0\n",
      "\u001b[2m\u001b[36m(pid=31951)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=31951)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=31951)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=31951)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=31951)\u001b[0m   File \"python/ray/_raylet.pyx\", line 599, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=31951)\u001b[0m   File \"python/ray/_raylet.pyx\", line 451, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=31951)\u001b[0m   File \"python/ray/includes/libcoreworker.pxi\", line 33, in ray._raylet.ProfileEvent.__exit__\n",
      "\u001b[2m\u001b[36m(pid=31951)\u001b[0m   File \"/Users/matiascovarrubias/.pyenv/versions/3.8.7/lib/python3.8/traceback.py\", line 167, in format_exc\n",
      "\u001b[2m\u001b[36m(pid=31951)\u001b[0m     return \"\".join(format_exception(*sys.exc_info(), limit=limit, chain=chain))\n",
      "\u001b[2m\u001b[36m(pid=31951)\u001b[0m   File \"/Users/matiascovarrubias/.pyenv/versions/3.8.7/lib/python3.8/traceback.py\", line 120, in format_exception\n",
      "\u001b[2m\u001b[36m(pid=31951)\u001b[0m     return list(TracebackException(\n",
      "\u001b[2m\u001b[36m(pid=31951)\u001b[0m   File \"/Users/matiascovarrubias/.pyenv/versions/3.8.7/lib/python3.8/traceback.py\", line 508, in __init__\n",
      "\u001b[2m\u001b[36m(pid=31951)\u001b[0m     self.stack = StackSummary.extract(\n",
      "\u001b[2m\u001b[36m(pid=31951)\u001b[0m   File \"/Users/matiascovarrubias/.pyenv/versions/3.8.7/lib/python3.8/traceback.py\", line 366, in extract\n",
      "\u001b[2m\u001b[36m(pid=31951)\u001b[0m     f.line\n",
      "\u001b[2m\u001b[36m(pid=31951)\u001b[0m   File \"/Users/matiascovarrubias/.pyenv/versions/3.8.7/lib/python3.8/traceback.py\", line 288, in line\n",
      "\u001b[2m\u001b[36m(pid=31951)\u001b[0m     self._line = linecache.getline(self.filename, self.lineno).strip()\n",
      "\u001b[2m\u001b[36m(pid=31951)\u001b[0m   File \"/Users/matiascovarrubias/.pyenv/versions/3.8.7/lib/python3.8/linecache.py\", line 15, in getline\n",
      "\u001b[2m\u001b[36m(pid=31951)\u001b[0m     def getline(filename, lineno, module_globals=None):\n",
      "\u001b[2m\u001b[36m(pid=31951)\u001b[0m   File \"/Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/worker.py\", line 379, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=31951)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=31951)\u001b[0m SystemExit: 1\n",
      "2021-09-03 20:29:05,534\tINFO tune.py:549 -- Total run time: 24.30 seconds (23.50 seconds for the tuning loop).\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.9/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/4 CPUs, 0/0 GPUs, 0.0/10.89 GiB heap, 0.0/5.44 GiB objects<br>Result logdir: /Users/matiascovarrubias/ray_results/native_2_firms_townsend_test_Sep3_PPO<br>Number of trials: 6/6 (5 PENDING, 1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_townsend_1c945_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_1c945_00001</td><td>PENDING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_1c945_00002</td><td>PENDING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_1c945_00003</td><td>PENDING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_1c945_00004</td><td>PENDING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_1c945_00005</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(pid=31957)\u001b[0m WARNING:tensorflow:From /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=31957)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=31957)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=31953)\u001b[0m WARNING:tensorflow:From /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=31953)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=31953)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=31979)\u001b[0m WARNING:tensorflow:From /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=31979)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=31979)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=31980)\u001b[0m WARNING:tensorflow:From /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=31980)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=31980)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=31957)\u001b[0m 2021-09-03 20:29:09,092\tINFO trainer.py:694 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=31953)\u001b[0m 2021-09-03 20:29:09,118\tINFO trainer.py:694 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=31957)\u001b[0m 2021-09-03 20:29:09,243\tWARNING deprecation.py:33 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=31953)\u001b[0m 2021-09-03 20:29:09,267\tWARNING deprecation.py:33 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=31979)\u001b[0m 2021-09-03 20:29:09,661\tINFO trainer.py:694 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=31980)\u001b[0m 2021-09-03 20:29:09,680\tINFO trainer.py:694 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=31979)\u001b[0m 2021-09-03 20:29:09,820\tWARNING deprecation.py:33 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=31980)\u001b[0m 2021-09-03 20:29:09,840\tWARNING deprecation.py:33 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.4/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/4 CPUs, 0/0 GPUs, 0.0/10.89 GiB heap, 0.0/5.44 GiB objects<br>Result logdir: /Users/matiascovarrubias/ray_results/native_2_firms_townsend_test_Sep3_PPO<br>Number of trials: 6/6 (2 PENDING, 4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_townsend_1c945_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_1c945_00001</td><td>RUNNING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_1c945_00002</td><td>RUNNING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_1c945_00003</td><td>RUNNING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_1c945_00004</td><td>PENDING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_1c945_00005</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=31957)\u001b[0m 2021-09-03 20:29:11,126\tWARNING deprecation.py:33 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=31953)\u001b[0m 2021-09-03 20:29:11,187\tWARNING deprecation.py:33 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=31979)\u001b[0m 2021-09-03 20:29:11,753\tWARNING deprecation.py:33 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=31980)\u001b[0m 2021-09-03 20:29:11,790\tWARNING deprecation.py:33 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_townsend_1c945_00000:\n",
      "  agent_timesteps_total: 2000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: 101.6068345064938\n",
      "    discounted_rewards_mean: 101.6068345064938\n",
      "    discounted_rewards_min: 101.6068345064938\n",
      "  date: 2021-09-03_20-29-13\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - -2086.9566691450796\n",
      "  episode_reward_mean: -2086.9566691450796\n",
      "  episode_reward_min:\n",
      "  - -2086.9566691450796\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: 10.85118086884955\n",
      "      discounted_rewards_mean: 10.85118086884955\n",
      "      discounted_rewards_min: 10.85118086884955\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -1955.477668708667\n",
      "    episode_reward_mean: -1955.477668708667\n",
      "    episode_reward_min:\n",
      "    - -1955.477668708667\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -1955.477668708667\n",
      "      policy_firm_reward:\n",
      "      - - -1136.9395571293035\n",
      "      - - -818.5381115793602\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -818.5381115793602\n",
      "    policy_reward_mean:\n",
      "      firm: -977.7388343543319\n",
      "    policy_reward_min:\n",
      "      firm: -1136.9395571293035\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1432416917798998\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.14882511668629222\n",
      "      mean_inference_ms: 1.2055281754378435\n",
      "      mean_raw_obs_processing_ms: 0.22721195316219425\n",
      "  experiment_id: 52752522d2a14ff888863f8db8b5c80c\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.4202657341957092\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0005004665581510466\n",
      "          policy_loss: -1.74790620803833e-05\n",
      "          total_loss: 5881.8642578125\n",
      "          vf_explained_var: 1.788139358804841e-10\n",
      "          vf_loss: 5881.864013671875\n",
      "    num_agent_steps_sampled: 2000\n",
      "    num_agent_steps_trained: 2000\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.049999999999997\n",
      "    ram_util_percent: 51.71666666666667\n",
      "  pid: 31957\n",
      "  policy_reward_max:\n",
      "    firm: -591.4825053927987\n",
      "  policy_reward_mean:\n",
      "    firm: -1043.4783345725386\n",
      "  policy_reward_min:\n",
      "    firm: -1495.4741637522786\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14985167420470155\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14737817076417237\n",
      "    mean_inference_ms: 1.2527305286723776\n",
      "    mean_raw_obs_processing_ms: 0.2360608313348029\n",
      "  time_since_restore: 3.852874994277954\n",
      "  time_this_iter_s: 3.852874994277954\n",
      "  time_total_s: 3.852874994277954\n",
      "  timers:\n",
      "    learn_throughput: 3387.306\n",
      "    learn_time_ms: 295.22\n",
      "    sample_throughput: 554.691\n",
      "    sample_time_ms: 1802.806\n",
      "  timestamp: 1630715353\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: 1c945_00000\n",
      "  \n",
      "Result for PPO_townsend_1c945_00001:\n",
      "  agent_timesteps_total: 2000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: -26.89053576393052\n",
      "    discounted_rewards_mean: -26.89053576393052\n",
      "    discounted_rewards_min: -26.89053576393052\n",
      "  date: 2021-09-03_20-29-13\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - -1822.976638864052\n",
      "  episode_reward_mean: -1822.976638864052\n",
      "  episode_reward_min:\n",
      "  - -1822.976638864052\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: -14.529324832384146\n",
      "      discounted_rewards_mean: -14.529324832384146\n",
      "      discounted_rewards_min: -14.529324832384146\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -2153.9195368716287\n",
      "    episode_reward_mean: -2153.9195368716287\n",
      "    episode_reward_min:\n",
      "    - -2153.9195368716287\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -2153.9195368716287\n",
      "      policy_firm_reward:\n",
      "      - - -1205.9946697518674\n",
      "      - - -947.9248671197627\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -947.9248671197627\n",
      "    policy_reward_mean:\n",
      "      firm: -1076.959768435815\n",
      "    policy_reward_min:\n",
      "      firm: -1205.9946697518674\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14170471366707024\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.14547653846092873\n",
      "      mean_inference_ms: 1.190239137464708\n",
      "      mean_raw_obs_processing_ms: 0.23447645532263148\n",
      "  experiment_id: 23b7f01eba954f19a7dda08b87968775\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.4294484853744507\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0012103000190109867\n",
      "          policy_loss: 0.0002622294705361128\n",
      "          total_loss: 4411.47119140625\n",
      "          vf_explained_var: -2.294778811418041e-09\n",
      "          vf_loss: 4411.470458984375\n",
      "    num_agent_steps_sampled: 2000\n",
      "    num_agent_steps_trained: 2000\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.166666666666668\n",
      "    ram_util_percent: 51.699999999999996\n",
      "  pid: 31953\n",
      "  policy_reward_max:\n",
      "    firm: -582.679923887141\n",
      "  policy_reward_mean:\n",
      "    firm: -911.4883194320248\n",
      "  policy_reward_min:\n",
      "    firm: -1240.2967149769086\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15205079382592507\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15008413827383554\n",
      "    mean_inference_ms: 1.261886421378914\n",
      "    mean_raw_obs_processing_ms: 0.2478800572596349\n",
      "  time_since_restore: 3.9036788940429688\n",
      "  time_this_iter_s: 3.9036788940429688\n",
      "  time_total_s: 3.9036788940429688\n",
      "  timers:\n",
      "    learn_throughput: 3157.962\n",
      "    learn_time_ms: 316.66\n",
      "    sample_throughput: 546.715\n",
      "    sample_time_ms: 1829.106\n",
      "  timestamp: 1630715353\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: 1c945_00001\n",
      "  \n",
      "Result for PPO_townsend_1c945_00003:\n",
      "  agent_timesteps_total: 2000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: -82.52696500280652\n",
      "    discounted_rewards_mean: -82.52696500280652\n",
      "    discounted_rewards_min: -82.52696500280652\n",
      "  date: 2021-09-03_20-29-13\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - -1882.2576989148877\n",
      "  episode_reward_mean: -1882.2576989148877\n",
      "  episode_reward_min:\n",
      "  - -1882.2576989148877\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: -25.01991495784893\n",
      "      discounted_rewards_mean: -25.01991495784893\n",
      "      discounted_rewards_min: -25.01991495784893\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -1923.4400196991394\n",
      "    episode_reward_mean: -1923.4400196991394\n",
      "    episode_reward_min:\n",
      "    - -1923.4400196991394\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -1923.4400196991394\n",
      "      policy_firm_reward:\n",
      "      - - -1120.5950977843152\n",
      "      - - -802.8449219148229\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -802.8449219148229\n",
      "    policy_reward_mean:\n",
      "      firm: -961.720009849569\n",
      "    policy_reward_min:\n",
      "      firm: -1120.5950977843152\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.13796027961906257\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.14580475105034127\n",
      "      mean_inference_ms: 1.1549191279606623\n",
      "      mean_raw_obs_processing_ms: 0.2179615028373726\n",
      "  experiment_id: 308da4e3e212490db3b1ba1db0a6913f\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.3982264399528503\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0015961137833074943\n",
      "          policy_loss: 0.0007001901976764202\n",
      "          total_loss: 3602.79248046875\n",
      "          vf_explained_var: -3.933906533859499e-09\n",
      "          vf_loss: 3602.791259765625\n",
      "    num_agent_steps_sampled: 2000\n",
      "    num_agent_steps_trained: 2000\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.46666666666667\n",
      "    ram_util_percent: 51.9\n",
      "  pid: 31979\n",
      "  policy_reward_max:\n",
      "    firm: -654.2626208901481\n",
      "  policy_reward_mean:\n",
      "    firm: -941.128849457444\n",
      "  policy_reward_min:\n",
      "    firm: -1227.9950780247398\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14737483623859052\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15154037323150482\n",
      "    mean_inference_ms: 1.2644068463579878\n",
      "    mean_raw_obs_processing_ms: 0.2558776786872795\n",
      "  time_since_restore: 3.8724050521850586\n",
      "  time_this_iter_s: 3.8724050521850586\n",
      "  time_total_s: 3.8724050521850586\n",
      "  timers:\n",
      "    learn_throughput: 2950.627\n",
      "    learn_time_ms: 338.911\n",
      "    sample_throughput: 544.28\n",
      "    sample_time_ms: 1837.289\n",
      "  timestamp: 1630715353\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: 1c945_00003\n",
      "  \n",
      "Result for PPO_townsend_1c945_00002:\n",
      "  agent_timesteps_total: 2000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: 103.50649672546638\n",
      "    discounted_rewards_mean: 103.50649672546638\n",
      "    discounted_rewards_min: 103.50649672546638\n",
      "  date: 2021-09-03_20-29-13\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - -1318.830962598973\n",
      "  episode_reward_mean: -1318.830962598973\n",
      "  episode_reward_min:\n",
      "  - -1318.830962598973\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: -33.73131827077619\n",
      "      discounted_rewards_mean: -33.73131827077619\n",
      "      discounted_rewards_min: -33.73131827077619\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -2444.3400620770035\n",
      "    episode_reward_mean: -2444.3400620770035\n",
      "    episode_reward_min:\n",
      "    - -2444.3400620770035\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -2444.3400620770035\n",
      "      policy_firm_reward:\n",
      "      - - -1485.724609124198\n",
      "      - - -958.6154529527967\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -958.6154529527967\n",
      "    policy_reward_mean:\n",
      "      firm: -1222.1700310384972\n",
      "    policy_reward_min:\n",
      "      firm: -1485.724609124198\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.13813438949051438\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.14250404708511705\n",
      "      mean_inference_ms: 1.1575226778988834\n",
      "      mean_raw_obs_processing_ms: 0.2230502270556592\n",
      "  experiment_id: fd4b52b1bde3421ab4695c22074b32bc\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.3993116617202759\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.001705597154796223\n",
      "          policy_loss: 0.00036346164415590465\n",
      "          total_loss: 3345.1798095703125\n",
      "          vf_explained_var: -4.440546153006153e-09\n",
      "          vf_loss: 3345.1790771484375\n",
      "    num_agent_steps_sampled: 2000\n",
      "    num_agent_steps_trained: 2000\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.25\n",
      "    ram_util_percent: 51.9\n",
      "  pid: 31980\n",
      "  policy_reward_max:\n",
      "    firm: -430.7374926314127\n",
      "  policy_reward_mean:\n",
      "    firm: -659.4154812994867\n",
      "  policy_reward_min:\n",
      "    firm: -888.0934699675607\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14792026935162006\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15234899568510105\n",
      "    mean_inference_ms: 1.2743337290151255\n",
      "    mean_raw_obs_processing_ms: 0.2588165866268741\n",
      "  time_since_restore: 3.8876051902770996\n",
      "  time_this_iter_s: 3.8876051902770996\n",
      "  time_total_s: 3.8876051902770996\n",
      "  timers:\n",
      "    learn_throughput: 3034.994\n",
      "    learn_time_ms: 329.49\n",
      "    sample_throughput: 540.401\n",
      "    sample_time_ms: 1850.476\n",
      "  timestamp: 1630715353\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: 1c945_00002\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.9/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/4 CPUs, 0/0 GPUs, 0.0/10.89 GiB heap, 0.0/5.44 GiB objects<br>Result logdir: /Users/matiascovarrubias/ray_results/native_2_firms_townsend_test_Sep3_PPO<br>Number of trials: 6/6 (2 PENDING, 4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_len_mean</th><th style=\"text-align: right;\">  episodes_this_iter</th><th style=\"text-align: right;\">  num_healthy_workers</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_townsend_1c945_00000</td><td>RUNNING </td><td>192.168.1.202:31957</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.85287</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">-2086.96</td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_1c945_00001</td><td>RUNNING </td><td>192.168.1.202:31953</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.90368</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">-1822.98</td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_1c945_00002</td><td>RUNNING </td><td>192.168.1.202:31980</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.88761</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">-1318.83</td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_1c945_00003</td><td>RUNNING </td><td>192.168.1.202:31979</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.87241</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">-1882.26</td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_1c945_00004</td><td>PENDING </td><td>                   </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                     </td></tr>\n",
       "<tr><td>PPO_townsend_1c945_00005</td><td>PENDING </td><td>                   </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_townsend_1c945_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: 194.1375223957281\n",
      "    discounted_rewards_mean: 147.87217845111095\n",
      "    discounted_rewards_min: 101.6068345064938\n",
      "  date: 2021-09-03_20-29-16\n",
      "  done: true\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - -1651.8128213807975\n",
      "  episode_reward_mean: -1869.3847452629384\n",
      "  episode_reward_min:\n",
      "  - -2086.9566691450796\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: 32.79935156341983\n",
      "      discounted_rewards_mean: 32.79935156341983\n",
      "      discounted_rewards_min: 32.79935156341983\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -2127.851473775402\n",
      "    episode_reward_mean: -2127.851473775402\n",
      "    episode_reward_min:\n",
      "    - -2127.851473775402\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -2127.851473775402\n",
      "      policy_firm_reward:\n",
      "      - - -757.8520292675452\n",
      "      - - -1369.9994445078596\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -757.8520292675452\n",
      "    policy_reward_mean:\n",
      "      firm: -1063.9257368877024\n",
      "    policy_reward_min:\n",
      "      firm: -1369.9994445078596\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1405179053768404\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.14483469954018352\n",
      "      mean_inference_ms: 1.178195391935685\n",
      "      mean_raw_obs_processing_ms: 0.2275948998690962\n",
      "  experiment_id: 52752522d2a14ff888863f8db8b5c80c\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.1\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.4238741993904114\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0004154066555204068\n",
      "          policy_loss: 5.752593278884888e-05\n",
      "          total_loss: 5098.623046875\n",
      "          vf_explained_var: -4.1723252630632146e-10\n",
      "          vf_loss: 5098.62255859375\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.2\n",
      "    ram_util_percent: 52.580000000000005\n",
      "  pid: 31957\n",
      "  policy_reward_max:\n",
      "    firm: -498.05408385407685\n",
      "  policy_reward_mean:\n",
      "    firm: -934.6923726314678\n",
      "  policy_reward_min:\n",
      "    firm: -1495.4741637522786\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14624009881381073\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14443959021847413\n",
      "    mean_inference_ms: 1.2270810106843588\n",
      "    mean_raw_obs_processing_ms: 0.2309819026276262\n",
      "  time_since_restore: 7.473642826080322\n",
      "  time_this_iter_s: 3.620767831802368\n",
      "  time_total_s: 7.473642826080322\n",
      "  timers:\n",
      "    learn_throughput: 3489.361\n",
      "    learn_time_ms: 286.585\n",
      "    sample_throughput: 579.949\n",
      "    sample_time_ms: 1724.289\n",
      "  timestamp: 1630715356\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: 1c945_00000\n",
      "  \n",
      "Result for PPO_townsend_1c945_00001:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: -26.89053576393052\n",
      "    discounted_rewards_mean: -39.4289065883154\n",
      "    discounted_rewards_min: -51.96727741270028\n",
      "  date: 2021-09-03_20-29-16\n",
      "  done: true\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - -1822.976638864052\n",
      "  episode_reward_mean: -2028.399726557605\n",
      "  episode_reward_min:\n",
      "  - -2233.822814251158\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: -14.314678096327853\n",
      "      discounted_rewards_mean: -14.314678096327853\n",
      "      discounted_rewards_min: -14.314678096327853\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -2795.0490527056354\n",
      "    episode_reward_mean: -2795.0490527056354\n",
      "    episode_reward_min:\n",
      "    - -2795.0490527056354\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -2795.0490527056354\n",
      "      policy_firm_reward:\n",
      "      - - -1528.3326081511373\n",
      "      - - -1266.716444554504\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -1266.716444554504\n",
      "    policy_reward_mean:\n",
      "      firm: -1397.5245263528207\n",
      "    policy_reward_min:\n",
      "      firm: -1528.3326081511373\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14074405153532854\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.14411527356286458\n",
      "      mean_inference_ms: 1.17337209233518\n",
      "      mean_raw_obs_processing_ms: 0.23858800999585655\n",
      "  experiment_id: 23b7f01eba954f19a7dda08b87968775\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.1\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.3880823850631714\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0006595557788388859\n",
      "          policy_loss: 4.6737492084503174e-05\n",
      "          total_loss: 7053.979736328125\n",
      "          vf_explained_var: -4.5597552400522545e-09\n",
      "          vf_loss: 7053.978515625\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.3\n",
      "    ram_util_percent: 52.580000000000005\n",
      "  pid: 31953\n",
      "  policy_reward_max:\n",
      "    firm: -409.721105539651\n",
      "  policy_reward_mean:\n",
      "    firm: -1014.199863278802\n",
      "  policy_reward_min:\n",
      "    firm: -1824.1017087115074\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14809274439614234\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14671354169302864\n",
      "    mean_inference_ms: 1.2360240664102782\n",
      "    mean_raw_obs_processing_ms: 0.24139775779974787\n",
      "  time_since_restore: 7.5456719398498535\n",
      "  time_this_iter_s: 3.6419930458068848\n",
      "  time_total_s: 7.5456719398498535\n",
      "  timers:\n",
      "    learn_throughput: 3435.753\n",
      "    learn_time_ms: 291.057\n",
      "    sample_throughput: 573.077\n",
      "    sample_time_ms: 1744.967\n",
      "  timestamp: 1630715356\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: 1c945_00001\n",
      "  \n",
      "Result for PPO_townsend_1c945_00003:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: 180.49378338653344\n",
      "    discounted_rewards_mean: 48.98340919186346\n",
      "    discounted_rewards_min: -82.52696500280652\n",
      "  date: 2021-09-03_20-29-17\n",
      "  done: true\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - 662.0080985680778\n",
      "  episode_reward_mean: -610.124800173405\n",
      "  episode_reward_min:\n",
      "  - -1882.2576989148877\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: 26.751470321545252\n",
      "      discounted_rewards_mean: 26.751470321545252\n",
      "      discounted_rewards_min: 26.751470321545252\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -1954.3479769933674\n",
      "    episode_reward_mean: -1954.3479769933674\n",
      "    episode_reward_min:\n",
      "    - -1954.3479769933674\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -1954.3479769933674\n",
      "      policy_firm_reward:\n",
      "      - - -1216.7688912546867\n",
      "      - - -737.5790857386756\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -737.5790857386756\n",
      "    policy_reward_mean:\n",
      "      firm: -977.1739884966812\n",
      "    policy_reward_min:\n",
      "      firm: -1216.7688912546867\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1387681918165673\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.14624769600673296\n",
      "      mean_inference_ms: 1.1671754731231185\n",
      "      mean_raw_obs_processing_ms: 0.23020165732715916\n",
      "  experiment_id: 308da4e3e212490db3b1ba1db0a6913f\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.1\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.4345955848693848\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0005813995376230281\n",
      "          policy_loss: 0.00018838606774806976\n",
      "          total_loss: 1919.7787475585938\n",
      "          vf_explained_var: -1.1026859425555813e-09\n",
      "          vf_loss: 1919.7784423828125\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.260000000000005\n",
      "    ram_util_percent: 52.660000000000004\n",
      "  pid: 31979\n",
      "  policy_reward_max:\n",
      "    firm: 348.3924089457414\n",
      "  policy_reward_mean:\n",
      "    firm: -305.0624000867018\n",
      "  policy_reward_min:\n",
      "    firm: -1227.9950780247398\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14417948722451315\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14807666757632193\n",
      "    mean_inference_ms: 1.2400313351469296\n",
      "    mean_raw_obs_processing_ms: 0.24839997537184758\n",
      "  time_since_restore: 7.5556440353393555\n",
      "  time_this_iter_s: 3.683238983154297\n",
      "  time_total_s: 7.5556440353393555\n",
      "  timers:\n",
      "    learn_throughput: 3328.833\n",
      "    learn_time_ms: 300.406\n",
      "    sample_throughput: 569.896\n",
      "    sample_time_ms: 1754.705\n",
      "  timestamp: 1630715357\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: 1c945_00003\n",
      "  \n",
      "Result for PPO_townsend_1c945_00002:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: 103.50649672546638\n",
      "    discounted_rewards_mean: 78.55910041714208\n",
      "    discounted_rewards_min: 53.61170410881778\n",
      "  date: 2021-09-03_20-29-17\n",
      "  done: true\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - -1298.299583285397\n",
      "  episode_reward_mean: -1308.565272942185\n",
      "  episode_reward_min:\n",
      "  - -1318.830962598973\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: -0.8910582026733511\n",
      "      discounted_rewards_mean: -0.8910582026733511\n",
      "      discounted_rewards_min: -0.8910582026733511\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -2959.329588936324\n",
      "    episode_reward_mean: -2959.329588936324\n",
      "    episode_reward_min:\n",
      "    - -2959.329588936324\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -2959.329588936324\n",
      "      policy_firm_reward:\n",
      "      - - -1698.7887593642913\n",
      "      - - -1260.5408295720342\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -1260.5408295720342\n",
      "    policy_reward_mean:\n",
      "      firm: -1479.6647944681627\n",
      "    policy_reward_min:\n",
      "      firm: -1698.7887593642913\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.14046810079609853\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.14432283236586052\n",
      "      mean_inference_ms: 1.175638319908649\n",
      "      mean_raw_obs_processing_ms: 0.23318373638650647\n",
      "  experiment_id: fd4b52b1bde3421ab4695c22074b32bc\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.1\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.4306191802024841\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0006285801064224401\n",
      "          policy_loss: 0.00013720616698265076\n",
      "          total_loss: 3815.89453125\n",
      "          vf_explained_var: -2.1755695023273347e-09\n",
      "          vf_loss: 3815.89404296875\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.44\n",
      "    ram_util_percent: 52.660000000000004\n",
      "  pid: 31980\n",
      "  policy_reward_max:\n",
      "    firm: -430.7374926314127\n",
      "  policy_reward_mean:\n",
      "    firm: -654.2826364710908\n",
      "  policy_reward_min:\n",
      "    firm: -888.0934699675607\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14486583201558617\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1487447762714513\n",
      "    mean_inference_ms: 1.248389292272257\n",
      "    mean_raw_obs_processing_ms: 0.2528644361450364\n",
      "  time_since_restore: 7.660112142562866\n",
      "  time_this_iter_s: 3.7725069522857666\n",
      "  time_total_s: 7.660112142562866\n",
      "  timers:\n",
      "    learn_throughput: 3082.714\n",
      "    learn_time_ms: 324.389\n",
      "    sample_throughput: 565.306\n",
      "    sample_time_ms: 1768.952\n",
      "  timestamp: 1630715357\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: 1c945_00002\n",
      "  \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(pid=32029)\u001b[0m WARNING:tensorflow:From /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=32029)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=32029)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=32030)\u001b[0m WARNING:tensorflow:From /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=32030)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=32030)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=32029)\u001b[0m 2021-09-03 20:29:21,095\tINFO trainer.py:694 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=32030)\u001b[0m 2021-09-03 20:29:21,136\tINFO trainer.py:694 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=32029)\u001b[0m 2021-09-03 20:29:21,239\tWARNING deprecation.py:33 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=32030)\u001b[0m 2021-09-03 20:29:21,279\tWARNING deprecation.py:33 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=32029)\u001b[0m 2021-09-03 20:29:23,042\tWARNING deprecation.py:33 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=32030)\u001b[0m 2021-09-03 20:29:23,072\tWARNING deprecation.py:33 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_townsend_1c945_00005:\n",
      "  agent_timesteps_total: 2000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: 7.185957299512204\n",
      "    discounted_rewards_mean: 7.185957299512204\n",
      "    discounted_rewards_min: 7.185957299512204\n",
      "  date: 2021-09-03_20-29-24\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - -2237.204874313453\n",
      "  episode_reward_mean: -2237.204874313453\n",
      "  episode_reward_min:\n",
      "  - -2237.204874313453\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: -59.49882939152509\n",
      "      discounted_rewards_mean: -59.49882939152509\n",
      "      discounted_rewards_min: -59.49882939152509\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -2330.474889733412\n",
      "    episode_reward_mean: -2330.474889733412\n",
      "    episode_reward_min:\n",
      "    - -2330.474889733412\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -2330.474889733412\n",
      "      policy_firm_reward:\n",
      "      - - -1583.7155935003796\n",
      "      - - -746.7592962330326\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -746.7592962330326\n",
      "    policy_reward_mean:\n",
      "      firm: -1165.237444866706\n",
      "    policy_reward_min:\n",
      "      firm: -1583.7155935003796\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.13865838636766067\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.13929551893419082\n",
      "      mean_inference_ms: 1.1215900684093738\n",
      "      mean_raw_obs_processing_ms: 0.23423851310432733\n",
      "  experiment_id: 93386e6e40484bf1a269cf9601e459ad\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.4391154646873474\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0016862117918209592\n",
      "          policy_loss: 0.0007825076463632286\n",
      "          total_loss: 2622.4134521484375\n",
      "          vf_explained_var: -4.917383389368979e-09\n",
      "          vf_loss: 2622.412353515625\n",
      "    num_agent_steps_sampled: 2000\n",
      "    num_agent_steps_trained: 2000\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.633333333333336\n",
      "    ram_util_percent: 51.0\n",
      "  pid: 32029\n",
      "  policy_reward_max:\n",
      "    firm: -760.6987403692893\n",
      "  policy_reward_mean:\n",
      "    firm: -1118.6024371567269\n",
      "  policy_reward_min:\n",
      "    firm: -1476.5061339441645\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14416487900527208\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.14017416642500566\n",
      "    mean_inference_ms: 1.1749465267855923\n",
      "    mean_raw_obs_processing_ms: 0.24131890181656723\n",
      "  time_since_restore: 3.6280930042266846\n",
      "  time_this_iter_s: 3.6280930042266846\n",
      "  time_total_s: 3.6280930042266846\n",
      "  timers:\n",
      "    learn_throughput: 4084.799\n",
      "    learn_time_ms: 244.81\n",
      "    sample_throughput: 581.964\n",
      "    sample_time_ms: 1718.32\n",
      "  timestamp: 1630715364\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: 1c945_00005\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.4/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/0 GPUs, 0.0/10.89 GiB heap, 0.0/5.44 GiB objects<br>Result logdir: /Users/matiascovarrubias/ray_results/native_2_firms_townsend_test_Sep3_PPO<br>Number of trials: 6/6 (2 RUNNING, 4 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_len_mean</th><th style=\"text-align: right;\">  episodes_this_iter</th><th style=\"text-align: right;\">  num_healthy_workers</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_townsend_1c945_00004</td><td>RUNNING   </td><td>                   </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                     </td></tr>\n",
       "<tr><td>PPO_townsend_1c945_00005</td><td>RUNNING   </td><td>192.168.1.202:32029</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.62809</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">-2237.2  </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_1c945_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         7.47364</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">-1869.38 </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_1c945_00001</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         7.54567</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">-2028.4  </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_1c945_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         7.66011</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">-1308.57 </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_1c945_00003</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         7.55564</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\"> -610.125</td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_townsend_1c945_00004:\n",
      "  agent_timesteps_total: 2000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: 112.78230351062575\n",
      "    discounted_rewards_mean: 112.78230351062575\n",
      "    discounted_rewards_min: 112.78230351062575\n",
      "  date: 2021-09-03_20-29-24\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - -857.8838027266127\n",
      "  episode_reward_mean: -857.8838027266127\n",
      "  episode_reward_min:\n",
      "  - -857.8838027266127\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: -40.27438744214976\n",
      "      discounted_rewards_mean: -40.27438744214976\n",
      "      discounted_rewards_min: -40.27438744214976\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -2569.337091044352\n",
      "    episode_reward_mean: -2569.337091044352\n",
      "    episode_reward_min:\n",
      "    - -2569.337091044352\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -2569.337091044352\n",
      "      policy_firm_reward:\n",
      "      - - -1799.1167165880354\n",
      "      - - -770.2203744563135\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -770.2203744563135\n",
      "    policy_reward_mean:\n",
      "      firm: -1284.6685455221746\n",
      "    policy_reward_min:\n",
      "      firm: -1799.1167165880354\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.13536030238682217\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.1383954352074927\n",
      "      mean_inference_ms: 1.1163077035269418\n",
      "      mean_raw_obs_processing_ms: 0.2228687335918476\n",
      "  experiment_id: 0f7d9eeccd47449386b3f1b8d6276f9c\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.4479317665100098\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00196954421699057\n",
      "          policy_loss: 0.0007828269153833389\n",
      "          total_loss: 2743.2891845703125\n",
      "          vf_explained_var: -1.1920929521291868e-10\n",
      "          vf_loss: 2743.2879638671875\n",
      "    num_agent_steps_sampled: 2000\n",
      "    num_agent_steps_trained: 2000\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.55\n",
      "    ram_util_percent: 50.98333333333334\n",
      "  pid: 32030\n",
      "  policy_reward_max:\n",
      "    firm: -72.31350443767032\n",
      "  policy_reward_mean:\n",
      "    firm: -428.9419013633057\n",
      "  policy_reward_min:\n",
      "    firm: -785.5702982889411\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14076413927259265\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1410437630606698\n",
      "    mean_inference_ms: 1.175617719149137\n",
      "    mean_raw_obs_processing_ms: 0.23283158148918953\n",
      "  time_since_restore: 3.6287732124328613\n",
      "  time_this_iter_s: 3.6287732124328613\n",
      "  time_total_s: 3.6287732124328613\n",
      "  timers:\n",
      "    learn_throughput: 3642.482\n",
      "    learn_time_ms: 274.538\n",
      "    sample_throughput: 585.971\n",
      "    sample_time_ms: 1706.57\n",
      "  timestamp: 1630715364\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: 1c945_00004\n",
      "  \n",
      "Result for PPO_townsend_1c945_00005:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: 7.185957299512204\n",
      "    discounted_rewards_mean: -4.426342974598514\n",
      "    discounted_rewards_min: -16.03864324870923\n",
      "  date: 2021-09-03_20-29-28\n",
      "  done: true\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - -2237.204874313453\n",
      "  episode_reward_mean: -2976.0881523290027\n",
      "  episode_reward_min:\n",
      "  - -3714.971430344552\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: -56.274344165301976\n",
      "      discounted_rewards_mean: -56.274344165301976\n",
      "      discounted_rewards_min: -56.274344165301976\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -2304.5975987819656\n",
      "    episode_reward_mean: -2304.5975987819656\n",
      "    episode_reward_min:\n",
      "    - -2304.5975987819656\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -2304.5975987819656\n",
      "      policy_firm_reward:\n",
      "      - - -1216.0165816156355\n",
      "      - - -1088.5810171663293\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -1088.5810171663293\n",
      "    policy_reward_mean:\n",
      "      firm: -1152.2987993909824\n",
      "    policy_reward_min:\n",
      "      firm: -1216.0165816156355\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1361440385001591\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.1370587508598606\n",
      "      mean_inference_ms: 1.1044379057495788\n",
      "      mean_raw_obs_processing_ms: 0.22442384936224516\n",
      "  experiment_id: 93386e6e40484bf1a269cf9601e459ad\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.1\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.4063695669174194\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0006658246275038309\n",
      "          policy_loss: 0.0001330217346549034\n",
      "          total_loss: 5260.656005859375\n",
      "          vf_explained_var: -5.513429712777906e-09\n",
      "          vf_loss: 5260.65576171875\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.9\n",
      "    ram_util_percent: 51.3\n",
      "  pid: 32029\n",
      "  policy_reward_max:\n",
      "    firm: -760.6987403692893\n",
      "  policy_reward_mean:\n",
      "    firm: -1488.0440761645016\n",
      "  policy_reward_min:\n",
      "    firm: -1909.7882449632027\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.14227806411769667\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1387959576876188\n",
      "    mean_inference_ms: 1.1657776633054193\n",
      "    mean_raw_obs_processing_ms: 0.23193021199304276\n",
      "  time_since_restore: 7.0940141677856445\n",
      "  time_this_iter_s: 3.46592116355896\n",
      "  time_total_s: 7.0940141677856445\n",
      "  timers:\n",
      "    learn_throughput: 4121.959\n",
      "    learn_time_ms: 242.603\n",
      "    sample_throughput: 598.784\n",
      "    sample_time_ms: 1670.051\n",
      "  timestamp: 1630715368\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: 1c945_00005\n",
      "  \n",
      "Result for PPO_townsend_1c945_00004:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: 112.78230351062575\n",
      "    discounted_rewards_mean: 97.49237060508105\n",
      "    discounted_rewards_min: 82.20243769953636\n",
      "  date: 2021-09-03_20-29-28\n",
      "  done: true\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - 627.7156877812333\n",
      "  episode_reward_mean: -115.08405747268966\n",
      "  episode_reward_min:\n",
      "  - -857.8838027266127\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: -32.67133676053514\n",
      "      discounted_rewards_mean: -32.67133676053514\n",
      "      discounted_rewards_min: -32.67133676053514\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -2354.853577990725\n",
      "    episode_reward_mean: -2354.853577990725\n",
      "    episode_reward_min:\n",
      "    - -2354.853577990725\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -2354.853577990725\n",
      "      policy_firm_reward:\n",
      "      - - -1271.0181563880915\n",
      "      - - -1083.8354216026316\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -1083.8354216026316\n",
      "    policy_reward_mean:\n",
      "      firm: -1177.4267889953617\n",
      "    policy_reward_min:\n",
      "      firm: -1271.0181563880915\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.13273850135479134\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.13578587445779064\n",
      "      mean_inference_ms: 1.0978359392081303\n",
      "      mean_raw_obs_processing_ms: 0.2166448027893402\n",
      "  experiment_id: 0f7d9eeccd47449386b3f1b8d6276f9c\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.1\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.4199694395065308\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0006443650927395624\n",
      "          policy_loss: 6.862077862024307e-05\n",
      "          total_loss: 2671.724609375\n",
      "          vf_explained_var: -6.556510889765832e-10\n",
      "          vf_loss: 2671.72412109375\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.675\n",
      "    ram_util_percent: 51.3\n",
      "  pid: 32030\n",
      "  policy_reward_max:\n",
      "    firm: 324.54999330997146\n",
      "  policy_reward_mean:\n",
      "    firm: -57.54202873634418\n",
      "  policy_reward_min:\n",
      "    firm: -785.5702982889411\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.13880456777214661\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.13917016837552926\n",
      "    mean_inference_ms: 1.163867049065791\n",
      "    mean_raw_obs_processing_ms: 0.22736460930025224\n",
      "  time_since_restore: 7.043003082275391\n",
      "  time_this_iter_s: 3.4142298698425293\n",
      "  time_total_s: 7.043003082275391\n",
      "  timers:\n",
      "    learn_throughput: 4093.981\n",
      "    learn_time_ms: 244.261\n",
      "    sample_throughput: 602.512\n",
      "    sample_time_ms: 1659.718\n",
      "  timestamp: 1630715368\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: 1c945_00004\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.3/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/4 CPUs, 0/0 GPUs, 0.0/10.89 GiB heap, 0.0/5.44 GiB objects<br>Result logdir: /Users/matiascovarrubias/ray_results/native_2_firms_townsend_test_Sep3_PPO<br>Number of trials: 6/6 (6 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_len_mean</th><th style=\"text-align: right;\">  episodes_this_iter</th><th style=\"text-align: right;\">  num_healthy_workers</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_townsend_1c945_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         7.47364</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">-1869.38 </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_1c945_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         7.54567</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">-2028.4  </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_1c945_00002</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         7.66011</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">-1308.57 </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_1c945_00003</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         7.55564</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\"> -610.125</td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_1c945_00004</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         7.043  </td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\"> -115.084</td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_1c945_00005</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         7.09401</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">-2976.09 </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=32030)\u001b[0m 2021-09-03 20:29:29,231\tERROR worker.py:382 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=32030)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=32030)\u001b[0m   File \"python/ray/_raylet.pyx\", line 495, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=32030)\u001b[0m   File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=32030)\u001b[0m   File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=32030)\u001b[0m   File \"/Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=32030)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=32030)\u001b[0m   File \"/Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/actor.py\", line 1001, in __ray_terminate__\n",
      "\u001b[2m\u001b[36m(pid=32030)\u001b[0m     ray.actor.exit_actor()\n",
      "\u001b[2m\u001b[36m(pid=32030)\u001b[0m   File \"/Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/actor.py\", line 1077, in exit_actor\n",
      "\u001b[2m\u001b[36m(pid=32030)\u001b[0m     raise exit\n",
      "\u001b[2m\u001b[36m(pid=32030)\u001b[0m SystemExit: 0\n",
      "\u001b[2m\u001b[36m(pid=32030)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32030)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=32030)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32030)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=32030)\u001b[0m   File \"python/ray/_raylet.pyx\", line 599, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=32030)\u001b[0m   File \"python/ray/_raylet.pyx\", line 451, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=32030)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=32030)\u001b[0m   File \"python/ray/includes/libcoreworker.pxi\", line 33, in ray._raylet.ProfileEvent.__exit__\n",
      "\u001b[2m\u001b[36m(pid=32030)\u001b[0m   File \"/Users/matiascovarrubias/.pyenv/versions/3.8.7/lib/python3.8/traceback.py\", line 167, in format_exc\n",
      "\u001b[2m\u001b[36m(pid=32030)\u001b[0m     return \"\".join(format_exception(*sys.exc_info(), limit=limit, chain=chain))\n",
      "\u001b[2m\u001b[36m(pid=32030)\u001b[0m   File \"/Users/matiascovarrubias/.pyenv/versions/3.8.7/lib/python3.8/traceback.py\", line 120, in format_exception\n",
      "\u001b[2m\u001b[36m(pid=32030)\u001b[0m     return list(TracebackException(\n",
      "\u001b[2m\u001b[36m(pid=32030)\u001b[0m   File \"/Users/matiascovarrubias/.pyenv/versions/3.8.7/lib/python3.8/traceback.py\", line 508, in __init__\n",
      "\u001b[2m\u001b[36m(pid=32030)\u001b[0m     self.stack = StackSummary.extract(\n",
      "\u001b[2m\u001b[36m(pid=32030)\u001b[0m   File \"/Users/matiascovarrubias/.pyenv/versions/3.8.7/lib/python3.8/traceback.py\", line 366, in extract\n",
      "\u001b[2m\u001b[36m(pid=32030)\u001b[0m     f.line\n",
      "\u001b[2m\u001b[36m(pid=32030)\u001b[0m   File \"/Users/matiascovarrubias/.pyenv/versions/3.8.7/lib/python3.8/traceback.py\", line 288, in line\n",
      "\u001b[2m\u001b[36m(pid=32030)\u001b[0m     self._line = linecache.getline(self.filename, self.lineno).strip()\n",
      "\u001b[2m\u001b[36m(pid=32030)\u001b[0m   File \"/Users/matiascovarrubias/.pyenv/versions/3.8.7/lib/python3.8/linecache.py\", line 16, in getline\n",
      "\u001b[2m\u001b[36m(pid=32030)\u001b[0m     lines = getlines(filename, module_globals)\n",
      "\u001b[2m\u001b[36m(pid=32030)\u001b[0m   File \"/Users/matiascovarrubias/.pyenv/versions/3.8.7/lib/python3.8/linecache.py\", line 47, in getlines\n",
      "\u001b[2m\u001b[36m(pid=32030)\u001b[0m     return updatecache(filename, module_globals)\n",
      "\u001b[2m\u001b[36m(pid=32030)\u001b[0m   File \"/Users/matiascovarrubias/.pyenv/versions/3.8.7/lib/python3.8/linecache.py\", line 137, in updatecache\n",
      "\u001b[2m\u001b[36m(pid=32030)\u001b[0m     lines = fp.readlines()\n",
      "\u001b[2m\u001b[36m(pid=32030)\u001b[0m   File \"/Users/matiascovarrubias/.pyenv/versions/3.8.7/lib/python3.8/codecs.py\", line 324, in decode\n",
      "\u001b[2m\u001b[36m(pid=32030)\u001b[0m     self.buffer = data[consumed:]\n",
      "\u001b[2m\u001b[36m(pid=32030)\u001b[0m   File \"/Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/worker.py\", line 379, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=32030)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=32030)\u001b[0m SystemExit: 1\n",
      "2021-09-03 20:29:29,335\tINFO tune.py:549 -- Total run time: 23.68 seconds (23.21 seconds for the tuning loop).\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.3/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/4 CPUs, 0/0 GPUs, 0.0/10.89 GiB heap, 0.0/5.44 GiB objects<br>Result logdir: /Users/matiascovarrubias/ray_results/native_3_firms_townsend_test_Sep3_PPO<br>Number of trials: 6/6 (5 PENDING, 1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_townsend_2ac4e_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_2ac4e_00001</td><td>PENDING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_2ac4e_00002</td><td>PENDING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_2ac4e_00003</td><td>PENDING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_2ac4e_00004</td><td>PENDING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_2ac4e_00005</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(pid=32031)\u001b[0m WARNING:tensorflow:From /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=32031)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=32031)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=32035)\u001b[0m WARNING:tensorflow:From /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=32035)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=32035)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=32065)\u001b[0m WARNING:tensorflow:From /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=32065)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=32065)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=32066)\u001b[0m WARNING:tensorflow:From /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=32066)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=32066)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=32031)\u001b[0m 2021-09-03 20:29:33,063\tINFO trainer.py:694 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=32035)\u001b[0m 2021-09-03 20:29:33,063\tINFO trainer.py:694 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=32031)\u001b[0m 2021-09-03 20:29:33,214\tWARNING deprecation.py:33 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=32035)\u001b[0m 2021-09-03 20:29:33,216\tWARNING deprecation.py:33 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=32065)\u001b[0m 2021-09-03 20:29:33,523\tINFO trainer.py:694 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=32066)\u001b[0m 2021-09-03 20:29:33,539\tINFO trainer.py:694 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=32065)\u001b[0m 2021-09-03 20:29:33,677\tWARNING deprecation.py:33 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=32066)\u001b[0m 2021-09-03 20:29:33,698\tWARNING deprecation.py:33 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.5/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/4 CPUs, 0/0 GPUs, 0.0/10.89 GiB heap, 0.0/5.44 GiB objects<br>Result logdir: /Users/matiascovarrubias/ray_results/native_3_firms_townsend_test_Sep3_PPO<br>Number of trials: 6/6 (2 PENDING, 4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_townsend_2ac4e_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_2ac4e_00001</td><td>RUNNING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_2ac4e_00002</td><td>RUNNING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_2ac4e_00003</td><td>RUNNING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_2ac4e_00004</td><td>PENDING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_2ac4e_00005</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=32031)\u001b[0m 2021-09-03 20:29:35,257\tWARNING deprecation.py:33 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=32035)\u001b[0m 2021-09-03 20:29:35,269\tWARNING deprecation.py:33 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=32065)\u001b[0m 2021-09-03 20:29:35,778\tWARNING deprecation.py:33 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=32066)\u001b[0m 2021-09-03 20:29:35,793\tWARNING deprecation.py:33 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_townsend_2ac4e_00001:\n",
      "  agent_timesteps_total: 3000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: 97.48538447005866\n",
      "    discounted_rewards_mean: 97.48538447005866\n",
      "    discounted_rewards_min: 97.48538447005866\n",
      "  date: 2021-09-03_20-29-37\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - -2473.357175313036\n",
      "  episode_reward_mean: -2473.357175313036\n",
      "  episode_reward_min:\n",
      "  - -2473.357175313036\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: -40.74293044977921\n",
      "      discounted_rewards_mean: -40.74293044977921\n",
      "      discounted_rewards_min: -40.74293044977921\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -2573.614424657202\n",
      "    episode_reward_mean: -2573.614424657202\n",
      "    episode_reward_min:\n",
      "    - -2573.614424657202\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -2573.614424657202\n",
      "      policy_firm_reward:\n",
      "      - - -1401.8034760124121\n",
      "      - - -193.61392783050255\n",
      "      - - -978.1970208142917\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -193.61392783050255\n",
      "    policy_reward_mean:\n",
      "      firm: -857.8714748857354\n",
      "    policy_reward_min:\n",
      "      firm: -1401.8034760124121\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.18662601322322697\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.1797480778498845\n",
      "      mean_inference_ms: 1.2080697984724016\n",
      "      mean_raw_obs_processing_ms: 0.3428070933430583\n",
      "  experiment_id: b4d2e97c4faa4aafa5603f720ee6889c\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.20000000000000004\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.3996522029240925\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0014318344765343529\n",
      "          policy_loss: 0.0005196740385144949\n",
      "          total_loss: 3911.2909342447915\n",
      "          vf_explained_var: -7.013480018969176e-09\n",
      "          vf_loss: 3911.2903645833335\n",
      "    num_agent_steps_sampled: 3000\n",
      "    num_agent_steps_trained: 3000\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.47142857142857\n",
      "    ram_util_percent: 52.34285714285715\n",
      "  pid: 32031\n",
      "  policy_reward_max:\n",
      "    firm: -569.3698834905401\n",
      "  policy_reward_mean:\n",
      "    firm: -824.4523917710111\n",
      "  policy_reward_min:\n",
      "    firm: -1302.5214266041687\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18478297329806426\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17647857551689033\n",
      "    mean_inference_ms: 1.214915817672318\n",
      "    mean_raw_obs_processing_ms: 0.3406563243427715\n",
      "  time_since_restore: 4.417747974395752\n",
      "  time_this_iter_s: 4.417747974395752\n",
      "  time_total_s: 4.417747974395752\n",
      "  timers:\n",
      "    learn_throughput: 1947.321\n",
      "    learn_time_ms: 513.526\n",
      "    sample_throughput: 516.18\n",
      "    sample_time_ms: 1937.309\n",
      "  timestamp: 1630715377\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: 2ac4e_00001\n",
      "  \n",
      "Result for PPO_townsend_2ac4e_00000:\n",
      "  agent_timesteps_total: 3000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: -111.48969085726785\n",
      "    discounted_rewards_mean: -111.48969085726785\n",
      "    discounted_rewards_min: -111.48969085726785\n",
      "  date: 2021-09-03_20-29-37\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - -535.8104441194814\n",
      "  episode_reward_mean: -535.8104441194814\n",
      "  episode_reward_min:\n",
      "  - -535.8104441194814\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: 68.68028597397753\n",
      "      discounted_rewards_mean: 68.68028597397753\n",
      "      discounted_rewards_min: 68.68028597397753\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -4242.846615471958\n",
      "    episode_reward_mean: -4242.846615471958\n",
      "    episode_reward_min:\n",
      "    - -4242.846615471958\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -4242.846615471958\n",
      "      policy_firm_reward:\n",
      "      - - -1967.7907020388084\n",
      "      - - -1076.7431419606532\n",
      "      - - -1198.3127714724817\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -1076.7431419606532\n",
      "    policy_reward_mean:\n",
      "      firm: -1414.2822051573146\n",
      "    policy_reward_min:\n",
      "      firm: -1967.7907020388084\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1872283714515465\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.1834474958025373\n",
      "      mean_inference_ms: 1.2131104102501502\n",
      "      mean_raw_obs_processing_ms: 0.3466205997066898\n",
      "  experiment_id: 46e63b664f284473b58d7203d9111134\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.20000000000000004\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.427851676940918\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0008160386205418556\n",
      "          policy_loss: 0.00023557494084040323\n",
      "          total_loss: 5511.655110677083\n",
      "          vf_explained_var: -5.165735617040923e-10\n",
      "          vf_loss: 5511.65478515625\n",
      "    num_agent_steps_sampled: 3000\n",
      "    num_agent_steps_trained: 3000\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.457142857142856\n",
      "    ram_util_percent: 52.34285714285714\n",
      "  pid: 32035\n",
      "  policy_reward_max:\n",
      "    firm: 221.90207791855633\n",
      "  policy_reward_mean:\n",
      "    firm: -178.6034813731628\n",
      "  policy_reward_min:\n",
      "    firm: -389.71484506279194\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18433281234451585\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18136841910226006\n",
      "    mean_inference_ms: 1.2136727541714878\n",
      "    mean_raw_obs_processing_ms: 0.3415749861405684\n",
      "  time_since_restore: 4.451347827911377\n",
      "  time_this_iter_s: 4.451347827911377\n",
      "  time_total_s: 4.451347827911377\n",
      "  timers:\n",
      "    learn_throughput: 1894.743\n",
      "    learn_time_ms: 527.776\n",
      "    sample_throughput: 514.649\n",
      "    sample_time_ms: 1943.072\n",
      "  timestamp: 1630715377\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: 2ac4e_00000\n",
      "  \n",
      "Result for PPO_townsend_2ac4e_00003:\n",
      "  agent_timesteps_total: 3000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: 17.372544137530177\n",
      "    discounted_rewards_mean: 17.372544137530177\n",
      "    discounted_rewards_min: 17.372544137530177\n",
      "  date: 2021-09-03_20-29-38\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - -1092.565413597494\n",
      "  episode_reward_mean: -1092.565413597494\n",
      "  episode_reward_min:\n",
      "  - -1092.565413597494\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: -38.2449120987882\n",
      "      discounted_rewards_mean: -38.2449120987882\n",
      "      discounted_rewards_min: -38.2449120987882\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -3939.498329369975\n",
      "    episode_reward_mean: -3939.498329369975\n",
      "    episode_reward_min:\n",
      "    - -3939.498329369975\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -3939.498329369975\n",
      "      policy_firm_reward:\n",
      "      - - -1404.6619945378036\n",
      "      - - -1022.2869700311303\n",
      "      - - -1512.5493648010483\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -1022.2869700311303\n",
      "    policy_reward_mean:\n",
      "      firm: -1313.166109789994\n",
      "    policy_reward_min:\n",
      "      firm: -1512.5493648010483\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.18820991287460098\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.18008653219644127\n",
      "      mean_inference_ms: 1.2079490410102594\n",
      "      mean_raw_obs_processing_ms: 0.34715722014496736\n",
      "  experiment_id: 9dd032ddae744b0fb8636179935e7a75\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.20000000000000004\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.4187382857004802\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0011048573651351698\n",
      "          policy_loss: 0.0002372946279744307\n",
      "          total_loss: 3855.5764973958335\n",
      "          vf_explained_var: -3.258387337723434e-09\n",
      "          vf_loss: 3855.5763346354165\n",
      "    num_agent_steps_sampled: 3000\n",
      "    num_agent_steps_trained: 3000\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.32857142857143\n",
      "    ram_util_percent: 52.52857142857143\n",
      "  pid: 32065\n",
      "  policy_reward_max:\n",
      "    firm: -84.86768258670769\n",
      "  policy_reward_mean:\n",
      "    firm: -364.1884711991648\n",
      "  policy_reward_min:\n",
      "    firm: -639.9570335816036\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18872747888098232\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17912928517405446\n",
      "    mean_inference_ms: 1.2370110987187861\n",
      "    mean_raw_obs_processing_ms: 0.36851223651226706\n",
      "  time_since_restore: 4.471184253692627\n",
      "  time_this_iter_s: 4.471184253692627\n",
      "  time_total_s: 4.471184253692627\n",
      "  timers:\n",
      "    learn_throughput: 1993.815\n",
      "    learn_time_ms: 501.551\n",
      "    sample_throughput: 501.365\n",
      "    sample_time_ms: 1994.556\n",
      "  timestamp: 1630715378\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: 2ac4e_00003\n",
      "  \n",
      "Result for PPO_townsend_2ac4e_00002:\n",
      "  agent_timesteps_total: 3000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: -33.35751970830057\n",
      "    discounted_rewards_mean: -33.35751970830057\n",
      "    discounted_rewards_min: -33.35751970830057\n",
      "  date: 2021-09-03_20-29-38\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - -4453.417881590434\n",
      "  episode_reward_mean: -4453.417881590434\n",
      "  episode_reward_min:\n",
      "  - -4453.417881590434\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: -71.86727144280295\n",
      "      discounted_rewards_mean: -71.86727144280295\n",
      "      discounted_rewards_min: -71.86727144280295\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -4041.9221296076603\n",
      "    episode_reward_mean: -4041.9221296076603\n",
      "    episode_reward_min:\n",
      "    - -4041.9221296076603\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -4041.9221296076603\n",
      "      policy_firm_reward:\n",
      "      - - -1785.8096448751955\n",
      "      - - -1069.7219212993575\n",
      "      - - -1186.390563433114\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -1069.7219212993575\n",
      "    policy_reward_mean:\n",
      "      firm: -1347.307376535889\n",
      "    policy_reward_min:\n",
      "      firm: -1785.8096448751955\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.18745964461868697\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.18279226152570574\n",
      "      mean_inference_ms: 1.2165618824077533\n",
      "      mean_raw_obs_processing_ms: 0.33039384550386136\n",
      "  experiment_id: d5584121d5274fa3b057466b2470e911\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.20000000000000004\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.4034475882848103\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0013123091581899234\n",
      "          policy_loss: 0.000576995157947143\n",
      "          total_loss: 5325.427734375\n",
      "          vf_explained_var: -5.563100202721216e-09\n",
      "          vf_loss: 5325.427408854167\n",
      "    num_agent_steps_sampled: 3000\n",
      "    num_agent_steps_trained: 3000\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.857142857142858\n",
      "    ram_util_percent: 52.52857142857143\n",
      "  pid: 32066\n",
      "  policy_reward_max:\n",
      "    firm: -848.7723105874531\n",
      "  policy_reward_mean:\n",
      "    firm: -1484.4726271968127\n",
      "  policy_reward_min:\n",
      "    firm: -2667.2537395349514\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1857685637878967\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1832319425417112\n",
      "    mean_inference_ms: 1.2361145876980686\n",
      "    mean_raw_obs_processing_ms: 0.35998585459950205\n",
      "  time_since_restore: 4.462191104888916\n",
      "  time_this_iter_s: 4.462191104888916\n",
      "  time_total_s: 4.462191104888916\n",
      "  timers:\n",
      "    learn_throughput: 1967.31\n",
      "    learn_time_ms: 508.308\n",
      "    sample_throughput: 503.397\n",
      "    sample_time_ms: 1986.504\n",
      "  timestamp: 1630715378\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: 2ac4e_00002\n",
      "  \n",
      "Result for PPO_townsend_2ac4e_00001:\n",
      "  agent_timesteps_total: 6000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: 229.46071685850586\n",
      "    discounted_rewards_mean: 163.47305066428225\n",
      "    discounted_rewards_min: 97.48538447005866\n",
      "  date: 2021-09-03_20-29-42\n",
      "  done: true\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - -1723.140813379028\n",
      "  episode_reward_mean: -2098.248994346032\n",
      "  episode_reward_min:\n",
      "  - -2473.357175313036\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: -82.69211973200993\n",
      "      discounted_rewards_mean: -82.69211973200993\n",
      "      discounted_rewards_min: -82.69211973200993\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -3723.0313230183665\n",
      "    episode_reward_mean: -3723.0313230183665\n",
      "    episode_reward_min:\n",
      "    - -3723.0313230183665\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -3723.0313230183665\n",
      "      policy_firm_reward:\n",
      "      - - -1462.7962041163091\n",
      "      - - -1164.7502312994459\n",
      "      - - -1095.484887602616\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -1095.484887602616\n",
      "    policy_reward_mean:\n",
      "      firm: -1241.0104410061238\n",
      "    policy_reward_min:\n",
      "      firm: -1462.7962041163091\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.19152423014109402\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.18414111807011532\n",
      "      mean_inference_ms: 1.2426792175277717\n",
      "      mean_raw_obs_processing_ms: 0.34896949718500125\n",
      "  experiment_id: b4d2e97c4faa4aafa5603f720ee6889c\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.10000000000000002\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.4544637203216553\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.000787632501063328\n",
      "          policy_loss: 0.00017529788116614023\n",
      "          total_loss: 3347.6981608072915\n",
      "          vf_explained_var: -5.165735839085528e-09\n",
      "          vf_loss: 3347.6979166666665\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 6000\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.099999999999998\n",
      "    ram_util_percent: 53.416666666666664\n",
      "  pid: 32031\n",
      "  policy_reward_max:\n",
      "    firm: -343.60358666758106\n",
      "  policy_reward_mean:\n",
      "    firm: -699.4163314486772\n",
      "  policy_reward_min:\n",
      "    firm: -1302.5214266041687\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18687792529921388\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17808949186019069\n",
      "    mean_inference_ms: 1.2299991424727281\n",
      "    mean_raw_obs_processing_ms: 0.3398530060154854\n",
      "  time_since_restore: 8.92653489112854\n",
      "  time_this_iter_s: 4.508786916732788\n",
      "  time_total_s: 8.92653489112854\n",
      "  timers:\n",
      "    learn_throughput: 2109.504\n",
      "    learn_time_ms: 474.045\n",
      "    sample_throughput: 508.364\n",
      "    sample_time_ms: 1967.095\n",
      "  timestamp: 1630715382\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: 2ac4e_00001\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.2/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/4 CPUs, 0/0 GPUs, 0.0/10.89 GiB heap, 0.0/5.44 GiB objects<br>Result logdir: /Users/matiascovarrubias/ray_results/native_3_firms_townsend_test_Sep3_PPO<br>Number of trials: 6/6 (2 PENDING, 4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_len_mean</th><th style=\"text-align: right;\">  episodes_this_iter</th><th style=\"text-align: right;\">  num_healthy_workers</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_townsend_2ac4e_00000</td><td>RUNNING </td><td>192.168.1.202:32035</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.45135</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\"> -535.81</td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_2ac4e_00001</td><td>RUNNING </td><td>192.168.1.202:32031</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         8.92653</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">-2098.25</td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_2ac4e_00002</td><td>RUNNING </td><td>192.168.1.202:32066</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.46219</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">-4453.42</td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_2ac4e_00003</td><td>RUNNING </td><td>192.168.1.202:32065</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.47118</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">-1092.57</td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_2ac4e_00004</td><td>PENDING </td><td>                   </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                     </td></tr>\n",
       "<tr><td>PPO_townsend_2ac4e_00005</td><td>PENDING </td><td>                   </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_townsend_2ac4e_00000:\n",
      "  agent_timesteps_total: 6000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: -26.154486445626635\n",
      "    discounted_rewards_mean: -68.82208865144725\n",
      "    discounted_rewards_min: -111.48969085726785\n",
      "  date: 2021-09-03_20-29-42\n",
      "  done: true\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - -535.8104441194814\n",
      "  episode_reward_mean: -831.3227499557468\n",
      "  episode_reward_min:\n",
      "  - -1126.835055792012\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: 50.42643748446326\n",
      "      discounted_rewards_mean: 50.42643748446326\n",
      "      discounted_rewards_min: 50.42643748446326\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -2826.1043986485665\n",
      "    episode_reward_mean: -2826.1043986485665\n",
      "    episode_reward_min:\n",
      "    - -2826.1043986485665\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -2826.1043986485665\n",
      "      policy_firm_reward:\n",
      "      - - -1076.997576310256\n",
      "      - - -957.3222517832538\n",
      "      - - -791.7845705550576\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -791.7845705550576\n",
      "    policy_reward_mean:\n",
      "      firm: -942.0347995495225\n",
      "    policy_reward_min:\n",
      "      firm: -1076.997576310256\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1915294727285405\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.18622087634008924\n",
      "      mean_inference_ms: 1.248035235502671\n",
      "      mean_raw_obs_processing_ms: 0.345598751756324\n",
      "  experiment_id: 46e63b664f284473b58d7203d9111134\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.10000000000000002\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.3840126593907673\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0006714203239730049\n",
      "          policy_loss: 0.00020205128627518812\n",
      "          total_loss: 2218.0889485677085\n",
      "          vf_explained_var: 1.1920929521291868e-10\n",
      "          vf_loss: 2218.0887858072915\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 6000\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.866666666666664\n",
      "    ram_util_percent: 53.416666666666664\n",
      "  pid: 32035\n",
      "  policy_reward_max:\n",
      "    firm: 221.90207791855633\n",
      "  policy_reward_mean:\n",
      "    firm: -277.1075833185832\n",
      "  policy_reward_min:\n",
      "    firm: -643.5262323759242\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18644343919886597\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1823427488727778\n",
      "    mean_inference_ms: 1.2332698744522035\n",
      "    mean_raw_obs_processing_ms: 0.34737803414541923\n",
      "  time_since_restore: 9.019691944122314\n",
      "  time_this_iter_s: 4.5683441162109375\n",
      "  time_total_s: 9.019691944122314\n",
      "  timers:\n",
      "    learn_throughput: 2000.236\n",
      "    learn_time_ms: 499.941\n",
      "    sample_throughput: 501.931\n",
      "    sample_time_ms: 1992.307\n",
      "  timestamp: 1630715382\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: 2ac4e_00000\n",
      "  \n",
      "Result for PPO_townsend_2ac4e_00003:\n",
      "  agent_timesteps_total: 6000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: 65.53520005181129\n",
      "    discounted_rewards_mean: 41.45387209467073\n",
      "    discounted_rewards_min: 17.372544137530177\n",
      "  date: 2021-09-03_20-29-42\n",
      "  done: true\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - -1092.565413597494\n",
      "  episode_reward_mean: -2687.8037083766985\n",
      "  episode_reward_min:\n",
      "  - -4283.042003155903\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: 12.897579411296576\n",
      "      discounted_rewards_mean: 12.897579411296576\n",
      "      discounted_rewards_min: 12.897579411296576\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -3370.899332237059\n",
      "    episode_reward_mean: -3370.899332237059\n",
      "    episode_reward_min:\n",
      "    - -3370.899332237059\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -3370.899332237059\n",
      "      policy_firm_reward:\n",
      "      - - -1089.0442022451207\n",
      "      - - -1233.2725438523516\n",
      "      - - -1048.5825861395851\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -1048.5825861395851\n",
      "    policy_reward_mean:\n",
      "      firm: -1123.6331107456858\n",
      "    policy_reward_min:\n",
      "      firm: -1233.2725438523516\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.19338022524687362\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.1848479141776768\n",
      "      mean_inference_ms: 1.2499111286108044\n",
      "      mean_raw_obs_processing_ms: 0.350001095414817\n",
      "  experiment_id: 9dd032ddae744b0fb8636179935e7a75\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.10000000000000002\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.418099045753479\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0009919332611997096\n",
      "          policy_loss: 0.00016754508639375368\n",
      "          total_loss: 7046.672200520833\n",
      "          vf_explained_var: -8.543332263855064e-09\n",
      "          vf_loss: 7046.671875\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 6000\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.2\n",
      "    ram_util_percent: 53.46666666666667\n",
      "  pid: 32065\n",
      "  policy_reward_max:\n",
      "    firm: -84.86768258670769\n",
      "  policy_reward_mean:\n",
      "    firm: -895.9345694588992\n",
      "  policy_reward_min:\n",
      "    firm: -2214.8136967304426\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19104652435647435\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18058960330079266\n",
      "    mean_inference_ms: 1.253838994239736\n",
      "    mean_raw_obs_processing_ms: 0.3698871439336329\n",
      "  time_since_restore: 9.083136081695557\n",
      "  time_this_iter_s: 4.61195182800293\n",
      "  time_total_s: 9.083136081695557\n",
      "  timers:\n",
      "    learn_throughput: 2064.283\n",
      "    learn_time_ms: 484.43\n",
      "    sample_throughput: 492.19\n",
      "    sample_time_ms: 2031.737\n",
      "  timestamp: 1630715382\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: 2ac4e_00003\n",
      "  \n",
      "Result for PPO_townsend_2ac4e_00002:\n",
      "  agent_timesteps_total: 6000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: -1.758994448311166\n",
      "    discounted_rewards_mean: -17.558257078305868\n",
      "    discounted_rewards_min: -33.35751970830057\n",
      "  date: 2021-09-03_20-29-42\n",
      "  done: true\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - -1199.2101817754417\n",
      "  episode_reward_mean: -2826.314031682938\n",
      "  episode_reward_min:\n",
      "  - -4453.417881590434\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: -11.562091855785145\n",
      "      discounted_rewards_mean: -11.562091855785145\n",
      "      discounted_rewards_min: -11.562091855785145\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -3588.55785551744\n",
      "    episode_reward_mean: -3588.55785551744\n",
      "    episode_reward_min:\n",
      "    - -3588.55785551744\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -3588.55785551744\n",
      "      policy_firm_reward:\n",
      "      - - -1187.0225031641571\n",
      "      - - -942.2384490260749\n",
      "      - - -1459.2969033272173\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -942.2384490260749\n",
      "    policy_reward_mean:\n",
      "      firm: -1196.18595183915\n",
      "    policy_reward_min:\n",
      "      firm: -1459.2969033272173\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.19227940103282098\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.18766747302618222\n",
      "      mean_inference_ms: 1.2553526960808536\n",
      "      mean_raw_obs_processing_ms: 0.33914500745995413\n",
      "  experiment_id: d5584121d5274fa3b057466b2470e911\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.10000000000000002\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.454804261525472\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0005057543166914276\n",
      "          policy_loss: 0.00018356554210186005\n",
      "          total_loss: 2541.8372395833335\n",
      "          vf_explained_var: -6.357828707947988e-10\n",
      "          vf_loss: 2541.8369140625\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 6000\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.216666666666665\n",
      "    ram_util_percent: 53.46666666666667\n",
      "  pid: 32066\n",
      "  policy_reward_max:\n",
      "    firm: -141.55521201858235\n",
      "  policy_reward_mean:\n",
      "    firm: -942.1046772276463\n",
      "  policy_reward_min:\n",
      "    firm: -2667.2537395349514\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.18863561391531558\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18483966124471635\n",
      "    mean_inference_ms: 1.255033157971569\n",
      "    mean_raw_obs_processing_ms: 0.36010547436636664\n",
      "  time_since_restore: 9.06162977218628\n",
      "  time_this_iter_s: 4.599438667297363\n",
      "  time_total_s: 9.06162977218628\n",
      "  timers:\n",
      "    learn_throughput: 2083.819\n",
      "    learn_time_ms: 479.888\n",
      "    sample_throughput: 493.192\n",
      "    sample_time_ms: 2027.607\n",
      "  timestamp: 1630715382\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: 2ac4e_00002\n",
      "  \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(pid=32124)\u001b[0m WARNING:tensorflow:From /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=32124)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=32124)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=32123)\u001b[0m WARNING:tensorflow:From /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=32123)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=32123)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=32124)\u001b[0m 2021-09-03 20:29:46,917\tINFO trainer.py:694 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=32123)\u001b[0m 2021-09-03 20:29:46,916\tINFO trainer.py:694 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=32124)\u001b[0m 2021-09-03 20:29:47,077\tWARNING deprecation.py:33 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=32123)\u001b[0m 2021-09-03 20:29:47,077\tWARNING deprecation.py:33 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=32123)\u001b[0m 2021-09-03 20:29:49,237\tWARNING deprecation.py:33 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=32124)\u001b[0m 2021-09-03 20:29:49,262\tWARNING deprecation.py:33 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_townsend_2ac4e_00005:\n",
      "  agent_timesteps_total: 3000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: 93.70034975285652\n",
      "    discounted_rewards_mean: 93.70034975285652\n",
      "    discounted_rewards_min: 93.70034975285652\n",
      "  date: 2021-09-03_20-29-51\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - -2014.223409560105\n",
      "  episode_reward_mean: -2014.223409560105\n",
      "  episode_reward_min:\n",
      "  - -2014.223409560105\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: 80.17029235218457\n",
      "      discounted_rewards_mean: 80.17029235218457\n",
      "      discounted_rewards_min: 80.17029235218457\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -3058.4696643393822\n",
      "    episode_reward_mean: -3058.4696643393822\n",
      "    episode_reward_min:\n",
      "    - -3058.4696643393822\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -3058.4696643393822\n",
      "      policy_firm_reward:\n",
      "      - - -1035.6156201301949\n",
      "      - - -1122.1536344093768\n",
      "      - - -900.7004097998134\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -900.7004097998134\n",
      "    policy_reward_mean:\n",
      "      firm: -1019.4898881131284\n",
      "    policy_reward_min:\n",
      "      firm: -1122.1536344093768\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.19467889250337067\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.18489682352864423\n",
      "      mean_inference_ms: 1.2309053441980384\n",
      "      mean_raw_obs_processing_ms: 0.35430834843562203\n",
      "  experiment_id: 58c52679e57e44bcb2ffaa3a44364007\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.20000000000000004\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.41819429397583\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0008380431099795279\n",
      "          policy_loss: 9.790745874245961e-05\n",
      "          total_loss: 6145.337890625\n",
      "          vf_explained_var: -3.1789144649962964e-09\n",
      "          vf_loss: 6145.337565104167\n",
      "    num_agent_steps_sampled: 3000\n",
      "    num_agent_steps_trained: 3000\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.25714285714286\n",
      "    ram_util_percent: 51.37142857142858\n",
      "  pid: 32123\n",
      "  policy_reward_max:\n",
      "    firm: -423.28219058327505\n",
      "  policy_reward_mean:\n",
      "    firm: -671.4078031867016\n",
      "  policy_reward_min:\n",
      "    firm: -1088.8367499521296\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19894041619696223\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.19060553132475436\n",
      "    mean_inference_ms: 1.284261802574257\n",
      "    mean_raw_obs_processing_ms: 0.3520453964675461\n",
      "  time_since_restore: 4.509088039398193\n",
      "  time_this_iter_s: 4.509088039398193\n",
      "  time_total_s: 4.509088039398193\n",
      "  timers:\n",
      "    learn_throughput: 2218.072\n",
      "    learn_time_ms: 450.842\n",
      "    sample_throughput: 488.629\n",
      "    sample_time_ms: 2046.542\n",
      "  timestamp: 1630715391\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: 2ac4e_00005\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.5/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/0 GPUs, 0.0/10.89 GiB heap, 0.0/5.44 GiB objects<br>Result logdir: /Users/matiascovarrubias/ray_results/native_3_firms_townsend_test_Sep3_PPO<br>Number of trials: 6/6 (2 RUNNING, 4 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_len_mean</th><th style=\"text-align: right;\">  episodes_this_iter</th><th style=\"text-align: right;\">  num_healthy_workers</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_townsend_2ac4e_00004</td><td>RUNNING   </td><td>                   </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                     </td></tr>\n",
       "<tr><td>PPO_townsend_2ac4e_00005</td><td>RUNNING   </td><td>192.168.1.202:32123</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.50909</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">-2014.22 </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_2ac4e_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         9.01969</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\"> -831.323</td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_2ac4e_00001</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         8.92653</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">-2098.25 </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_2ac4e_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         9.06163</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">-2826.31 </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_2ac4e_00003</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         9.08314</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">-2687.8  </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_townsend_2ac4e_00004:\n",
      "  agent_timesteps_total: 3000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: 51.300410984506655\n",
      "    discounted_rewards_mean: 51.300410984506655\n",
      "    discounted_rewards_min: 51.300410984506655\n",
      "  date: 2021-09-03_20-29-51\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - -2306.3045232097993\n",
      "  episode_reward_mean: -2306.3045232097993\n",
      "  episode_reward_min:\n",
      "  - -2306.3045232097993\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: 3.102211938509946\n",
      "      discounted_rewards_mean: 3.102211938509946\n",
      "      discounted_rewards_min: 3.102211938509946\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -3810.598574625952\n",
      "    episode_reward_mean: -3810.598574625952\n",
      "    episode_reward_min:\n",
      "    - -3810.598574625952\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -3810.598574625952\n",
      "      policy_firm_reward:\n",
      "      - - -1355.4690806650453\n",
      "      - - -1353.5526469949546\n",
      "      - - -1101.5768469659508\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -1101.5768469659508\n",
      "    policy_reward_mean:\n",
      "      firm: -1270.1995248753167\n",
      "    policy_reward_min:\n",
      "      firm: -1355.4690806650453\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1935927898852856\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.18916930352057612\n",
      "      mean_inference_ms: 1.2510758894425886\n",
      "      mean_raw_obs_processing_ms: 0.3423138217373447\n",
      "  experiment_id: 37e018561af74bc58ad3fce6ef90e1f8\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.20000000000000004\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.437394102414449\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.001250165844491415\n",
      "          policy_loss: 0.0005630685773212463\n",
      "          total_loss: 3522.6875813802085\n",
      "          vf_explained_var: -1.86761228704313e-09\n",
      "          vf_loss: 3522.68701171875\n",
      "    num_agent_steps_sampled: 3000\n",
      "    num_agent_steps_trained: 3000\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.414285714285715\n",
      "    ram_util_percent: 51.37142857142858\n",
      "  pid: 32124\n",
      "  policy_reward_max:\n",
      "    firm: -366.5434533992603\n",
      "  policy_reward_mean:\n",
      "    firm: -768.7681744032665\n",
      "  policy_reward_min:\n",
      "    firm: -1252.5603362895386\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19653360326806982\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.191768089850823\n",
      "    mean_inference_ms: 1.2976594500012928\n",
      "    mean_raw_obs_processing_ms: 0.36677328142133747\n",
      "  time_since_restore: 4.553380012512207\n",
      "  time_this_iter_s: 4.553380012512207\n",
      "  time_total_s: 4.553380012512207\n",
      "  timers:\n",
      "    learn_throughput: 2201.217\n",
      "    learn_time_ms: 454.294\n",
      "    sample_throughput: 481.924\n",
      "    sample_time_ms: 2075.017\n",
      "  timestamp: 1630715391\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: 2ac4e_00004\n",
      "  \n",
      "Result for PPO_townsend_2ac4e_00005:\n",
      "  agent_timesteps_total: 6000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: 93.70034975285652\n",
      "    discounted_rewards_mean: 12.248524510711903\n",
      "    discounted_rewards_min: -69.20330073143272\n",
      "  date: 2021-09-03_20-29-56\n",
      "  done: true\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - 985.311164557383\n",
      "  episode_reward_mean: -514.456122501361\n",
      "  episode_reward_min:\n",
      "  - -2014.223409560105\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: -5.110492286327353\n",
      "      discounted_rewards_mean: -5.110492286327353\n",
      "      discounted_rewards_min: -5.110492286327353\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -3805.4258857398945\n",
      "    episode_reward_mean: -3805.4258857398945\n",
      "    episode_reward_min:\n",
      "    - -3805.4258857398945\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -3805.4258857398945\n",
      "      policy_firm_reward:\n",
      "      - - -1512.7625979873396\n",
      "      - - -1142.816516016537\n",
      "      - - -1149.8467717360174\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -1142.816516016537\n",
      "    policy_reward_mean:\n",
      "      firm: -1268.4752952466313\n",
      "    policy_reward_min:\n",
      "      firm: -1512.7625979873396\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.19421856263945664\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.18524980616533773\n",
      "      mean_inference_ms: 1.2354185913634979\n",
      "      mean_raw_obs_processing_ms: 0.34531839247765034\n",
      "  experiment_id: 58c52679e57e44bcb2ffaa3a44364007\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.10000000000000002\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.4310396909713745\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0004655300678375435\n",
      "          policy_loss: 7.670745253562927e-05\n",
      "          total_loss: 6006.879231770833\n",
      "          vf_explained_var: -4.649162388403738e-09\n",
      "          vf_loss: 6006.87939453125\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 6000\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.666666666666668\n",
      "    ram_util_percent: 51.083333333333336\n",
      "  pid: 32123\n",
      "  policy_reward_max:\n",
      "    firm: 553.5730111887547\n",
      "  policy_reward_mean:\n",
      "    firm: -171.48537416712043\n",
      "  policy_reward_min:\n",
      "    firm: -1088.8367499521296\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19858340889478096\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.189513611022473\n",
      "    mean_inference_ms: 1.2860421724276812\n",
      "    mean_raw_obs_processing_ms: 0.3558313539530631\n",
      "  time_since_restore: 8.97302508354187\n",
      "  time_this_iter_s: 4.463937044143677\n",
      "  time_total_s: 8.97302508354187\n",
      "  timers:\n",
      "    learn_throughput: 2304.36\n",
      "    learn_time_ms: 433.96\n",
      "    sample_throughput: 488.138\n",
      "    sample_time_ms: 2048.602\n",
      "  timestamp: 1630715396\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: 2ac4e_00005\n",
      "  \n",
      "Result for PPO_townsend_2ac4e_00004:\n",
      "  agent_timesteps_total: 6000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: 51.300410984506655\n",
      "    discounted_rewards_mean: 35.38113946491743\n",
      "    discounted_rewards_min: 19.461867945328212\n",
      "  date: 2021-09-03_20-29-56\n",
      "  done: true\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - -2306.3045232097993\n",
      "  episode_reward_mean: -2506.433952730854\n",
      "  episode_reward_min:\n",
      "  - -2706.563382251908\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: -74.04538665302061\n",
      "      discounted_rewards_mean: -74.04538665302061\n",
      "      discounted_rewards_min: -74.04538665302061\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -3174.519068251437\n",
      "    episode_reward_mean: -3174.519068251437\n",
      "    episode_reward_min:\n",
      "    - -3174.519068251437\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -3174.519068251437\n",
      "      policy_firm_reward:\n",
      "      - - -1287.443061694163\n",
      "      - - -952.8115955010223\n",
      "      - - -934.2644110562484\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -934.2644110562484\n",
      "    policy_reward_mean:\n",
      "      firm: -1058.173022750478\n",
      "    policy_reward_min:\n",
      "      firm: -1287.443061694163\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.19358814149901368\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.18897144750378717\n",
      "      mean_inference_ms: 1.2557021621940494\n",
      "      mean_raw_obs_processing_ms: 0.33969953023213734\n",
      "  experiment_id: 37e018561af74bc58ad3fce6ef90e1f8\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.10000000000000002\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.3696840206782024\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0010078944615089587\n",
      "          policy_loss: 0.0001384727656841278\n",
      "          total_loss: 4941.75732421875\n",
      "          vf_explained_var: 1.390775050680304e-09\n",
      "          vf_loss: 4941.757161458333\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 6000\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.516666666666666\n",
      "    ram_util_percent: 51.06666666666666\n",
      "  pid: 32124\n",
      "  policy_reward_max:\n",
      "    firm: -325.23532325917756\n",
      "  policy_reward_mean:\n",
      "    firm: -835.4779842436184\n",
      "  policy_reward_min:\n",
      "    firm: -1811.1469124483442\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19649477962501988\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.19148697597712808\n",
      "    mean_inference_ms: 1.3040556317730596\n",
      "    mean_raw_obs_processing_ms: 0.3603843163609113\n",
      "  time_since_restore: 9.037156105041504\n",
      "  time_this_iter_s: 4.483776092529297\n",
      "  time_total_s: 9.037156105041504\n",
      "  timers:\n",
      "    learn_throughput: 2334.585\n",
      "    learn_time_ms: 428.342\n",
      "    sample_throughput: 483.859\n",
      "    sample_time_ms: 2066.716\n",
      "  timestamp: 1630715396\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: 2ac4e_00004\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.3/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/4 CPUs, 0/0 GPUs, 0.0/10.89 GiB heap, 0.0/5.44 GiB objects<br>Result logdir: /Users/matiascovarrubias/ray_results/native_3_firms_townsend_test_Sep3_PPO<br>Number of trials: 6/6 (6 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_len_mean</th><th style=\"text-align: right;\">  episodes_this_iter</th><th style=\"text-align: right;\">  num_healthy_workers</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_townsend_2ac4e_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         9.01969</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\"> -831.323</td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_2ac4e_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         8.92653</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">-2098.25 </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_2ac4e_00002</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         9.06163</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">-2826.31 </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_2ac4e_00003</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         9.08314</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">-2687.8  </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_2ac4e_00004</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         9.03716</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">-2506.43 </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_2ac4e_00005</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         8.97303</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\"> -514.456</td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=32123)\u001b[0m 2021-09-03 20:29:57,248\tERROR worker.py:382 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=32123)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=32123)\u001b[0m   File \"python/ray/_raylet.pyx\", line 495, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=32123)\u001b[0m   File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=32123)\u001b[0m   File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=32123)\u001b[0m   File \"/Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=32123)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=32123)\u001b[0m   File \"/Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/actor.py\", line 1001, in __ray_terminate__\n",
      "\u001b[2m\u001b[36m(pid=32123)\u001b[0m     ray.actor.exit_actor()\n",
      "\u001b[2m\u001b[36m(pid=32123)\u001b[0m   File \"/Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/actor.py\", line 1077, in exit_actor\n",
      "\u001b[2m\u001b[36m(pid=32123)\u001b[0m     raise exit\n",
      "\u001b[2m\u001b[36m(pid=32123)\u001b[0m SystemExit: 0\n",
      "\u001b[2m\u001b[36m(pid=32123)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32123)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=32123)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32123)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=32123)\u001b[0m   File \"python/ray/_raylet.pyx\", line 599, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=32123)\u001b[0m   File \"python/ray/_raylet.pyx\", line 451, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=32123)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=32123)\u001b[0m   File \"python/ray/includes/libcoreworker.pxi\", line 33, in ray._raylet.ProfileEvent.__exit__\n",
      "\u001b[2m\u001b[36m(pid=32123)\u001b[0m   File \"/Users/matiascovarrubias/.pyenv/versions/3.8.7/lib/python3.8/traceback.py\", line 167, in format_exc\n",
      "\u001b[2m\u001b[36m(pid=32123)\u001b[0m     return \"\".join(format_exception(*sys.exc_info(), limit=limit, chain=chain))\n",
      "\u001b[2m\u001b[36m(pid=32123)\u001b[0m   File \"/Users/matiascovarrubias/.pyenv/versions/3.8.7/lib/python3.8/traceback.py\", line 120, in format_exception\n",
      "\u001b[2m\u001b[36m(pid=32123)\u001b[0m     return list(TracebackException(\n",
      "\u001b[2m\u001b[36m(pid=32123)\u001b[0m   File \"/Users/matiascovarrubias/.pyenv/versions/3.8.7/lib/python3.8/traceback.py\", line 508, in __init__\n",
      "\u001b[2m\u001b[36m(pid=32123)\u001b[0m     self.stack = StackSummary.extract(\n",
      "\u001b[2m\u001b[36m(pid=32123)\u001b[0m   File \"/Users/matiascovarrubias/.pyenv/versions/3.8.7/lib/python3.8/traceback.py\", line 359, in extract\n",
      "\u001b[2m\u001b[36m(pid=32123)\u001b[0m     result.append(FrameSummary(\n",
      "\u001b[2m\u001b[36m(pid=32123)\u001b[0m   File \"/Users/matiascovarrubias/.pyenv/versions/3.8.7/lib/python3.8/traceback.py\", line 256, in __init__\n",
      "\u001b[2m\u001b[36m(pid=32123)\u001b[0m     self.name = name\n",
      "\u001b[2m\u001b[36m(pid=32123)\u001b[0m   File \"/Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/worker.py\", line 379, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=32123)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=32123)\u001b[0m SystemExit: 1\n",
      "2021-09-03 20:29:57,353\tINFO tune.py:549 -- Total run time: 27.90 seconds (27.20 seconds for the tuning loop).\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.0/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/4 CPUs, 0/0 GPUs, 0.0/10.89 GiB heap, 0.0/5.44 GiB objects<br>Result logdir: /Users/matiascovarrubias/ray_results/native_4_firms_townsend_test_Sep3_PPO<br>Number of trials: 6/6 (5 PENDING, 1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_townsend_3b7e5_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_3b7e5_00001</td><td>PENDING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_3b7e5_00002</td><td>PENDING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_3b7e5_00003</td><td>PENDING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_3b7e5_00004</td><td>PENDING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_3b7e5_00005</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(pid=32131)\u001b[0m WARNING:tensorflow:From /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=32131)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=32131)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=32129)\u001b[0m WARNING:tensorflow:From /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=32129)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=32129)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=32161)\u001b[0m WARNING:tensorflow:From /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=32161)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=32161)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=32162)\u001b[0m WARNING:tensorflow:From /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=32162)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=32162)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=32131)\u001b[0m 2021-09-03 20:30:01,407\tINFO trainer.py:694 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=32129)\u001b[0m 2021-09-03 20:30:01,443\tINFO trainer.py:694 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=32131)\u001b[0m 2021-09-03 20:30:01,588\tWARNING deprecation.py:33 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=32129)\u001b[0m 2021-09-03 20:30:01,625\tWARNING deprecation.py:33 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=32161)\u001b[0m 2021-09-03 20:30:02,085\tINFO trainer.py:694 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=32162)\u001b[0m 2021-09-03 20:30:02,118\tINFO trainer.py:694 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=32161)\u001b[0m 2021-09-03 20:30:02,261\tWARNING deprecation.py:33 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=32162)\u001b[0m 2021-09-03 20:30:02,299\tWARNING deprecation.py:33 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.3/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/4 CPUs, 0/0 GPUs, 0.0/10.89 GiB heap, 0.0/5.44 GiB objects<br>Result logdir: /Users/matiascovarrubias/ray_results/native_4_firms_townsend_test_Sep3_PPO<br>Number of trials: 6/6 (2 PENDING, 4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_townsend_3b7e5_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_3b7e5_00001</td><td>RUNNING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_3b7e5_00002</td><td>RUNNING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_3b7e5_00003</td><td>RUNNING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_3b7e5_00004</td><td>PENDING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_3b7e5_00005</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=32131)\u001b[0m 2021-09-03 20:30:04,283\tWARNING deprecation.py:33 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=32129)\u001b[0m 2021-09-03 20:30:04,298\tWARNING deprecation.py:33 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=32161)\u001b[0m 2021-09-03 20:30:05,099\tWARNING deprecation.py:33 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=32162)\u001b[0m 2021-09-03 20:30:05,142\tWARNING deprecation.py:33 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_townsend_3b7e5_00001:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: 36.703539079542175\n",
      "    discounted_rewards_mean: 36.703539079542175\n",
      "    discounted_rewards_min: 36.703539079542175\n",
      "  date: 2021-09-03_20-30-07\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - 15.219477499262263\n",
      "  episode_reward_mean: 15.219477499262263\n",
      "  episode_reward_min:\n",
      "  - 15.219477499262263\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: 53.24381124178576\n",
      "      discounted_rewards_mean: 53.24381124178576\n",
      "      discounted_rewards_min: 53.24381124178576\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -3857.545258350931\n",
      "    episode_reward_mean: -3857.545258350931\n",
      "    episode_reward_min:\n",
      "    - -3857.545258350931\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -3857.545258350931\n",
      "      policy_firm_reward:\n",
      "      - - -931.0199961215346\n",
      "      - - -1113.6157635178192\n",
      "      - - -727.4686483549096\n",
      "      - - -1085.4408503566544\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -727.4686483549096\n",
      "    policy_reward_mean:\n",
      "      firm: -964.3863145877294\n",
      "    policy_reward_min:\n",
      "      firm: -1113.6157635178192\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.28698141877348726\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.2705919873583448\n",
      "      mean_inference_ms: 1.5837836575198483\n",
      "      mean_raw_obs_processing_ms: 0.5317701326383577\n",
      "  experiment_id: cc918c7836f248e59ceab6a0a05f2f86\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.4289717972278595\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0012539822055259967\n",
      "          policy_loss: 0.0005690725520253181\n",
      "          total_loss: 4818.3304443359375\n",
      "          vf_explained_var: -5.960464760645934e-11\n",
      "          vf_loss: 4818.32958984375\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.36666666666666\n",
      "    ram_util_percent: 51.48888888888889\n",
      "  pid: 32129\n",
      "  policy_reward_max:\n",
      "    firm: 409.7803241027234\n",
      "  policy_reward_mean:\n",
      "    firm: 3.8048693748151052\n",
      "  policy_reward_min:\n",
      "    firm: -180.6138023689786\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.26779932218355373\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.24813562482744309\n",
      "    mean_inference_ms: 1.5068456724092558\n",
      "    mean_raw_obs_processing_ms: 0.48987300960453123\n",
      "  time_since_restore: 6.0435791015625\n",
      "  time_this_iter_s: 6.0435791015625\n",
      "  time_total_s: 6.0435791015625\n",
      "  timers:\n",
      "    learn_throughput: 1308.364\n",
      "    learn_time_ms: 764.313\n",
      "    sample_throughput: 393.69\n",
      "    sample_time_ms: 2540.071\n",
      "  timestamp: 1630715407\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: 3b7e5_00001\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.8/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/4 CPUs, 0/0 GPUs, 0.0/10.89 GiB heap, 0.0/5.44 GiB objects<br>Result logdir: /Users/matiascovarrubias/ray_results/native_4_firms_townsend_test_Sep3_PPO<br>Number of trials: 6/6 (2 PENDING, 4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_len_mean</th><th style=\"text-align: right;\">  episodes_this_iter</th><th style=\"text-align: right;\">  num_healthy_workers</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_townsend_3b7e5_00000</td><td>RUNNING </td><td>                   </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                     </td></tr>\n",
       "<tr><td>PPO_townsend_3b7e5_00001</td><td>RUNNING </td><td>192.168.1.202:32129</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         6.04358</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\"> 15.2195</td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_3b7e5_00002</td><td>RUNNING </td><td>                   </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                     </td></tr>\n",
       "<tr><td>PPO_townsend_3b7e5_00003</td><td>RUNNING </td><td>                   </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                     </td></tr>\n",
       "<tr><td>PPO_townsend_3b7e5_00004</td><td>PENDING </td><td>                   </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                     </td></tr>\n",
       "<tr><td>PPO_townsend_3b7e5_00005</td><td>PENDING </td><td>                   </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_townsend_3b7e5_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: 55.490958836544365\n",
      "    discounted_rewards_mean: 55.490958836544365\n",
      "    discounted_rewards_min: 55.490958836544365\n",
      "  date: 2021-09-03_20-30-07\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - -1084.764563397035\n",
      "  episode_reward_mean: -1084.764563397035\n",
      "  episode_reward_min:\n",
      "  - -1084.764563397035\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: 33.26464220096862\n",
      "      discounted_rewards_mean: 33.26464220096862\n",
      "      discounted_rewards_min: 33.26464220096862\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -4614.025699976236\n",
      "    episode_reward_mean: -4614.025699976236\n",
      "    episode_reward_min:\n",
      "    - -4614.025699976236\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -4614.025699976236\n",
      "      policy_firm_reward:\n",
      "      - - -1406.0657279885686\n",
      "      - - -931.7015342827992\n",
      "      - - -1281.7602808677145\n",
      "      - - -994.4981568371664\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -931.7015342827992\n",
      "    policy_reward_mean:\n",
      "      firm: -1153.5064249940624\n",
      "    policy_reward_min:\n",
      "      firm: -1406.0657279885686\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.2935630577308434\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.2681877944138381\n",
      "      mean_inference_ms: 1.5646837331674674\n",
      "      mean_raw_obs_processing_ms: 0.5278737394959777\n",
      "  experiment_id: f8219623f099400baf665d0ed490f216\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.4278120398521423\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0008261062175734075\n",
      "          policy_loss: 8.087558671832085e-05\n",
      "          total_loss: 3159.6962890625\n",
      "          vf_explained_var: -2.6226043559063328e-09\n",
      "          vf_loss: 3159.6959228515625\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.36666666666667\n",
      "    ram_util_percent: 51.511111111111106\n",
      "  pid: 32131\n",
      "  policy_reward_max:\n",
      "    firm: 115.84193185365656\n",
      "  policy_reward_mean:\n",
      "    firm: -271.19114084925866\n",
      "  policy_reward_min:\n",
      "    firm: -1341.6853785780236\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.27337036170921364\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.252937103484894\n",
      "    mean_inference_ms: 1.5057688588267202\n",
      "    mean_raw_obs_processing_ms: 0.5026111355075589\n",
      "  time_since_restore: 6.056132793426514\n",
      "  time_this_iter_s: 6.056132793426514\n",
      "  time_total_s: 6.056132793426514\n",
      "  timers:\n",
      "    learn_throughput: 1288.006\n",
      "    learn_time_ms: 776.394\n",
      "    sample_throughput: 390.464\n",
      "    sample_time_ms: 2561.055\n",
      "  timestamp: 1630715407\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: 3b7e5_00000\n",
      "  \n",
      "Result for PPO_townsend_3b7e5_00003:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: 88.52748349233954\n",
      "    discounted_rewards_mean: 88.52748349233954\n",
      "    discounted_rewards_min: 88.52748349233954\n",
      "  date: 2021-09-03_20-30-08\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - -1412.5021971496994\n",
      "  episode_reward_mean: -1412.5021971496994\n",
      "  episode_reward_min:\n",
      "  - -1412.5021971496994\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: -10.400946319327804\n",
      "      discounted_rewards_mean: -10.400946319327804\n",
      "      discounted_rewards_min: -10.400946319327804\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -4592.867910700091\n",
      "    episode_reward_mean: -4592.867910700091\n",
      "    episode_reward_min:\n",
      "    - -4592.867910700091\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -4592.867910700091\n",
      "      policy_firm_reward:\n",
      "      - - -1350.4341829047316\n",
      "      - - -1174.121841212584\n",
      "      - - -991.2323821676238\n",
      "      - - -1077.079504415149\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -991.2323821676238\n",
      "    policy_reward_mean:\n",
      "      firm: -1148.216977675022\n",
      "    policy_reward_min:\n",
      "      firm: -1350.4341829047316\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.2939148501797275\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.27301976016232304\n",
      "      mean_inference_ms: 1.580448655577211\n",
      "      mean_raw_obs_processing_ms: 0.5553881009737334\n",
      "  experiment_id: 0d2deba27b994304bae118b80901d774\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.4145446717739105\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0007645792575204063\n",
      "          policy_loss: 0.00010327901691198349\n",
      "          total_loss: 4683.8494873046875\n",
      "          vf_explained_var: -3.129243808519533e-10\n",
      "          vf_loss: 4683.849304199219\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.77777777777778\n",
      "    ram_util_percent: 51.71111111111111\n",
      "  pid: 32161\n",
      "  policy_reward_max:\n",
      "    firm: 173.99268607490794\n",
      "  policy_reward_mean:\n",
      "    firm: -353.1255492874244\n",
      "  policy_reward_min:\n",
      "    firm: -1114.4967887032703\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2837502634846843\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2658845899583815\n",
      "    mean_inference_ms: 1.5611186489596829\n",
      "    mean_raw_obs_processing_ms: 0.5618394552529989\n",
      "  time_since_restore: 6.205257892608643\n",
      "  time_this_iter_s: 6.205257892608643\n",
      "  time_total_s: 6.205257892608643\n",
      "  timers:\n",
      "    learn_throughput: 1364.727\n",
      "    learn_time_ms: 732.747\n",
      "    sample_throughput: 370.006\n",
      "    sample_time_ms: 2702.656\n",
      "  timestamp: 1630715408\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: 3b7e5_00003\n",
      "  \n",
      "Result for PPO_townsend_3b7e5_00002:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: -30.211564563253813\n",
      "    discounted_rewards_mean: -30.211564563253813\n",
      "    discounted_rewards_min: -30.211564563253813\n",
      "  date: 2021-09-03_20-30-08\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - -5461.074942000964\n",
      "  episode_reward_mean: -5461.074942000964\n",
      "  episode_reward_min:\n",
      "  - -5461.074942000964\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: -62.81234074331062\n",
      "      discounted_rewards_mean: -62.81234074331062\n",
      "      discounted_rewards_min: -62.81234074331062\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -5052.60128671703\n",
      "    episode_reward_mean: -5052.60128671703\n",
      "    episode_reward_min:\n",
      "    - -5052.60128671703\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -5052.60128671703\n",
      "      policy_firm_reward:\n",
      "      - - -1556.7503100870022\n",
      "      - - -1396.7281469122245\n",
      "      - - -1348.736656761589\n",
      "      - - -750.386172956211\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -750.386172956211\n",
      "    policy_reward_mean:\n",
      "      firm: -1263.1503216792569\n",
      "    policy_reward_min:\n",
      "      firm: -1556.7503100870022\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.2879372367134818\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.274708697369525\n",
      "      mean_inference_ms: 1.5881830876642888\n",
      "      mean_raw_obs_processing_ms: 0.5294197684639579\n",
      "  experiment_id: 7e3e117e3a3f4efb8c0284be5c3b8a63\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.417457789182663\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0008067408489296461\n",
      "          policy_loss: 0.0003766708541661501\n",
      "          total_loss: 5402.253662109375\n",
      "          vf_explained_var: -1.1920929521291868e-10\n",
      "          vf_loss: 5402.253173828125\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.77777777777778\n",
      "    ram_util_percent: 51.71111111111111\n",
      "  pid: 32162\n",
      "  policy_reward_max:\n",
      "    firm: -938.7335176808717\n",
      "  policy_reward_mean:\n",
      "    firm: -1365.2687355002402\n",
      "  policy_reward_min:\n",
      "    firm: -1966.7735110756519\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2773829868861607\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.26231712394661005\n",
      "    mean_inference_ms: 1.5639105995932778\n",
      "    mean_raw_obs_processing_ms: 0.5671144365430713\n",
      "  time_since_restore: 6.203644275665283\n",
      "  time_this_iter_s: 6.203644275665283\n",
      "  time_total_s: 6.203644275665283\n",
      "  timers:\n",
      "    learn_throughput: 1324.11\n",
      "    learn_time_ms: 755.224\n",
      "    sample_throughput: 370.204\n",
      "    sample_time_ms: 2701.21\n",
      "  timestamp: 1630715408\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: 3b7e5_00002\n",
      "  \n",
      "Result for PPO_townsend_3b7e5_00001:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: 66.32600986619842\n",
      "    discounted_rewards_mean: 51.5147744728703\n",
      "    discounted_rewards_min: 36.703539079542175\n",
      "  date: 2021-09-03_20-30-14\n",
      "  done: true\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - 1797.7111930092035\n",
      "  episode_reward_mean: 906.4653352542329\n",
      "  episode_reward_min:\n",
      "  - 15.219477499262263\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: -4.281216692071226\n",
      "      discounted_rewards_mean: -4.281216692071226\n",
      "      discounted_rewards_min: -4.281216692071226\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -4828.556821208395\n",
      "    episode_reward_mean: -4828.556821208395\n",
      "    episode_reward_min:\n",
      "    - -4828.556821208395\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -4828.556821208395\n",
      "      policy_firm_reward:\n",
      "      - - -1377.7209524401896\n",
      "      - - -644.5914988657413\n",
      "      - - -1599.5720145157024\n",
      "      - - -1206.6723553867757\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -644.5914988657413\n",
      "    policy_reward_mean:\n",
      "      firm: -1207.1392053021023\n",
      "    policy_reward_min:\n",
      "      firm: -1599.5720145157024\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.2959465634995613\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.2786305354631644\n",
      "      mean_inference_ms: 1.6414501975620466\n",
      "      mean_raw_obs_processing_ms: 0.5294775021546844\n",
      "  experiment_id: cc918c7836f248e59ceab6a0a05f2f86\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.1\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.3871270418167114\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0005788625348942205\n",
      "          policy_loss: 1.669576158747077e-05\n",
      "          total_loss: 3476.1640625\n",
      "          vf_explained_var: -2.5033950468156263e-09\n",
      "          vf_loss: 3476.164306640625\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.900000000000006\n",
      "    ram_util_percent: 52.68888888888889\n",
      "  pid: 32129\n",
      "  policy_reward_max:\n",
      "    firm: 725.8507711631363\n",
      "  policy_reward_mean:\n",
      "    firm: 226.61633381355858\n",
      "  policy_reward_min:\n",
      "    firm: -180.6138023689786\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2760941816789259\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.25588653754593693\n",
      "    mean_inference_ms: 1.5548714175436886\n",
      "    mean_raw_obs_processing_ms: 0.5066048273068153\n",
      "  time_since_restore: 12.53461217880249\n",
      "  time_this_iter_s: 6.49103307723999\n",
      "  time_total_s: 12.53461217880249\n",
      "  timers:\n",
      "    learn_throughput: 1304.804\n",
      "    learn_time_ms: 766.399\n",
      "    sample_throughput: 371.31\n",
      "    sample_time_ms: 2693.17\n",
      "  timestamp: 1630715414\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: 3b7e5_00001\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.8/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/4 CPUs, 0/0 GPUs, 0.0/10.89 GiB heap, 0.0/5.44 GiB objects<br>Result logdir: /Users/matiascovarrubias/ray_results/native_4_firms_townsend_test_Sep3_PPO<br>Number of trials: 6/6 (2 PENDING, 4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_len_mean</th><th style=\"text-align: right;\">  episodes_this_iter</th><th style=\"text-align: right;\">  num_healthy_workers</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_townsend_3b7e5_00000</td><td>RUNNING </td><td>192.168.1.202:32131</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         6.05613</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">-1084.76 </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_3b7e5_00001</td><td>RUNNING </td><td>192.168.1.202:32129</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        12.5346 </td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">  906.465</td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_3b7e5_00002</td><td>RUNNING </td><td>192.168.1.202:32162</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         6.20364</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">-5461.07 </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_3b7e5_00003</td><td>RUNNING </td><td>192.168.1.202:32161</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         6.20526</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">-1412.5  </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_3b7e5_00004</td><td>PENDING </td><td>                   </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                     </td></tr>\n",
       "<tr><td>PPO_townsend_3b7e5_00005</td><td>PENDING </td><td>                   </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_townsend_3b7e5_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: 55.490958836544365\n",
      "    discounted_rewards_mean: -6.006102793310603\n",
      "    discounted_rewards_min: -67.50316442316557\n",
      "  date: 2021-09-03_20-30-14\n",
      "  done: true\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - -1084.764563397035\n",
      "  episode_reward_mean: -1570.6715680864604\n",
      "  episode_reward_min:\n",
      "  - -2056.5785727758857\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: 69.1541121888637\n",
      "      discounted_rewards_mean: 69.1541121888637\n",
      "      discounted_rewards_min: 69.1541121888637\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -4718.330721669589\n",
      "    episode_reward_mean: -4718.330721669589\n",
      "    episode_reward_min:\n",
      "    - -4718.330721669589\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -4718.330721669589\n",
      "      policy_firm_reward:\n",
      "      - - -840.1581245776529\n",
      "      - - -1145.2326502649619\n",
      "      - - -1710.9321751724842\n",
      "      - - -1022.0077716544774\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -840.1581245776529\n",
      "    policy_reward_mean:\n",
      "      firm: -1179.5826804173942\n",
      "    policy_reward_min:\n",
      "      firm: -1710.9321751724842\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.30184268713116585\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.27603664617428836\n",
      "      mean_inference_ms: 1.6276922183058251\n",
      "      mean_raw_obs_processing_ms: 0.5244278180962619\n",
      "  experiment_id: f8219623f099400baf665d0ed490f216\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.1\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.4356692731380463\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0003258633987571322\n",
      "          policy_loss: 6.673391908407211e-06\n",
      "          total_loss: 4034.9868774414062\n",
      "          vf_explained_var: -4.678964771187566e-09\n",
      "          vf_loss: 4034.98681640625\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.91111111111111\n",
      "    ram_util_percent: 52.68888888888889\n",
      "  pid: 32131\n",
      "  policy_reward_max:\n",
      "    firm: 115.84193185365656\n",
      "  policy_reward_mean:\n",
      "    firm: -392.6678920216144\n",
      "  policy_reward_min:\n",
      "    firm: -1341.6853785780236\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2830415414011402\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2595364424835084\n",
      "    mean_inference_ms: 1.5574241713755699\n",
      "    mean_raw_obs_processing_ms: 0.5129075834421022\n",
      "  time_since_restore: 12.55347990989685\n",
      "  time_this_iter_s: 6.497347116470337\n",
      "  time_total_s: 12.55347990989685\n",
      "  timers:\n",
      "    learn_throughput: 1288.528\n",
      "    learn_time_ms: 776.08\n",
      "    sample_throughput: 369.106\n",
      "    sample_time_ms: 2709.245\n",
      "  timestamp: 1630715414\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: 3b7e5_00000\n",
      "  \n",
      "Result for PPO_townsend_3b7e5_00003:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: 171.68796769333417\n",
      "    discounted_rewards_mean: 130.10772559283686\n",
      "    discounted_rewards_min: 88.52748349233954\n",
      "  date: 2021-09-03_20-30-15\n",
      "  done: true\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - -1007.8382828949423\n",
      "  episode_reward_mean: -1210.1702400223207\n",
      "  episode_reward_min:\n",
      "  - -1412.5021971496994\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: -15.974297566294869\n",
      "      discounted_rewards_mean: -15.974297566294869\n",
      "      discounted_rewards_min: -15.974297566294869\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -5461.148513109238\n",
      "    episode_reward_mean: -5461.148513109238\n",
      "    episode_reward_min:\n",
      "    - -5461.148513109238\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -5461.148513109238\n",
      "      policy_firm_reward:\n",
      "      - - -1901.8614373202938\n",
      "      - - -1080.1896222630824\n",
      "      - - -1349.542407266803\n",
      "      - - -1129.555046259053\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -1080.1896222630824\n",
      "    policy_reward_mean:\n",
      "      firm: -1365.287128277308\n",
      "    policy_reward_min:\n",
      "      firm: -1901.8614373202938\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.30033246449742657\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.27823066902065324\n",
      "      mean_inference_ms: 1.62862504142216\n",
      "      mean_raw_obs_processing_ms: 0.5577536358468715\n",
      "  experiment_id: 0d2deba27b994304bae118b80901d774\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.1\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.4043685793876648\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0005544976593228875\n",
      "          policy_loss: 5.8022793382406235e-05\n",
      "          total_loss: 5151.722412109375\n",
      "          vf_explained_var: 5.960464760645934e-11\n",
      "          vf_loss: 5151.7220458984375\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.06666666666666\n",
      "    ram_util_percent: 52.67777777777778\n",
      "  pid: 32161\n",
      "  policy_reward_max:\n",
      "    firm: 173.99268607490794\n",
      "  policy_reward_mean:\n",
      "    firm: -302.54256000557945\n",
      "  policy_reward_min:\n",
      "    firm: -1114.4967887032703\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2890357528630562\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.26880746357210983\n",
      "    mean_inference_ms: 1.5959418692037257\n",
      "    mean_raw_obs_processing_ms: 0.5622248167353874\n",
      "  time_since_restore: 12.705751895904541\n",
      "  time_this_iter_s: 6.500494003295898\n",
      "  time_total_s: 12.705751895904541\n",
      "  timers:\n",
      "    learn_throughput: 1347.397\n",
      "    learn_time_ms: 742.172\n",
      "    sample_throughput: 359.672\n",
      "    sample_time_ms: 2780.308\n",
      "  timestamp: 1630715415\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: 3b7e5_00003\n",
      "  \n",
      "Result for PPO_townsend_3b7e5_00002:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: 81.62937070781682\n",
      "    discounted_rewards_mean: 25.7089030722815\n",
      "    discounted_rewards_min: -30.211564563253813\n",
      "  date: 2021-09-03_20-30-15\n",
      "  done: true\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - -1847.845449995326\n",
      "  episode_reward_mean: -3654.460195998145\n",
      "  episode_reward_min:\n",
      "  - -5461.074942000964\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: 24.09878857831473\n",
      "      discounted_rewards_mean: 24.09878857831473\n",
      "      discounted_rewards_min: 24.09878857831473\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -5273.328948879678\n",
      "    episode_reward_mean: -5273.328948879678\n",
      "    episode_reward_min:\n",
      "    - -5273.328948879678\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -5273.328948879678\n",
      "      policy_firm_reward:\n",
      "      - - -1329.1333399158846\n",
      "      - - -1439.741513598411\n",
      "      - - -1196.5746354461687\n",
      "      - - -1307.8794599192324\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -1196.5746354461687\n",
      "    policy_reward_mean:\n",
      "      firm: -1318.3322372199243\n",
      "    policy_reward_min:\n",
      "      firm: -1439.741513598411\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.2962136971599039\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.28177823262593554\n",
      "      mean_inference_ms: 1.6349926404748065\n",
      "      mean_raw_obs_processing_ms: 0.5494914371808847\n",
      "  experiment_id: 7e3e117e3a3f4efb8c0284be5c3b8a63\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.1\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.3908238112926483\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.000445439822215171\n",
      "          policy_loss: 5.1779641580651514e-05\n",
      "          total_loss: 4388.2596435546875\n",
      "          vf_explained_var: -9.983778248567887e-10\n",
      "          vf_loss: 4388.259765625\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.01111111111111\n",
      "    ram_util_percent: 52.67777777777778\n",
      "  pid: 32162\n",
      "  policy_reward_max:\n",
      "    firm: 174.1228296662419\n",
      "  policy_reward_mean:\n",
      "    firm: -913.6150489995359\n",
      "  policy_reward_min:\n",
      "    firm: -1966.7735110756519\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2830873050091906\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.26688212113031096\n",
      "    mean_inference_ms: 1.5928843260595693\n",
      "    mean_raw_obs_processing_ms: 0.563871458353033\n",
      "  time_since_restore: 12.717868328094482\n",
      "  time_this_iter_s: 6.514224052429199\n",
      "  time_total_s: 12.717868328094482\n",
      "  timers:\n",
      "    learn_throughput: 1300.462\n",
      "    learn_time_ms: 768.957\n",
      "    sample_throughput: 361.977\n",
      "    sample_time_ms: 2762.608\n",
      "  timestamp: 1630715415\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: 3b7e5_00002\n",
      "  \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(pid=32240)\u001b[0m WARNING:tensorflow:From /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=32240)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=32240)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=32241)\u001b[0m WARNING:tensorflow:From /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=32241)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=32241)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=32240)\u001b[0m 2021-09-03 20:30:19,775\tINFO trainer.py:694 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=32241)\u001b[0m 2021-09-03 20:30:19,792\tINFO trainer.py:694 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=32240)\u001b[0m 2021-09-03 20:30:19,961\tWARNING deprecation.py:33 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=32241)\u001b[0m 2021-09-03 20:30:19,974\tWARNING deprecation.py:33 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=32240)\u001b[0m 2021-09-03 20:30:22,602\tWARNING deprecation.py:33 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=32241)\u001b[0m 2021-09-03 20:30:22,629\tWARNING deprecation.py:33 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_townsend_3b7e5_00005:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: 148.86716861849715\n",
      "    discounted_rewards_mean: 148.86716861849715\n",
      "    discounted_rewards_min: 148.86716861849715\n",
      "  date: 2021-09-03_20-30-25\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - -3901.289620686341\n",
      "  episode_reward_mean: -3901.289620686341\n",
      "  episode_reward_min:\n",
      "  - -3901.289620686341\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: 95.65662219532773\n",
      "      discounted_rewards_mean: 95.65662219532773\n",
      "      discounted_rewards_min: 95.65662219532773\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -4312.2142198803\n",
      "    episode_reward_mean: -4312.2142198803\n",
      "    episode_reward_min:\n",
      "    - -4312.2142198803\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -4312.2142198803\n",
      "      policy_firm_reward:\n",
      "      - - -1193.3638524381413\n",
      "      - - -1021.6017666581891\n",
      "      - - -951.2406407444535\n",
      "      - - -1146.0079600395206\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -951.2406407444535\n",
      "    policy_reward_mean:\n",
      "      firm: -1078.0535549700762\n",
      "    policy_reward_min:\n",
      "      firm: -1193.3638524381413\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.2599176946100774\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.2417414338438661\n",
      "      mean_inference_ms: 1.3956211425445892\n",
      "      mean_raw_obs_processing_ms: 0.4474340261636557\n",
      "  experiment_id: 6598df4153264289bbb4228bcc50c3b0\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.4155164659023285\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0009606164167053492\n",
      "          policy_loss: 0.0001493064919486642\n",
      "          total_loss: 4334.7333984375\n",
      "          vf_explained_var: -6.58631327254966e-09\n",
      "          vf_loss: 4334.73291015625\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.075\n",
      "    ram_util_percent: 50.587500000000006\n",
      "  pid: 32240\n",
      "  policy_reward_max:\n",
      "    firm: -274.5281906197386\n",
      "  policy_reward_mean:\n",
      "    firm: -975.3224051715847\n",
      "  policy_reward_min:\n",
      "    firm: -2007.187336434584\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.26853791959993134\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.251805270230258\n",
      "    mean_inference_ms: 1.4738029056972082\n",
      "    mean_raw_obs_processing_ms: 0.4849653024892588\n",
      "  time_since_restore: 5.5717902183532715\n",
      "  time_this_iter_s: 5.5717902183532715\n",
      "  time_total_s: 5.5717902183532715\n",
      "  timers:\n",
      "    learn_throughput: 1508.924\n",
      "    learn_time_ms: 662.724\n",
      "    sample_throughput: 399.151\n",
      "    sample_time_ms: 2505.319\n",
      "  timestamp: 1630715425\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: 3b7e5_00005\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.3/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/0 GPUs, 0.0/10.89 GiB heap, 0.0/5.44 GiB objects<br>Result logdir: /Users/matiascovarrubias/ray_results/native_4_firms_townsend_test_Sep3_PPO<br>Number of trials: 6/6 (2 RUNNING, 4 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_len_mean</th><th style=\"text-align: right;\">  episodes_this_iter</th><th style=\"text-align: right;\">  num_healthy_workers</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_townsend_3b7e5_00004</td><td>RUNNING   </td><td>                   </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                     </td></tr>\n",
       "<tr><td>PPO_townsend_3b7e5_00005</td><td>RUNNING   </td><td>192.168.1.202:32240</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.57179</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">-3901.29 </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_3b7e5_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        12.5535 </td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">-1570.67 </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_3b7e5_00001</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        12.5346 </td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">  906.465</td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_3b7e5_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        12.7179 </td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">-3654.46 </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_3b7e5_00003</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        12.7058 </td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">-1210.17 </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_townsend_3b7e5_00004:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: 82.80791972122567\n",
      "    discounted_rewards_mean: 82.80791972122567\n",
      "    discounted_rewards_min: 82.80791972122567\n",
      "  date: 2021-09-03_20-30-25\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - -2440.3380725972197\n",
      "  episode_reward_mean: -2440.3380725972197\n",
      "  episode_reward_min:\n",
      "  - -2440.3380725972197\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: 22.67412646014813\n",
      "      discounted_rewards_mean: 22.67412646014813\n",
      "      discounted_rewards_min: 22.67412646014813\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -4373.028438955171\n",
      "    episode_reward_mean: -4373.028438955171\n",
      "    episode_reward_min:\n",
      "    - -4373.028438955171\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -4373.028438955171\n",
      "      policy_firm_reward:\n",
      "      - - -1314.668358715444\n",
      "      - - -1035.774145925823\n",
      "      - - -1072.7208208817844\n",
      "      - - -949.8651134321168\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -949.8651134321168\n",
      "    policy_reward_mean:\n",
      "      firm: -1093.2571097387922\n",
      "    policy_reward_min:\n",
      "      firm: -1314.668358715444\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.25642930449067536\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.24266771741442153\n",
      "      mean_inference_ms: 1.4097149912770335\n",
      "      mean_raw_obs_processing_ms: 0.4611784642512029\n",
      "  experiment_id: 038e0c9b3a94455cb8dd5cb66c844e78\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.415470838546753\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0011555022574612616\n",
      "          policy_loss: 0.00036864704452455044\n",
      "          total_loss: 5409.4703369140625\n",
      "          vf_explained_var: 6.258487617039066e-10\n",
      "          vf_loss: 5409.4696044921875\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.975\n",
      "    ram_util_percent: 50.587500000000006\n",
      "  pid: 32241\n",
      "  policy_reward_max:\n",
      "    firm: 97.4782179109062\n",
      "  policy_reward_mean:\n",
      "    firm: -610.084518149305\n",
      "  policy_reward_min:\n",
      "    firm: -944.9602265717073\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2652386447170993\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.24806679069221796\n",
      "    mean_inference_ms: 1.4891631596095556\n",
      "    mean_raw_obs_processing_ms: 0.48921444080211784\n",
      "  time_since_restore: 5.565076112747192\n",
      "  time_this_iter_s: 5.565076112747192\n",
      "  time_total_s: 5.565076112747192\n",
      "  timers:\n",
      "    learn_throughput: 1631.596\n",
      "    learn_time_ms: 612.897\n",
      "    sample_throughput: 397.047\n",
      "    sample_time_ms: 2518.595\n",
      "  timestamp: 1630715425\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: 3b7e5_00004\n",
      "  \n",
      "Result for PPO_townsend_3b7e5_00005:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: 148.86716861849715\n",
      "    discounted_rewards_mean: 69.40427636361314\n",
      "    discounted_rewards_min: -10.058615891270872\n",
      "  date: 2021-09-03_20-30-30\n",
      "  done: true\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - -781.2974615926099\n",
      "  episode_reward_mean: -2341.2935411394756\n",
      "  episode_reward_min:\n",
      "  - -3901.289620686341\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: -58.52445214635774\n",
      "      discounted_rewards_mean: -58.52445214635774\n",
      "      discounted_rewards_min: -58.52445214635774\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -5168.265209316794\n",
      "    episode_reward_mean: -5168.265209316794\n",
      "    episode_reward_min:\n",
      "    - -5168.265209316794\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -5168.265209316794\n",
      "      policy_firm_reward:\n",
      "      - - -1683.398012864174\n",
      "      - - -984.7080772826561\n",
      "      - - -1493.931501097697\n",
      "      - - -1006.2276180722816\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -984.7080772826561\n",
      "    policy_reward_mean:\n",
      "      firm: -1292.0663023292022\n",
      "    policy_reward_min:\n",
      "      firm: -1683.398012864174\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.2540010979388846\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.2355421858391483\n",
      "      mean_inference_ms: 1.3663927237431088\n",
      "      mean_raw_obs_processing_ms: 0.4370841665425222\n",
      "  experiment_id: 6598df4153264289bbb4228bcc50c3b0\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.1\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.4391274154186249\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00019239454195493623\n",
      "          policy_loss: 6.187765393406153e-06\n",
      "          total_loss: 2792.187744140625\n",
      "          vf_explained_var: -1.043081288010228e-09\n",
      "          vf_loss: 2792.1876220703125\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.714285714285714\n",
      "    ram_util_percent: 51.22857142857142\n",
      "  pid: 32240\n",
      "  policy_reward_max:\n",
      "    firm: -1.555869011843468\n",
      "  policy_reward_mean:\n",
      "    firm: -585.3233852848683\n",
      "  policy_reward_min:\n",
      "    firm: -2007.187336434584\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2641184380225853\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.24617611034269465\n",
      "    mean_inference_ms: 1.451828873114454\n",
      "    mean_raw_obs_processing_ms: 0.4767749565766436\n",
      "  time_since_restore: 10.75347113609314\n",
      "  time_this_iter_s: 5.181680917739868\n",
      "  time_total_s: 10.75347113609314\n",
      "  timers:\n",
      "    learn_throughput: 1628.642\n",
      "    learn_time_ms: 614.008\n",
      "    sample_throughput: 413.96\n",
      "    sample_time_ms: 2415.693\n",
      "  timestamp: 1630715430\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: 3b7e5_00005\n",
      "  \n",
      "Result for PPO_townsend_3b7e5_00004:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: 192.58513633195074\n",
      "    discounted_rewards_mean: 137.6965280265882\n",
      "    discounted_rewards_min: 82.80791972122567\n",
      "  date: 2021-09-03_20-30-31\n",
      "  done: true\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - -2151.263251348961\n",
      "  episode_reward_mean: -2295.8006619730904\n",
      "  episode_reward_min:\n",
      "  - -2440.3380725972197\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: -3.9782973914271826\n",
      "      discounted_rewards_mean: -3.9782973914271826\n",
      "      discounted_rewards_min: -3.9782973914271826\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -4775.84072105678\n",
      "    episode_reward_mean: -4775.84072105678\n",
      "    episode_reward_min:\n",
      "    - -4775.84072105678\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -4775.84072105678\n",
      "      policy_firm_reward:\n",
      "      - - -1199.8740431245778\n",
      "      - - -1214.1049489919574\n",
      "      - - -1176.396537894177\n",
      "      - - -1185.4651910460664\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -1176.396537894177\n",
      "    policy_reward_mean:\n",
      "      firm: -1193.9601802641946\n",
      "    policy_reward_min:\n",
      "      firm: -1214.1049489919574\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.2505589818311059\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.236665529349278\n",
      "      mean_inference_ms: 1.3805365097755078\n",
      "      mean_raw_obs_processing_ms: 0.44685575379424547\n",
      "  experiment_id: 038e0c9b3a94455cb8dd5cb66c844e78\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.1\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.4724054634571075\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00047425413504254975\n",
      "          policy_loss: 5.655211862176657e-05\n",
      "          total_loss: 2207.1517333984375\n",
      "          vf_explained_var: -3.8594007989445345e-09\n",
      "          vf_loss: 2207.1515502929688\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.674999999999999\n",
      "    ram_util_percent: 51.25\n",
      "  pid: 32241\n",
      "  policy_reward_max:\n",
      "    firm: 97.4782179109062\n",
      "  policy_reward_mean:\n",
      "    firm: -573.9501654932731\n",
      "  policy_reward_min:\n",
      "    firm: -944.9602265717073\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.2606856059510674\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2446841581452514\n",
      "    mean_inference_ms: 1.466551165233011\n",
      "    mean_raw_obs_processing_ms: 0.47441133470514346\n",
      "  time_since_restore: 10.757390975952148\n",
      "  time_this_iter_s: 5.192314863204956\n",
      "  time_total_s: 10.757390975952148\n",
      "  timers:\n",
      "    learn_throughput: 1702.173\n",
      "    learn_time_ms: 587.484\n",
      "    sample_throughput: 413.529\n",
      "    sample_time_ms: 2418.208\n",
      "  timestamp: 1630715431\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: 3b7e5_00004\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.4/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/4 CPUs, 0/0 GPUs, 0.0/10.89 GiB heap, 0.0/5.44 GiB objects<br>Result logdir: /Users/matiascovarrubias/ray_results/native_4_firms_townsend_test_Sep3_PPO<br>Number of trials: 6/6 (1 RUNNING, 5 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_len_mean</th><th style=\"text-align: right;\">  episodes_this_iter</th><th style=\"text-align: right;\">  num_healthy_workers</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_townsend_3b7e5_00004</td><td>RUNNING   </td><td>192.168.1.202:32241</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         10.7574</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">-2295.8  </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_3b7e5_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         12.5535</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">-1570.67 </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_3b7e5_00001</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         12.5346</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">  906.465</td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_3b7e5_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         12.7179</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">-3654.46 </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_3b7e5_00003</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         12.7058</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">-1210.17 </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_3b7e5_00005</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         10.7535</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">-2341.29 </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.4/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/4 CPUs, 0/0 GPUs, 0.0/10.89 GiB heap, 0.0/5.44 GiB objects<br>Result logdir: /Users/matiascovarrubias/ray_results/native_4_firms_townsend_test_Sep3_PPO<br>Number of trials: 6/6 (6 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_len_mean</th><th style=\"text-align: right;\">  episodes_this_iter</th><th style=\"text-align: right;\">  num_healthy_workers</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_townsend_3b7e5_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         12.5535</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">-1570.67 </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_3b7e5_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         12.5346</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">  906.465</td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_3b7e5_00002</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         12.7179</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">-3654.46 </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_3b7e5_00003</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         12.7058</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">-1210.17 </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_3b7e5_00004</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         10.7574</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">-2295.8  </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_3b7e5_00005</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         10.7535</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">-2341.29 </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=32240)\u001b[0m 2021-09-03 20:30:31,618\tERROR worker.py:382 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=32240)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=32240)\u001b[0m   File \"python/ray/_raylet.pyx\", line 495, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=32240)\u001b[0m   File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=32240)\u001b[0m   File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=32240)\u001b[0m   File \"/Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=32240)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=32240)\u001b[0m   File \"/Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/actor.py\", line 1001, in __ray_terminate__\n",
      "\u001b[2m\u001b[36m(pid=32240)\u001b[0m     ray.actor.exit_actor()\n",
      "\u001b[2m\u001b[36m(pid=32240)\u001b[0m   File \"/Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/actor.py\", line 1077, in exit_actor\n",
      "\u001b[2m\u001b[36m(pid=32240)\u001b[0m     raise exit\n",
      "\u001b[2m\u001b[36m(pid=32240)\u001b[0m SystemExit: 0\n",
      "\u001b[2m\u001b[36m(pid=32240)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32240)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=32240)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=32240)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=32240)\u001b[0m   File \"python/ray/_raylet.pyx\", line 599, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=32240)\u001b[0m   File \"python/ray/_raylet.pyx\", line 451, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=32240)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=32240)\u001b[0m   File \"python/ray/includes/libcoreworker.pxi\", line 33, in ray._raylet.ProfileEvent.__exit__\n",
      "\u001b[2m\u001b[36m(pid=32240)\u001b[0m   File \"/Users/matiascovarrubias/.pyenv/versions/3.8.7/lib/python3.8/traceback.py\", line 167, in format_exc\n",
      "\u001b[2m\u001b[36m(pid=32240)\u001b[0m     return \"\".join(format_exception(*sys.exc_info(), limit=limit, chain=chain))\n",
      "\u001b[2m\u001b[36m(pid=32240)\u001b[0m   File \"/Users/matiascovarrubias/.pyenv/versions/3.8.7/lib/python3.8/traceback.py\", line 120, in format_exception\n",
      "\u001b[2m\u001b[36m(pid=32240)\u001b[0m     return list(TracebackException(\n",
      "\u001b[2m\u001b[36m(pid=32240)\u001b[0m   File \"/Users/matiascovarrubias/.pyenv/versions/3.8.7/lib/python3.8/traceback.py\", line 508, in __init__\n",
      "\u001b[2m\u001b[36m(pid=32240)\u001b[0m     self.stack = StackSummary.extract(\n",
      "\u001b[2m\u001b[36m(pid=32240)\u001b[0m   File \"/Users/matiascovarrubias/.pyenv/versions/3.8.7/lib/python3.8/traceback.py\", line 321, in extract\n",
      "\u001b[2m\u001b[36m(pid=32240)\u001b[0m     @classmethod\n",
      "\u001b[2m\u001b[36m(pid=32240)\u001b[0m   File \"/Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/worker.py\", line 379, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=32240)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=32240)\u001b[0m SystemExit: 1\n",
      "2021-09-03 20:30:31,723\tINFO tune.py:549 -- Total run time: 34.21 seconds (33.81 seconds for the tuning loop).\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.1/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/4 CPUs, 0/0 GPUs, 0.0/10.89 GiB heap, 0.0/5.44 GiB objects<br>Result logdir: /Users/matiascovarrubias/ray_results/native_5_firms_townsend_test_Sep3_PPO<br>Number of trials: 6/6 (5 PENDING, 1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_townsend_50129_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_50129_00001</td><td>PENDING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_50129_00002</td><td>PENDING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_50129_00003</td><td>PENDING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_50129_00004</td><td>PENDING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_50129_00005</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(pid=32248)\u001b[0m WARNING:tensorflow:From /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=32248)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=32248)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=32246)\u001b[0m WARNING:tensorflow:From /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=32246)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=32246)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=32282)\u001b[0m WARNING:tensorflow:From /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=32282)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=32282)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=32283)\u001b[0m WARNING:tensorflow:From /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=32283)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=32283)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=32248)\u001b[0m 2021-09-03 20:30:36,163\tINFO trainer.py:694 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=32246)\u001b[0m 2021-09-03 20:30:36,225\tINFO trainer.py:694 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=32248)\u001b[0m 2021-09-03 20:30:36,349\tWARNING deprecation.py:33 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=32246)\u001b[0m 2021-09-03 20:30:36,409\tWARNING deprecation.py:33 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=32282)\u001b[0m 2021-09-03 20:30:36,869\tINFO trainer.py:694 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=32283)\u001b[0m 2021-09-03 20:30:36,909\tINFO trainer.py:694 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=32282)\u001b[0m 2021-09-03 20:30:37,053\tWARNING deprecation.py:33 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=32283)\u001b[0m 2021-09-03 20:30:37,097\tWARNING deprecation.py:33 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.3/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/4 CPUs, 0/0 GPUs, 0.0/10.89 GiB heap, 0.0/5.44 GiB objects<br>Result logdir: /Users/matiascovarrubias/ray_results/native_5_firms_townsend_test_Sep3_PPO<br>Number of trials: 6/6 (2 PENDING, 4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_townsend_50129_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_50129_00001</td><td>RUNNING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_50129_00002</td><td>RUNNING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_50129_00003</td><td>RUNNING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_50129_00004</td><td>PENDING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_50129_00005</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=32248)\u001b[0m 2021-09-03 20:30:39,254\tWARNING deprecation.py:33 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=32246)\u001b[0m 2021-09-03 20:30:39,315\tWARNING deprecation.py:33 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=32282)\u001b[0m 2021-09-03 20:30:40,006\tWARNING deprecation.py:33 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=32283)\u001b[0m 2021-09-03 20:30:40,027\tWARNING deprecation.py:33 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.0/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/4 CPUs, 0/0 GPUs, 0.0/10.89 GiB heap, 0.0/5.44 GiB objects<br>Result logdir: /Users/matiascovarrubias/ray_results/native_5_firms_townsend_test_Sep3_PPO<br>Number of trials: 6/6 (2 PENDING, 4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_townsend_50129_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_50129_00001</td><td>RUNNING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_50129_00002</td><td>RUNNING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_50129_00003</td><td>RUNNING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_50129_00004</td><td>PENDING </td><td>     </td></tr>\n",
       "<tr><td>PPO_townsend_50129_00005</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_townsend_50129_00000:\n",
      "  agent_timesteps_total: 5000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: 22.103949578820654\n",
      "    discounted_rewards_mean: 22.103949578820654\n",
      "    discounted_rewards_min: 22.103949578820654\n",
      "  date: 2021-09-03_20-30-42\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - -3785.856053772891\n",
      "  episode_reward_mean: -3785.856053772891\n",
      "  episode_reward_min:\n",
      "  - -3785.856053772891\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: 7.375903190873078\n",
      "      discounted_rewards_mean: 7.375903190873078\n",
      "      discounted_rewards_min: 7.375903190873078\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -5817.718802726897\n",
      "    episode_reward_mean: -5817.718802726897\n",
      "    episode_reward_min:\n",
      "    - -5817.718802726897\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -5817.718802726897\n",
      "      policy_firm_reward:\n",
      "      - - -1547.6833157978024\n",
      "      - - -770.2883884764245\n",
      "      - - -619.1128105450484\n",
      "      - - -1403.9009970488075\n",
      "      - - -1476.733290858816\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -619.1128105450484\n",
      "    policy_reward_mean:\n",
      "      firm: -1163.5437605453797\n",
      "    policy_reward_min:\n",
      "      firm: -1547.6833157978024\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.3184574348228676\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.28126604192621346\n",
      "      mean_inference_ms: 1.4570271456753696\n",
      "      mean_raw_obs_processing_ms: 0.5703284428431676\n",
      "  experiment_id: 03636c60d9034daf8bdcad83b5c0a3c5\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.4208724975585938\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00102032453869474\n",
      "          policy_loss: 0.0003303876146674156\n",
      "          total_loss: 3321.016259765625\n",
      "          vf_explained_var: -3.7193297330873065e-09\n",
      "          vf_loss: 3321.015771484375\n",
      "    num_agent_steps_sampled: 5000\n",
      "    num_agent_steps_trained: 5000\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.211111111111112\n",
      "    ram_util_percent: 52.01111111111111\n",
      "  pid: 32248\n",
      "  policy_reward_max:\n",
      "    firm: -274.56188327868864\n",
      "  policy_reward_mean:\n",
      "    firm: -757.171210754577\n",
      "  policy_reward_min:\n",
      "    firm: -1515.506400171593\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3261223182335243\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2908330340009112\n",
      "    mean_inference_ms: 1.511915580376045\n",
      "    mean_raw_obs_processing_ms: 0.6037037093918045\n",
      "  time_since_restore: 6.347454071044922\n",
      "  time_this_iter_s: 6.347454071044922\n",
      "  time_total_s: 6.347454071044922\n",
      "  timers:\n",
      "    learn_throughput: 1133.931\n",
      "    learn_time_ms: 881.888\n",
      "    sample_throughput: 361.942\n",
      "    sample_time_ms: 2762.876\n",
      "  timestamp: 1630715442\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: '50129_00000'\n",
      "  \n",
      "Result for PPO_townsend_50129_00001:\n",
      "  agent_timesteps_total: 5000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: -13.621124467587926\n",
      "    discounted_rewards_mean: -13.621124467587926\n",
      "    discounted_rewards_min: -13.621124467587926\n",
      "  date: 2021-09-03_20-30-42\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - -3028.2226600148874\n",
      "  episode_reward_mean: -3028.2226600148874\n",
      "  episode_reward_min:\n",
      "  - -3028.2226600148874\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: 103.43277632628858\n",
      "      discounted_rewards_mean: 103.43277632628858\n",
      "      discounted_rewards_min: 103.43277632628858\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -5893.958946202721\n",
      "    episode_reward_mean: -5893.958946202721\n",
      "    episode_reward_min:\n",
      "    - -5893.958946202721\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -5893.958946202721\n",
      "      policy_firm_reward:\n",
      "      - - -899.1015842266721\n",
      "      - - -1084.4793493899476\n",
      "      - - -1509.3746685234619\n",
      "      - - -1201.3706634419536\n",
      "      - - -1199.6326806206823\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -899.1015842266721\n",
      "    policy_reward_mean:\n",
      "      firm: -1178.7917892405435\n",
      "    policy_reward_min:\n",
      "      firm: -1509.3746685234619\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.3157107384650262\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.283116465443736\n",
      "      mean_inference_ms: 1.4543588106687015\n",
      "      mean_raw_obs_processing_ms: 0.5744932652948858\n",
      "  experiment_id: 9686e01bb6f3415b9bdb57ded3ffb166\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.4215949296951294\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0011137781199068088\n",
      "          policy_loss: 0.00027224626392126084\n",
      "          total_loss: 3966.981689453125\n",
      "          vf_explained_var: -3.2305718100644754e-09\n",
      "          vf_loss: 3966.9810546875\n",
      "    num_agent_steps_sampled: 5000\n",
      "    num_agent_steps_trained: 5000\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.1\n",
      "    ram_util_percent: 51.98888888888889\n",
      "  pid: 32246\n",
      "  policy_reward_max:\n",
      "    firm: -94.79923780327908\n",
      "  policy_reward_mean:\n",
      "    firm: -605.6445320029788\n",
      "  policy_reward_min:\n",
      "    firm: -1569.6829831644304\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3271874609765235\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2906070008025422\n",
      "    mean_inference_ms: 1.501890329214243\n",
      "    mean_raw_obs_processing_ms: 0.6023601337627217\n",
      "  time_since_restore: 6.30774712562561\n",
      "  time_this_iter_s: 6.30774712562561\n",
      "  time_total_s: 6.30774712562561\n",
      "  timers:\n",
      "    learn_throughput: 1168.553\n",
      "    learn_time_ms: 855.759\n",
      "    sample_throughput: 363.096\n",
      "    sample_time_ms: 2754.089\n",
      "  timestamp: 1630715442\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: '50129_00001'\n",
      "  \n",
      "Result for PPO_townsend_50129_00002:\n",
      "  agent_timesteps_total: 5000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: -97.40474455957333\n",
      "    discounted_rewards_mean: -97.40474455957333\n",
      "    discounted_rewards_min: -97.40474455957333\n",
      "  date: 2021-09-03_20-30-43\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - -1765.8042932391518\n",
      "  episode_reward_mean: -1765.8042932391518\n",
      "  episode_reward_min:\n",
      "  - -1765.8042932391518\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: -2.421856891113797\n",
      "      discounted_rewards_mean: -2.421856891113797\n",
      "      discounted_rewards_min: -2.421856891113797\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -6282.014152592082\n",
      "    episode_reward_mean: -6282.014152592082\n",
      "    episode_reward_min:\n",
      "    - -6282.014152592082\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -6282.014152592082\n",
      "      policy_firm_reward:\n",
      "      - - -1540.5908554074633\n",
      "      - - -1490.8855364108933\n",
      "      - - -1093.2876468742122\n",
      "      - - -1004.398921585941\n",
      "      - - -1152.8511923135684\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -1004.398921585941\n",
      "    policy_reward_mean:\n",
      "      firm: -1256.4028305184156\n",
      "    policy_reward_min:\n",
      "      firm: -1540.5908554074633\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.31143254214352545\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.28698832600504964\n",
      "      mean_inference_ms: 1.441549945187259\n",
      "      mean_raw_obs_processing_ms: 0.5824549214823262\n",
      "  experiment_id: 98f42a9076014f1896652fd4f98c88d7\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.4146313667297363\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0008032923855353601\n",
      "          policy_loss: 0.00013358145952224731\n",
      "          total_loss: 7515.96630859375\n",
      "          vf_explained_var: -7.271766522265466e-10\n",
      "          vf_loss: 7515.96630859375\n",
      "    num_agent_steps_sampled: 5000\n",
      "    num_agent_steps_trained: 5000\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.200000000000003\n",
      "    ram_util_percent: 52.21111111111111\n",
      "  pid: 32283\n",
      "  policy_reward_max:\n",
      "    firm: 101.22197026739214\n",
      "  policy_reward_mean:\n",
      "    firm: -353.16085864782985\n",
      "  policy_reward_min:\n",
      "    firm: -1221.6707876810838\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3190276386973622\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.29488829346922607\n",
      "    mean_inference_ms: 1.5149147479565113\n",
      "    mean_raw_obs_processing_ms: 0.6120921848537205\n",
      "  time_since_restore: 6.345928907394409\n",
      "  time_this_iter_s: 6.345928907394409\n",
      "  time_total_s: 6.345928907394409\n",
      "  timers:\n",
      "    learn_throughput: 1155.005\n",
      "    learn_time_ms: 865.797\n",
      "    sample_throughput: 359.821\n",
      "    sample_time_ms: 2779.157\n",
      "  timestamp: 1630715443\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: '50129_00002'\n",
      "  \n",
      "Result for PPO_townsend_50129_00003:\n",
      "  agent_timesteps_total: 5000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: -23.645582630052754\n",
      "    discounted_rewards_mean: -23.645582630052754\n",
      "    discounted_rewards_min: -23.645582630052754\n",
      "  date: 2021-09-03_20-30-43\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - -3373.1881429438236\n",
      "  episode_reward_mean: -3373.1881429438236\n",
      "  episode_reward_min:\n",
      "  - -3373.1881429438236\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: 107.00087524571947\n",
      "      discounted_rewards_mean: 107.00087524571947\n",
      "      discounted_rewards_min: 107.00087524571947\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -5579.380848434921\n",
      "    episode_reward_mean: -5579.380848434921\n",
      "    episode_reward_min:\n",
      "    - -5579.380848434921\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -5579.380848434921\n",
      "      policy_firm_reward:\n",
      "      - - -918.0143803776435\n",
      "      - - -1065.0541901035228\n",
      "      - - -1273.6810257852062\n",
      "      - - -927.4465916454983\n",
      "      - - -1395.1846605230407\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -918.0143803776435\n",
      "    policy_reward_mean:\n",
      "      firm: -1115.8761696869824\n",
      "    policy_reward_min:\n",
      "      firm: -1395.1846605230407\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.31380076984782795\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.28730462957452707\n",
      "      mean_inference_ms: 1.4416452173467402\n",
      "      mean_raw_obs_processing_ms: 0.5808355329515456\n",
      "  experiment_id: 46d9de70df144b71876ea07c4bb788d5\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.4160915851593017\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0012661257715081062\n",
      "          policy_loss: 0.00038155987858772277\n",
      "          total_loss: 5533.36279296875\n",
      "          vf_explained_var: 9.536742923144104e-11\n",
      "          vf_loss: 5533.36181640625\n",
      "    num_agent_steps_sampled: 5000\n",
      "    num_agent_steps_trained: 5000\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.366666666666667\n",
      "    ram_util_percent: 52.22222222222222\n",
      "  pid: 32282\n",
      "  policy_reward_max:\n",
      "    firm: -456.9679439843315\n",
      "  policy_reward_mean:\n",
      "    firm: -674.6376285887621\n",
      "  policy_reward_min:\n",
      "    firm: -1136.1776681561075\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3231481595949217\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.29636667920397475\n",
      "    mean_inference_ms: 1.516466731434459\n",
      "    mean_raw_obs_processing_ms: 0.6292930968872437\n",
      "  time_since_restore: 6.398087024688721\n",
      "  time_this_iter_s: 6.398087024688721\n",
      "  time_total_s: 6.398087024688721\n",
      "  timers:\n",
      "    learn_throughput: 1113.489\n",
      "    learn_time_ms: 898.078\n",
      "    sample_throughput: 357.145\n",
      "    sample_time_ms: 2799.981\n",
      "  timestamp: 1630715443\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: '50129_00003'\n",
      "  \n",
      "Result for PPO_townsend_50129_00000:\n",
      "  agent_timesteps_total: 10000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: 30.264240166187477\n",
      "    discounted_rewards_mean: 26.184094872504065\n",
      "    discounted_rewards_min: 22.103949578820654\n",
      "  date: 2021-09-03_20-30-49\n",
      "  done: true\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - -1121.2406238056165\n",
      "  episode_reward_mean: -2453.5483387892536\n",
      "  episode_reward_min:\n",
      "  - -3785.856053772891\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: 77.16594588976334\n",
      "      discounted_rewards_mean: 77.16594588976334\n",
      "      discounted_rewards_min: 77.16594588976334\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -5459.924915957143\n",
      "    episode_reward_mean: -5459.924915957143\n",
      "    episode_reward_min:\n",
      "    - -5459.924915957143\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -5459.924915957143\n",
      "      policy_firm_reward:\n",
      "      - - -1266.2851290367869\n",
      "      - - -1026.687539534343\n",
      "      - - -1206.4260258920588\n",
      "      - - -1187.6893606395531\n",
      "      - - -772.8368608544058\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -772.8368608544058\n",
      "    policy_reward_mean:\n",
      "      firm: -1091.9849831914296\n",
      "    policy_reward_min:\n",
      "      firm: -1266.2851290367869\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.3191484682920991\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.28309674336873314\n",
      "      mean_inference_ms: 1.4591908109360847\n",
      "      mean_raw_obs_processing_ms: 0.575342635879631\n",
      "  experiment_id: 03636c60d9034daf8bdcad83b5c0a3c5\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.1\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.3893540143966674\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00023337856400768259\n",
      "          policy_loss: 2.2374093532562256e-05\n",
      "          total_loss: 2651.203564453125\n",
      "          vf_explained_var: -4.053115898461357e-10\n",
      "          vf_loss: 2651.20341796875\n",
      "    num_agent_steps_sampled: 10000\n",
      "    num_agent_steps_trained: 10000\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.777777777777782\n",
      "    ram_util_percent: 53.166666666666664\n",
      "  pid: 32248\n",
      "  policy_reward_max:\n",
      "    firm: 141.8781650223737\n",
      "  policy_reward_mean:\n",
      "    firm: -490.70966775785064\n",
      "  policy_reward_min:\n",
      "    firm: -1515.506400171593\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.32430885100568096\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.28942020685550973\n",
      "    mean_inference_ms: 1.5125594316890494\n",
      "    mean_raw_obs_processing_ms: 0.5964794904286645\n",
      "  time_since_restore: 12.505380153656006\n",
      "  time_this_iter_s: 6.157926082611084\n",
      "  time_total_s: 12.505380153656006\n",
      "  timers:\n",
      "    learn_throughput: 1232.669\n",
      "    learn_time_ms: 811.247\n",
      "    sample_throughput: 365.736\n",
      "    sample_time_ms: 2734.214\n",
      "  timestamp: 1630715449\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: '50129_00000'\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.1/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/4 CPUs, 0/0 GPUs, 0.0/10.89 GiB heap, 0.0/5.44 GiB objects<br>Result logdir: /Users/matiascovarrubias/ray_results/native_5_firms_townsend_test_Sep3_PPO<br>Number of trials: 6/6 (2 PENDING, 4 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_len_mean</th><th style=\"text-align: right;\">  episodes_this_iter</th><th style=\"text-align: right;\">  num_healthy_workers</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_townsend_50129_00000</td><td>RUNNING </td><td>192.168.1.202:32248</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        12.5054 </td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">-2453.55</td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_50129_00001</td><td>RUNNING </td><td>192.168.1.202:32246</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         6.30775</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">-3028.22</td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_50129_00002</td><td>RUNNING </td><td>192.168.1.202:32283</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         6.34593</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">-1765.8 </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_50129_00003</td><td>RUNNING </td><td>192.168.1.202:32282</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         6.39809</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">-3373.19</td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_50129_00004</td><td>PENDING </td><td>                   </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                     </td></tr>\n",
       "<tr><td>PPO_townsend_50129_00005</td><td>PENDING </td><td>                   </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_townsend_50129_00001:\n",
      "  agent_timesteps_total: 10000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: 205.8700629617457\n",
      "    discounted_rewards_mean: 96.12446924707889\n",
      "    discounted_rewards_min: -13.621124467587926\n",
      "  date: 2021-09-03_20-30-49\n",
      "  done: true\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - 3308.710058940584\n",
      "  episode_reward_mean: 140.24369946284833\n",
      "  episode_reward_min:\n",
      "  - -3028.2226600148874\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: 20.864998967905734\n",
      "      discounted_rewards_mean: 20.864998967905734\n",
      "      discounted_rewards_min: 20.864998967905734\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -5838.158858043396\n",
      "    episode_reward_mean: -5838.158858043396\n",
      "    episode_reward_min:\n",
      "    - -5838.158858043396\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -5838.158858043396\n",
      "      policy_firm_reward:\n",
      "      - - -1400.0934966339817\n",
      "      - - -697.9637502368611\n",
      "      - - -1095.4519805074267\n",
      "      - - -1125.2671501727539\n",
      "      - - -1519.3824804923674\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -697.9637502368611\n",
      "    policy_reward_mean:\n",
      "      firm: -1167.6317716086783\n",
      "    policy_reward_min:\n",
      "      firm: -1519.3824804923674\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.3175998794502285\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.28515016955175976\n",
      "      mean_inference_ms: 1.462122370516402\n",
      "      mean_raw_obs_processing_ms: 0.581357909225929\n",
      "  experiment_id: 9686e01bb6f3415b9bdb57ded3ffb166\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.1\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.4327730417251587\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0002927814712166565\n",
      "          policy_loss: 5.323905497789383e-06\n",
      "          total_loss: 4867.634765625\n",
      "          vf_explained_var: 9.536742923144104e-11\n",
      "          vf_loss: 4867.6345703125\n",
      "    num_agent_steps_sampled: 10000\n",
      "    num_agent_steps_trained: 10000\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.744444444444447\n",
      "    ram_util_percent: 53.199999999999996\n",
      "  pid: 32246\n",
      "  policy_reward_max:\n",
      "    firm: 865.6986069546726\n",
      "  policy_reward_mean:\n",
      "    firm: 28.04873989256871\n",
      "  policy_reward_min:\n",
      "    firm: -1569.6829831644304\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.32554619293959225\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2888596639279367\n",
      "    mean_inference_ms: 1.5042400440762616\n",
      "    mean_raw_obs_processing_ms: 0.5993183298078505\n",
      "  time_since_restore: 12.60034728050232\n",
      "  time_this_iter_s: 6.292600154876709\n",
      "  time_total_s: 12.60034728050232\n",
      "  timers:\n",
      "    learn_throughput: 1175.522\n",
      "    learn_time_ms: 850.686\n",
      "    sample_throughput: 365.669\n",
      "    sample_time_ms: 2734.712\n",
      "  timestamp: 1630715449\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: '50129_00001'\n",
      "  \n",
      "Result for PPO_townsend_50129_00002:\n",
      "  agent_timesteps_total: 10000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: 26.287563051892388\n",
      "    discounted_rewards_mean: -35.55859075384047\n",
      "    discounted_rewards_min: -97.40474455957333\n",
      "  date: 2021-09-03_20-30-49\n",
      "  done: true\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - -291.5623825199443\n",
      "  episode_reward_mean: -1028.6833378795482\n",
      "  episode_reward_min:\n",
      "  - -1765.8042932391518\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: -9.567647367952189\n",
      "      discounted_rewards_mean: -9.567647367952189\n",
      "      discounted_rewards_min: -9.567647367952189\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -5402.91544323771\n",
      "    episode_reward_mean: -5402.91544323771\n",
      "    episode_reward_min:\n",
      "    - -5402.91544323771\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -5402.91544323771\n",
      "      policy_firm_reward:\n",
      "      - - -1171.9213295134416\n",
      "      - - -419.00979241240356\n",
      "      - - -1980.0267119289522\n",
      "      - - -225.69097236872994\n",
      "      - - -1606.2666370141874\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -225.69097236872994\n",
      "    policy_reward_mean:\n",
      "      firm: -1080.5830886475428\n",
      "    policy_reward_min:\n",
      "      firm: -1980.0267119289522\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.31411320134915455\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.2881040101287247\n",
      "      mean_inference_ms: 1.455165337825167\n",
      "      mean_raw_obs_processing_ms: 0.5832402364186559\n",
      "  experiment_id: 98f42a9076014f1896652fd4f98c88d7\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.1\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.4171711444854735\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00034403693862263083\n",
      "          policy_loss: 4.787687212228775e-05\n",
      "          total_loss: 2639.833935546875\n",
      "          vf_explained_var: -4.410743714711174e-10\n",
      "          vf_loss: 2639.833984375\n",
      "    num_agent_steps_sampled: 10000\n",
      "    num_agent_steps_trained: 10000\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.61111111111111\n",
      "    ram_util_percent: 53.23333333333333\n",
      "  pid: 32283\n",
      "  policy_reward_max:\n",
      "    firm: 506.5691914333706\n",
      "  policy_reward_mean:\n",
      "    firm: -205.73666757590954\n",
      "  policy_reward_min:\n",
      "    firm: -1221.6707876810838\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.31791907564387334\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.29305993222946936\n",
      "    mean_inference_ms: 1.5126456019887315\n",
      "    mean_raw_obs_processing_ms: 0.6036409135046332\n",
      "  time_since_restore: 12.619656085968018\n",
      "  time_this_iter_s: 6.273727178573608\n",
      "  time_total_s: 12.619656085968018\n",
      "  timers:\n",
      "    learn_throughput: 1169.986\n",
      "    learn_time_ms: 854.711\n",
      "    sample_throughput: 365.063\n",
      "    sample_time_ms: 2739.256\n",
      "  timestamp: 1630715449\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: '50129_00002'\n",
      "  \n",
      "Result for PPO_townsend_50129_00003:\n",
      "  agent_timesteps_total: 10000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: 76.65875764367229\n",
      "    discounted_rewards_mean: 26.506587506809765\n",
      "    discounted_rewards_min: -23.645582630052754\n",
      "  date: 2021-09-03_20-30-50\n",
      "  done: true\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - -3373.1881429438236\n",
      "  episode_reward_mean: -4481.315572156604\n",
      "  episode_reward_min:\n",
      "  - -5589.443001369383\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: -91.01295702163041\n",
      "      discounted_rewards_mean: -91.01295702163041\n",
      "      discounted_rewards_min: -91.01295702163041\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -5173.667915731848\n",
      "    episode_reward_mean: -5173.667915731848\n",
      "    episode_reward_min:\n",
      "    - -5173.667915731848\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -5173.667915731848\n",
      "      policy_firm_reward:\n",
      "      - - -1438.904063006066\n",
      "      - - -495.6837886654666\n",
      "      - - -1288.2415174022028\n",
      "      - - -1042.3207312303707\n",
      "      - - -908.5178154277452\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -495.6837886654666\n",
      "    policy_reward_mean:\n",
      "      firm: -1034.7335831463704\n",
      "    policy_reward_min:\n",
      "      firm: -1438.904063006066\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.315690981871125\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.2877532333686672\n",
      "      mean_inference_ms: 1.4519504402233088\n",
      "      mean_raw_obs_processing_ms: 0.5888077451371361\n",
      "  experiment_id: 46d9de70df144b71876ea07c4bb788d5\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.1\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.4256365299224854\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0003574851114536216\n",
      "          policy_loss: 1.2279674410820008e-05\n",
      "          total_loss: 4691.0123046875\n",
      "          vf_explained_var: -8.296966669263384e-09\n",
      "          vf_loss: 4691.01220703125\n",
      "    num_agent_steps_sampled: 10000\n",
      "    num_agent_steps_trained: 10000\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.08888888888888\n",
      "    ram_util_percent: 53.21111111111111\n",
      "  pid: 32282\n",
      "  policy_reward_max:\n",
      "    firm: -456.9679439843315\n",
      "  policy_reward_mean:\n",
      "    firm: -896.2631144313189\n",
      "  policy_reward_min:\n",
      "    firm: -1914.7084280453178\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.32180917783605756\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.29383695513125885\n",
      "    mean_inference_ms: 1.512517723992041\n",
      "    mean_raw_obs_processing_ms: 0.6200060585536791\n",
      "  time_since_restore: 12.746037006378174\n",
      "  time_this_iter_s: 6.347949981689453\n",
      "  time_total_s: 12.746037006378174\n",
      "  timers:\n",
      "    learn_throughput: 1117.674\n",
      "    learn_time_ms: 894.716\n",
      "    sample_throughput: 363.007\n",
      "    sample_time_ms: 2754.767\n",
      "  timestamp: 1630715450\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: '50129_00003'\n",
      "  \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(pid=32359)\u001b[0m WARNING:tensorflow:From /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=32359)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=32359)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=32361)\u001b[0m WARNING:tensorflow:From /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=32361)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=32361)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=32359)\u001b[0m 2021-09-03 20:30:54,062\tINFO trainer.py:694 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=32361)\u001b[0m 2021-09-03 20:30:54,134\tINFO trainer.py:694 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=32359)\u001b[0m 2021-09-03 20:30:54,245\tWARNING deprecation.py:33 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=32361)\u001b[0m 2021-09-03 20:30:54,324\tWARNING deprecation.py:33 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=32359)\u001b[0m 2021-09-03 20:30:57,096\tWARNING deprecation.py:33 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=32361)\u001b[0m 2021-09-03 20:30:57,194\tWARNING deprecation.py:33 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_townsend_50129_00005:\n",
      "  agent_timesteps_total: 5000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: -10.52968452289197\n",
      "    discounted_rewards_mean: -10.52968452289197\n",
      "    discounted_rewards_min: -10.52968452289197\n",
      "  date: 2021-09-03_20-31-00\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - -6803.441608374246\n",
      "  episode_reward_mean: -6803.441608374246\n",
      "  episode_reward_min:\n",
      "  - -6803.441608374246\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: -37.47637337638055\n",
      "      discounted_rewards_mean: -37.47637337638055\n",
      "      discounted_rewards_min: -37.47637337638055\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -6252.2992158071775\n",
      "    episode_reward_mean: -6252.2992158071775\n",
      "    episode_reward_min:\n",
      "    - -6252.2992158071775\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -6252.2992158071775\n",
      "      policy_firm_reward:\n",
      "      - - -1533.6875541307863\n",
      "      - - -797.0310329718416\n",
      "      - - -1704.9655507673417\n",
      "      - - -1087.0065059679644\n",
      "      - - -1129.6085719692446\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -797.0310329718416\n",
      "    policy_reward_mean:\n",
      "      firm: -1250.4598431614356\n",
      "    policy_reward_min:\n",
      "      firm: -1704.9655507673417\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.29634571932888887\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.27028759280880255\n",
      "      mean_inference_ms: 1.3696332792421202\n",
      "      mean_raw_obs_processing_ms: 0.5418125327888664\n",
      "  experiment_id: b7466e8853cc412ba0a5d4c35a7dde26\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.4145599603652954\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0012857537250966287\n",
      "          policy_loss: 0.0004662090912461281\n",
      "          total_loss: 5896.2728515625\n",
      "          vf_explained_var: -2.3841859042583735e-10\n",
      "          vf_loss: 5896.27216796875\n",
      "    num_agent_steps_sampled: 5000\n",
      "    num_agent_steps_trained: 5000\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.333333333333332\n",
      "    ram_util_percent: 50.52222222222222\n",
      "  pid: 32359\n",
      "  policy_reward_max:\n",
      "    firm: -795.8217139127935\n",
      "  policy_reward_mean:\n",
      "    firm: -1360.6883216748474\n",
      "  policy_reward_min:\n",
      "    firm: -2605.906530605243\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.32080970444045703\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.28938084811001985\n",
      "    mean_inference_ms: 1.4777750402063756\n",
      "    mean_raw_obs_processing_ms: 0.5874397990467785\n",
      "  time_since_restore: 6.012314319610596\n",
      "  time_this_iter_s: 6.012314319610596\n",
      "  time_total_s: 6.012314319610596\n",
      "  timers:\n",
      "    learn_throughput: 1309.993\n",
      "    learn_time_ms: 763.363\n",
      "    sample_throughput: 369.697\n",
      "    sample_time_ms: 2704.919\n",
      "  timestamp: 1630715460\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: '50129_00005'\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.4/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/0 GPUs, 0.0/10.89 GiB heap, 0.0/5.44 GiB objects<br>Result logdir: /Users/matiascovarrubias/ray_results/native_5_firms_townsend_test_Sep3_PPO<br>Number of trials: 6/6 (2 RUNNING, 4 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_len_mean</th><th style=\"text-align: right;\">  episodes_this_iter</th><th style=\"text-align: right;\">  num_healthy_workers</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_townsend_50129_00004</td><td>RUNNING   </td><td>                   </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">                  </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                     </td></tr>\n",
       "<tr><td>PPO_townsend_50129_00005</td><td>RUNNING   </td><td>192.168.1.202:32359</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         6.01231</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">-6803.44 </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_50129_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        12.5054 </td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">-2453.55 </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_50129_00001</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        12.6003 </td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">  140.244</td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_50129_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        12.6197 </td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">-1028.68 </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_50129_00003</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        12.746  </td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">-4481.32 </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_townsend_50129_00004:\n",
      "  agent_timesteps_total: 5000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: -18.23153161432789\n",
      "    discounted_rewards_mean: -18.23153161432789\n",
      "    discounted_rewards_min: -18.23153161432789\n",
      "  date: 2021-09-03_20-31-00\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - -8217.949441348368\n",
      "  episode_reward_mean: -8217.949441348368\n",
      "  episode_reward_min:\n",
      "  - -8217.949441348368\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: 22.452404566554875\n",
      "      discounted_rewards_mean: 22.452404566554875\n",
      "      discounted_rewards_min: 22.452404566554875\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -6992.886811313094\n",
      "    episode_reward_mean: -6992.886811313094\n",
      "    episode_reward_min:\n",
      "    - -6992.886811313094\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -6992.886811313094\n",
      "      policy_firm_reward:\n",
      "      - - -1665.1752987001416\n",
      "      - - -1136.3890369987002\n",
      "      - - -1479.8881493888764\n",
      "      - - -1021.0077873319213\n",
      "      - - -1690.4265388934436\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -1021.0077873319213\n",
      "    policy_reward_mean:\n",
      "      firm: -1398.5773622626168\n",
      "    policy_reward_min:\n",
      "      firm: -1690.4265388934436\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.3018374447817807\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.2699350858187223\n",
      "      mean_inference_ms: 1.3734758436143935\n",
      "      mean_raw_obs_processing_ms: 0.5563470152589111\n",
      "  experiment_id: f3b82161c5104d4392703895ecd0f798\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.4079053163528443\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0008735083625652066\n",
      "          policy_loss: 0.0001909742597490549\n",
      "          total_loss: 5689.31552734375\n",
      "          vf_explained_var: -6.008148201175345e-09\n",
      "          vf_loss: 5689.31513671875\n",
      "    num_agent_steps_sampled: 5000\n",
      "    num_agent_steps_trained: 5000\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.200000000000003\n",
      "    ram_util_percent: 50.52222222222222\n",
      "  pid: 32361\n",
      "  policy_reward_max:\n",
      "    firm: -1057.3377502382393\n",
      "  policy_reward_mean:\n",
      "    firm: -1643.5898882696808\n",
      "  policy_reward_min:\n",
      "    firm: -2485.798685532177\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3224860180865277\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2953063000689496\n",
      "    mean_inference_ms: 1.4915571107969179\n",
      "    mean_raw_obs_processing_ms: 0.5827538378826984\n",
      "  time_since_restore: 6.082819223403931\n",
      "  time_this_iter_s: 6.082819223403931\n",
      "  time_total_s: 6.082819223403931\n",
      "  timers:\n",
      "    learn_throughput: 1257.851\n",
      "    learn_time_ms: 795.007\n",
      "    sample_throughput: 367.508\n",
      "    sample_time_ms: 2721.031\n",
      "  timestamp: 1630715460\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: '50129_00004'\n",
      "  \n",
      "Result for PPO_townsend_50129_00005:\n",
      "  agent_timesteps_total: 10000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: -10.52968452289197\n",
      "    discounted_rewards_mean: -115.27132980575286\n",
      "    discounted_rewards_min: -220.01297508861373\n",
      "  date: 2021-09-03_20-31-06\n",
      "  done: true\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - -4900.59607375374\n",
      "  episode_reward_mean: -5852.018841063993\n",
      "  episode_reward_min:\n",
      "  - -6803.441608374246\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: -30.759305247000828\n",
      "      discounted_rewards_mean: -30.759305247000828\n",
      "      discounted_rewards_min: -30.759305247000828\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -6739.208170899658\n",
      "    episode_reward_mean: -6739.208170899658\n",
      "    episode_reward_min:\n",
      "    - -6739.208170899658\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -6739.208170899658\n",
      "      policy_firm_reward:\n",
      "      - - -1707.6134337152564\n",
      "      - - -812.8756856131888\n",
      "      - - -1804.9621616677857\n",
      "      - - -1435.9164401518567\n",
      "      - - -977.8404497515679\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -812.8756856131888\n",
      "    policy_reward_mean:\n",
      "      firm: -1347.841634179931\n",
      "    policy_reward_min:\n",
      "      firm: -1804.9621616677857\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.2913975465422806\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.2660379595663594\n",
      "      mean_inference_ms: 1.3354422747046277\n",
      "      mean_raw_obs_processing_ms: 0.5347262853863596\n",
      "  experiment_id: b7466e8853cc412ba0a5d4c35a7dde26\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.1\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.3820980310440063\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00026745295908775686\n",
      "          policy_loss: 1.0442640632390977e-05\n",
      "          total_loss: 3248.9923828125\n",
      "          vf_explained_var: -1.0490417423625331e-09\n",
      "          vf_loss: 3248.992431640625\n",
      "    num_agent_steps_sampled: 10000\n",
      "    num_agent_steps_trained: 10000\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.6\n",
      "    ram_util_percent: 51.3875\n",
      "  pid: 32359\n",
      "  policy_reward_max:\n",
      "    firm: -415.9514043869541\n",
      "  policy_reward_mean:\n",
      "    firm: -1170.4037682127978\n",
      "  policy_reward_min:\n",
      "    firm: -2605.906530605243\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3147060561676628\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2845294739323845\n",
      "    mean_inference_ms: 1.45831182932614\n",
      "    mean_raw_obs_processing_ms: 0.5740957400073831\n",
      "  time_since_restore: 11.71910834312439\n",
      "  time_this_iter_s: 5.706794023513794\n",
      "  time_total_s: 11.71910834312439\n",
      "  timers:\n",
      "    learn_throughput: 1316.618\n",
      "    learn_time_ms: 759.521\n",
      "    sample_throughput: 383.532\n",
      "    sample_time_ms: 2607.345\n",
      "  timestamp: 1630715466\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: '50129_00005'\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.5/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/0 GPUs, 0.0/10.89 GiB heap, 0.0/5.44 GiB objects<br>Result logdir: /Users/matiascovarrubias/ray_results/native_5_firms_townsend_test_Sep3_PPO<br>Number of trials: 6/6 (2 RUNNING, 4 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_len_mean</th><th style=\"text-align: right;\">  episodes_this_iter</th><th style=\"text-align: right;\">  num_healthy_workers</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_townsend_50129_00004</td><td>RUNNING   </td><td>192.168.1.202:32361</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         6.08282</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">-8217.95 </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_50129_00005</td><td>RUNNING   </td><td>192.168.1.202:32359</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        11.7191 </td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">-5852.02 </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_50129_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        12.5054 </td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">-2453.55 </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_50129_00001</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        12.6003 </td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">  140.244</td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_50129_00002</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        12.6197 </td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">-1028.68 </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_50129_00003</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        12.746  </td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">-4481.32 </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for PPO_townsend_50129_00004:\n",
      "  agent_timesteps_total: 10000\n",
      "  custom_metrics:\n",
      "    discounted_rewards_max: -18.23153161432789\n",
      "    discounted_rewards_mean: -145.99600034460704\n",
      "    discounted_rewards_min: -273.7604690748862\n",
      "  date: 2021-09-03_20-31-06\n",
      "  done: true\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max:\n",
      "  - -8217.949441348368\n",
      "  episode_reward_mean: -10919.36622256487\n",
      "  episode_reward_min:\n",
      "  - -13620.783003781371\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2\n",
      "  evaluation:\n",
      "    custom_metrics:\n",
      "      discounted_rewards_max: -1.4453119984394092\n",
      "      discounted_rewards_mean: -1.4453119984394092\n",
      "      discounted_rewards_min: -1.4453119984394092\n",
      "    episode_len_mean: 1000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max:\n",
      "    - -6023.46370922509\n",
      "    episode_reward_mean: -6023.46370922509\n",
      "    episode_reward_min:\n",
      "    - -6023.46370922509\n",
      "    episodes_this_iter: 1\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 1000\n",
      "      episode_reward:\n",
      "      - - -6023.46370922509\n",
      "      policy_firm_reward:\n",
      "      - - -962.1575749592465\n",
      "      - - -1541.1760937770327\n",
      "      - - -1256.5169435145976\n",
      "      - - -925.6605020410797\n",
      "      - - -1337.9525949331496\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max:\n",
      "      firm: -925.6605020410797\n",
      "    policy_reward_mean:\n",
      "      firm: -1204.6927418450214\n",
      "    policy_reward_min:\n",
      "      firm: -1541.1760937770327\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.2949387952126842\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.26485539864802704\n",
      "      mean_inference_ms: 1.3431299334463627\n",
      "      mean_raw_obs_processing_ms: 0.5337409172458448\n",
      "  experiment_id: f3b82161c5104d4392703895ecd0f798\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      firm:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.1\n",
      "          cur_lr: 0.0005\n",
      "          entropy: 1.4342287063598633\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00039004475693222104\n",
      "          policy_loss: 6.359973922371864e-05\n",
      "          total_loss: 10431.8603515625\n",
      "          vf_explained_var: -1.0275840622853138e-08\n",
      "          vf_loss: 10431.859765625\n",
      "    num_agent_steps_sampled: 10000\n",
      "    num_agent_steps_trained: 10000\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.549999999999999\n",
      "    ram_util_percent: 51.3875\n",
      "  pid: 32361\n",
      "  policy_reward_max:\n",
      "    firm: -1057.3377502382393\n",
      "  policy_reward_mean:\n",
      "    firm: -2183.8732445129763\n",
      "  policy_reward_min:\n",
      "    firm: -3493.1952697100683\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.3170265545911991\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.29018039619990527\n",
      "    mean_inference_ms: 1.4714471437193497\n",
      "    mean_raw_obs_processing_ms: 0.5795829212258514\n",
      "  time_since_restore: 11.805477380752563\n",
      "  time_this_iter_s: 5.722658157348633\n",
      "  time_total_s: 11.805477380752563\n",
      "  timers:\n",
      "    learn_throughput: 1319.192\n",
      "    learn_time_ms: 758.04\n",
      "    sample_throughput: 378.31\n",
      "    sample_time_ms: 2643.338\n",
      "  timestamp: 1630715466\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: '50129_00004'\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.4/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/4 CPUs, 0/0 GPUs, 0.0/10.89 GiB heap, 0.0/5.44 GiB objects<br>Result logdir: /Users/matiascovarrubias/ray_results/native_5_firms_townsend_test_Sep3_PPO<br>Number of trials: 6/6 (6 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_len_mean</th><th style=\"text-align: right;\">  episodes_this_iter</th><th style=\"text-align: right;\">  num_healthy_workers</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_townsend_50129_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         12.5054</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\"> -2453.55 </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_50129_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         12.6003</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">   140.244</td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_50129_00002</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         12.6197</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\"> -1028.68 </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_50129_00003</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         12.746 </td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\"> -4481.32 </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_50129_00004</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         11.8055</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">-10919.4  </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "<tr><td>PPO_townsend_50129_00005</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         11.7191</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\"> -5852.02 </td><td style=\"text-align: right;\">              1000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                    0</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-03 20:31:06,841\tINFO tune.py:549 -- Total run time: 34.80 seconds (34.56 seconds for the tuning loop).\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "\"\"\" STEP 5 (optional): Organize and Plot \"\"\"\n",
    "\n",
    "# global experiment name\n",
    "if len(exp_names) > 1:\n",
    "    EXP_LABEL = DEVICE + f\"_multi_firm_\"\n",
    "    if TEST == True:\n",
    "        EXP_NAME = EXP_LABEL + ENV_LABEL + \"_test_\" + DATE + ALGO\n",
    "    else:\n",
    "        EXP_NAME = EXP_LABEL + ENV_LABEL + \"_run_\" + DATE + ALGO\n",
    "\n",
    "\n",
    "# create CSV with information on each experiment\n",
    "if SAVE_EXP_INFO:\n",
    "    progress_csv_dirs = [exp_dirs[i] + \"/progress.csv\" for i in range(len(exp_dirs))]\n",
    "\n",
    "    # Create CSV with economy level\n",
    "    exp_dict = {\n",
    "        \"n_agents\": n_firms_LIST,\n",
    "        \"exp_names\": exp_names,\n",
    "        \"exp_dirs\": exp_dirs,\n",
    "        \"progress_csv_dirs\": progress_csv_dirs,\n",
    "        \"best_rewards\": best_rewards,\n",
    "        \"checkpoints\": checkpoints,\n",
    "        # \"best_config\": best_configs,\n",
    "    }\n",
    "    # for i in range(len(exp_dict.values())):\n",
    "    #     print(type(exp_dict.values()[i]))\n",
    "    print(\n",
    "        \"exp_names =\",\n",
    "        exp_names,\n",
    "        \"\\n\" \"exp_dirs =\",\n",
    "        exp_dirs,\n",
    "        \"\\n\" \"progress_csv_dirs =\",\n",
    "        progress_csv_dirs,\n",
    "        \"\\n\" \"best_rewards =\",\n",
    "        best_rewards,\n",
    "        \"\\n\" \"checkpoints =\",\n",
    "        checkpoints,\n",
    "        # \"\\n\" \"best_config =\",\n",
    "        # best_configs,\n",
    "    )\n",
    "\n",
    "    with open(OUTPUT_PATH_EXPERS + \"expINFO_\" + EXP_NAME + \".json\", \"w+\") as f:\n",
    "        json.dump(exp_dict, f)\n",
    "\n",
    "    # exp_df = pd.DataFrame(exp_dict)\n",
    "    # exp_df.to_csv(OUTPUT_PATH_EXPERS + \"exp_info\" + EXP_NAME + \".csv\")\n",
    "    print(OUTPUT_PATH_EXPERS + \"expINFO_\" + EXP_NAME + \".json\")\n",
    "\n",
    "# Plot and save progress\n",
    "if PLOT_PROGRESS:\n",
    "    for i in range(len(exp_names)):\n",
    "        learning_plot = sn.lineplot(\n",
    "            data=learning_dta[i],\n",
    "            y=f\"{i+1} households\",\n",
    "            x=\"episodes_total\",\n",
    "        )\n",
    "    learning_plot = learning_plot.get_figure()\n",
    "    plt.ylabel(\"Discounted utility\")\n",
    "    plt.xlabel(\"Timesteps (thousands)\")\n",
    "    plt.legend(labels=[f\"{i+1} households\" for i in range(len(learning_dta))])\n",
    "    learning_plot.savefig(OUTPUT_PATH_FIGURES + \"progress_\" + EXP_NAME + \".png\")\n",
    "\n",
    "# Save progress as CSV\n",
    "if SAVE_PROGRESS_CSV:\n",
    "    # merge data\n",
    "    learning_dta = [df.set_index(\"episodes_total\") for df in learning_dta]\n",
    "    learning_dta_merged = pd.concat(learning_dta, axis=1)\n",
    "    learning_dta_merged.to_csv(OUTPUT_PATH_EXPERS + \"progress_\" + EXP_NAME + \".csv\")\n",
    "\n",
    "\"\"\" Annex_env_hyp: For Environment hyperparameter tuning\"\"\"\n",
    "\n",
    "# # We create a list that contain the main config + altered copies.\n",
    "env_configs = [env_config]\n",
    "for i in range(1, 15):\n",
    "    env_configs.append(env_config.copy())\n",
    "    env_configs[i][\"parameteres\"] = (\n",
    "        {\n",
    "            \"depreciation\": np.random.choice([0.02, 0.04, 0.06, 0.08]),\n",
    "            \"alpha\": 0.3,\n",
    "            \"phi\": 0.3,\n",
    "            \"beta\": 0.98,\n",
    "        },\n",
    "    )\n",
    "    env_configs[i][\"bgt_penalty\"] = np.random.choice([1, 5, 10, 50])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "exp_names = ['native_1_firms_townsend_test_Sep3_PPO', 'native_2_firms_townsend_test_Sep3_PPO', 'native_3_firms_townsend_test_Sep3_PPO', 'native_4_firms_townsend_test_Sep3_PPO', 'native_5_firms_townsend_test_Sep3_PPO'] \n",
      "exp_dirs = ['/Users/matiascovarrubias/ray_results/native_1_firms_townsend_test_Sep3_PPO/PPO_townsend_0e084_00002_2_2021-09-03_20-28-41', '/Users/matiascovarrubias/ray_results/native_2_firms_townsend_test_Sep3_PPO/PPO_townsend_1c945_00000_0_2021-09-03_20-29-05', '/Users/matiascovarrubias/ray_results/native_3_firms_townsend_test_Sep3_PPO/PPO_townsend_2ac4e_00000_0_2021-09-03_20-29-29', '/Users/matiascovarrubias/ray_results/native_4_firms_townsend_test_Sep3_PPO/PPO_townsend_3b7e5_00000_0_2021-09-03_20-29-57', '/Users/matiascovarrubias/ray_results/native_5_firms_townsend_test_Sep3_PPO/PPO_townsend_50129_00000_0_2021-09-03_20-30-32'] \n",
      "progress_csv_dirs = ['/Users/matiascovarrubias/ray_results/native_1_firms_townsend_test_Sep3_PPO/PPO_townsend_0e084_00002_2_2021-09-03_20-28-41/progress.csv', '/Users/matiascovarrubias/ray_results/native_2_firms_townsend_test_Sep3_PPO/PPO_townsend_1c945_00000_0_2021-09-03_20-29-05/progress.csv', '/Users/matiascovarrubias/ray_results/native_3_firms_townsend_test_Sep3_PPO/PPO_townsend_2ac4e_00000_0_2021-09-03_20-29-29/progress.csv', '/Users/matiascovarrubias/ray_results/native_4_firms_townsend_test_Sep3_PPO/PPO_townsend_3b7e5_00000_0_2021-09-03_20-29-57/progress.csv', '/Users/matiascovarrubias/ray_results/native_5_firms_townsend_test_Sep3_PPO/PPO_townsend_50129_00000_0_2021-09-03_20-30-32/progress.csv'] \n",
      "best_rewards = [55.29194996937671, 32.79935156341983, 50.42643748446326, 69.1541121888637, 77.16594588976334] \n",
      "checkpoints = ['/Users/matiascovarrubias/ray_results/native_1_firms_townsend_test_Sep3_PPO/PPO_townsend_0e084_00002_2_2021-09-03_20-28-41/checkpoint_000002/checkpoint-2', '/Users/matiascovarrubias/ray_results/native_2_firms_townsend_test_Sep3_PPO/PPO_townsend_1c945_00000_0_2021-09-03_20-29-05/checkpoint_000002/checkpoint-2', '/Users/matiascovarrubias/ray_results/native_3_firms_townsend_test_Sep3_PPO/PPO_townsend_2ac4e_00000_0_2021-09-03_20-29-29/checkpoint_000002/checkpoint-2', '/Users/matiascovarrubias/ray_results/native_4_firms_townsend_test_Sep3_PPO/PPO_townsend_3b7e5_00000_0_2021-09-03_20-29-57/checkpoint_000002/checkpoint-2', '/Users/matiascovarrubias/ray_results/native_5_firms_townsend_test_Sep3_PPO/PPO_townsend_50129_00000_0_2021-09-03_20-30-32/checkpoint_000002/checkpoint-2']\n",
      "/Users/matiascovarrubias/Dropbox/RL_macro/Tests/expINFO_native_multi_firm_townsend_test_Sep3_PPO.json\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"262.19625pt\" version=\"1.1\" viewBox=\"0 0 382.603125 262.19625\" width=\"382.603125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-09-03T20:31:07.655370</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 262.19625 \nL 382.603125 262.19625 \nL 382.603125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 40.603125 224.64 \nL 375.403125 224.64 \nL 375.403125 7.2 \nL 40.603125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m114ac83f9d\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"55.821307\" xlink:href=\"#m114ac83f9d\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 1.0 -->\n      <g transform=\"translate(47.869744 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" id=\"DejaVuSans-2e\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"116.694034\" xlink:href=\"#m114ac83f9d\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 1.2 -->\n      <g transform=\"translate(108.742472 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"177.566761\" xlink:href=\"#m114ac83f9d\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 1.4 -->\n      <g transform=\"translate(169.615199 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-34\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"238.439489\" xlink:href=\"#m114ac83f9d\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 1.6 -->\n      <g transform=\"translate(230.487926 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-36\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"299.312216\" xlink:href=\"#m114ac83f9d\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 1.8 -->\n      <g transform=\"translate(291.360653 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-38\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"360.184943\" xlink:href=\"#m114ac83f9d\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 2.0 -->\n      <g transform=\"translate(352.233381 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- Timesteps (thousands) -->\n     <g transform=\"translate(150.885156 252.916562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M -19 4666 \nL 3928 4666 \nL 3928 4134 \nL 2272 4134 \nL 2272 0 \nL 1638 0 \nL 1638 4134 \nL -19 4134 \nL -19 4666 \nz\n\" id=\"DejaVuSans-54\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" id=\"DejaVuSans-69\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" id=\"DejaVuSans-6d\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" id=\"DejaVuSans-73\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" id=\"DejaVuSans-74\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" id=\"DejaVuSans-70\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1984 4856 \nQ 1566 4138 1362 3434 \nQ 1159 2731 1159 2009 \nQ 1159 1288 1364 580 \nQ 1569 -128 1984 -844 \nL 1484 -844 \nQ 1016 -109 783 600 \nQ 550 1309 550 2009 \nQ 550 2706 781 3412 \nQ 1013 4119 1484 4856 \nL 1984 4856 \nz\n\" id=\"DejaVuSans-28\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-68\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 544 1381 \nL 544 3500 \nL 1119 3500 \nL 1119 1403 \nQ 1119 906 1312 657 \nQ 1506 409 1894 409 \nQ 2359 409 2629 706 \nQ 2900 1003 2900 1516 \nL 2900 3500 \nL 3475 3500 \nL 3475 0 \nL 2900 0 \nL 2900 538 \nQ 2691 219 2414 64 \nQ 2138 -91 1772 -91 \nQ 1169 -91 856 284 \nQ 544 659 544 1381 \nz\nM 1991 3584 \nL 1991 3584 \nz\n\" id=\"DejaVuSans-75\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-6e\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" id=\"DejaVuSans-64\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 513 4856 \nL 1013 4856 \nQ 1481 4119 1714 3412 \nQ 1947 2706 1947 2009 \nQ 1947 1309 1714 600 \nQ 1481 -109 1013 -844 \nL 513 -844 \nQ 928 -128 1133 580 \nQ 1338 1288 1338 2009 \nQ 1338 2731 1133 3434 \nQ 928 4138 513 4856 \nz\n\" id=\"DejaVuSans-29\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-54\"/>\n      <use x=\"57.958984\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"85.742188\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"183.154297\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"244.677734\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"296.777344\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"335.986328\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"397.509766\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"460.986328\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"513.085938\" xlink:href=\"#DejaVuSans-20\"/>\n      <use x=\"544.873047\" xlink:href=\"#DejaVuSans-28\"/>\n      <use x=\"583.886719\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"623.095703\" xlink:href=\"#DejaVuSans-68\"/>\n      <use x=\"686.474609\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"747.65625\" xlink:href=\"#DejaVuSans-75\"/>\n      <use x=\"811.035156\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"863.134766\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"924.414062\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"987.792969\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"1051.269531\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"1103.369141\" xlink:href=\"#DejaVuSans-29\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m3b07b9f675\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m3b07b9f675\" y=\"207.660803\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 10 -->\n      <g transform=\"translate(20.878125 211.460022)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m3b07b9f675\" y=\"180.620791\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 20 -->\n      <g transform=\"translate(20.878125 184.42001)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m3b07b9f675\" y=\"153.58078\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 30 -->\n      <g transform=\"translate(20.878125 157.379999)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m3b07b9f675\" y=\"126.540769\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 40 -->\n      <g transform=\"translate(20.878125 130.339987)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m3b07b9f675\" y=\"99.500757\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 50 -->\n      <g transform=\"translate(20.878125 103.299976)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m3b07b9f675\" y=\"72.460746\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 60 -->\n      <g transform=\"translate(20.878125 76.259965)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-36\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m3b07b9f675\" y=\"45.420734\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 70 -->\n      <g transform=\"translate(20.878125 49.219953)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 525 4666 \nL 3525 4666 \nL 3525 4397 \nL 1831 0 \nL 1172 0 \nL 2766 4134 \nL 525 4134 \nL 525 4666 \nz\n\" id=\"DejaVuSans-37\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-37\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m3b07b9f675\" y=\"18.380723\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 80 -->\n      <g transform=\"translate(20.878125 22.179942)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-38\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_16\">\n     <!-- Discounted utility -->\n     <g transform=\"translate(14.798438 159.925469)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 1259 4147 \nL 1259 519 \nL 2022 519 \nQ 2988 519 3436 956 \nQ 3884 1394 3884 2338 \nQ 3884 3275 3436 3711 \nQ 2988 4147 2022 4147 \nL 1259 4147 \nz\nM 628 4666 \nL 1925 4666 \nQ 3281 4666 3915 4102 \nQ 4550 3538 4550 2338 \nQ 4550 1131 3912 565 \nQ 3275 0 1925 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-44\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" id=\"DejaVuSans-63\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2059 -325 \nQ 1816 -950 1584 -1140 \nQ 1353 -1331 966 -1331 \nL 506 -1331 \nL 506 -850 \nL 844 -850 \nQ 1081 -850 1212 -737 \nQ 1344 -625 1503 -206 \nL 1606 56 \nL 191 3500 \nL 800 3500 \nL 1894 763 \nL 2988 3500 \nL 3597 3500 \nL 2059 -325 \nz\n\" id=\"DejaVuSans-79\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-44\"/>\n      <use x=\"77.001953\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"104.785156\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"156.884766\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"211.865234\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"273.046875\" xlink:href=\"#DejaVuSans-75\"/>\n      <use x=\"336.425781\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"399.804688\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"439.013672\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"500.537109\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"564.013672\" xlink:href=\"#DejaVuSans-20\"/>\n      <use x=\"595.800781\" xlink:href=\"#DejaVuSans-75\"/>\n      <use x=\"659.179688\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"698.388672\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"726.171875\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"753.955078\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"781.738281\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"820.947266\" xlink:href=\"#DejaVuSans-79\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#pf4571c3fa4)\" d=\"M 55.821307 17.083636 \nL 360.184943 85.191319 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_16\">\n    <path clip-path=\"url(#pf4571c3fa4)\" d=\"M 55.821307 205.359209 \nL 360.184943 146.01133 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_17\">\n    <path clip-path=\"url(#pf4571c3fa4)\" d=\"M 55.821307 48.989243 \nL 360.184943 98.34767 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_18\">\n    <path clip-path=\"url(#pf4571c3fa4)\" d=\"M 55.821307 144.753184 \nL 360.184943 47.708016 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_19\">\n    <path clip-path=\"url(#pf4571c3fa4)\" d=\"M 55.821307 214.756364 \nL 360.184943 26.044009 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 40.603125 224.64 \nL 40.603125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 375.403125 224.64 \nL 375.403125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 40.603125 224.64 \nL 375.403125 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 40.603125 7.2 \nL 375.403125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 269.914063 219.64 \nL 368.403125 219.64 \nQ 370.403125 219.64 370.403125 217.64 \nL 370.403125 145.249375 \nQ 370.403125 143.249375 368.403125 143.249375 \nL 269.914063 143.249375 \nQ 267.914063 143.249375 267.914063 145.249375 \nL 267.914063 217.64 \nQ 267.914063 219.64 269.914063 219.64 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_20\">\n     <path d=\"M 271.914063 151.347812 \nL 291.914063 151.347812 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_21\"/>\n    <g id=\"text_17\">\n     <!-- 1 households -->\n     <g transform=\"translate(299.914063 154.847812)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"63.623047\" xlink:href=\"#DejaVuSans-20\"/>\n      <use x=\"95.410156\" xlink:href=\"#DejaVuSans-68\"/>\n      <use x=\"158.789062\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"219.970703\" xlink:href=\"#DejaVuSans-75\"/>\n      <use x=\"283.349609\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"335.449219\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"396.972656\" xlink:href=\"#DejaVuSans-68\"/>\n      <use x=\"460.351562\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"521.533203\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"549.316406\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"612.792969\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n    <g id=\"line2d_22\">\n     <path d=\"M 271.914063 166.025937 \nL 291.914063 166.025937 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_23\"/>\n    <g id=\"text_18\">\n     <!-- 2 households -->\n     <g transform=\"translate(299.914063 169.525937)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"63.623047\" xlink:href=\"#DejaVuSans-20\"/>\n      <use x=\"95.410156\" xlink:href=\"#DejaVuSans-68\"/>\n      <use x=\"158.789062\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"219.970703\" xlink:href=\"#DejaVuSans-75\"/>\n      <use x=\"283.349609\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"335.449219\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"396.972656\" xlink:href=\"#DejaVuSans-68\"/>\n      <use x=\"460.351562\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"521.533203\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"549.316406\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"612.792969\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n    <g id=\"line2d_24\">\n     <path d=\"M 271.914063 180.704062 \nL 291.914063 180.704062 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_25\"/>\n    <g id=\"text_19\">\n     <!-- 3 households -->\n     <g transform=\"translate(299.914063 184.204062)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-33\"/>\n      <use x=\"63.623047\" xlink:href=\"#DejaVuSans-20\"/>\n      <use x=\"95.410156\" xlink:href=\"#DejaVuSans-68\"/>\n      <use x=\"158.789062\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"219.970703\" xlink:href=\"#DejaVuSans-75\"/>\n      <use x=\"283.349609\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"335.449219\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"396.972656\" xlink:href=\"#DejaVuSans-68\"/>\n      <use x=\"460.351562\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"521.533203\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"549.316406\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"612.792969\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n    <g id=\"line2d_26\">\n     <path d=\"M 271.914063 195.382187 \nL 291.914063 195.382187 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_27\"/>\n    <g id=\"text_20\">\n     <!-- 4 households -->\n     <g transform=\"translate(299.914063 198.882187)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-34\"/>\n      <use x=\"63.623047\" xlink:href=\"#DejaVuSans-20\"/>\n      <use x=\"95.410156\" xlink:href=\"#DejaVuSans-68\"/>\n      <use x=\"158.789062\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"219.970703\" xlink:href=\"#DejaVuSans-75\"/>\n      <use x=\"283.349609\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"335.449219\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"396.972656\" xlink:href=\"#DejaVuSans-68\"/>\n      <use x=\"460.351562\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"521.533203\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"549.316406\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"612.792969\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n    <g id=\"line2d_28\">\n     <path d=\"M 271.914063 210.060312 \nL 291.914063 210.060312 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_29\"/>\n    <g id=\"text_21\">\n     <!-- 5 households -->\n     <g transform=\"translate(299.914063 213.560312)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"63.623047\" xlink:href=\"#DejaVuSans-20\"/>\n      <use x=\"95.410156\" xlink:href=\"#DejaVuSans-68\"/>\n      <use x=\"158.789062\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"219.970703\" xlink:href=\"#DejaVuSans-75\"/>\n      <use x=\"283.349609\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"335.449219\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"396.972656\" xlink:href=\"#DejaVuSans-68\"/>\n      <use x=\"460.351562\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"521.533203\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"549.316406\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"612.792969\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pf4571c3fa4\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"40.603125\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.7 64-bit ('marketsai-reVLCGV_-py3.8': poetry)"
  },
  "interpreter": {
   "hash": "1c99579f0a861f1ade1e1abc4784a15ca138f424327e789e2dba1116d0806699"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}