
\def\covmac{/Users/matiascovarrubias/Documents/universidad/NYU/Research/Repositories/marketsAI/marketsai/Documents/Figures}
\let\dir=\covmac

\documentclass[11pt,english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{geometry}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{setspace}
\usepackage[authoryear]{natbib}
\onehalfspacing

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\newlength{\lyxlabelwidth}      % auxiliary length 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
% Added by lyx2lyx
%  for proper underlining
\PassOptionsToPackage{normalem}{ulem}
\usepackage{ulem}
\let\cite@rig\cite
\newcommand{\b@xcite}[2][\%]{\def\def@pt{\%}\def\pas@pt{#1}
  \mbox{\ifx\def@pt\pas@pt\cite@rig{#2}\else\cite@rig[#1]{#2}\fi}}
\renewcommand{\underbar}[1]{{\let\cite\b@xcite\uline{#1}}}

\usepackage{tikz}
\usetikzlibrary{arrows,snakes,shapes,calc}
\usepackage{pgfplots}
\usepackage{color}
\definecolor{darkred}{rgb}{0.8,0.1,.3}

%\usepackage[pagebackref=true]{hyperref}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=blue,%
    filecolor=black,%
    anchorcolor=darkred,
    linkcolor=darkred,%
    urlcolor=cyan
}

\usepackage[titletoc,toc,title]{appendix}
\usepackage{colortbl}
\usepackage[bottom]{footmisc}
\usepackage{hhline}

% For auto-creation of tables
\usepackage{amsmath,amsthm, graphicx, setspace, booktabs, tabularx, subcaption}
\usepackage{amsfonts, fancyhdr, epstopdf, color, verbatim, pdflscape}
\usepackage[font=small,format=plain,labelfont=bf,textfont=it]{caption}
\usepackage{times}
\makeatother
\usepackage{babel}

% new commands
\newcommand{\E}{\mathbb{E}}

\begin{document}
\title{Deep Reinforcement Learning in Macroeconomic Models}
\author{Matias Covarrubias}
\maketitle
\begin{abstract}
Can artificial intelligent (AI) algorithms learn optimal intertemporal behavior without knowing any mathematical details of the economy beforehand?  How should we specify economies and markets so as to make them easy to learn? In this paper, I apply Deep Reinforcement Learning (Deep RL), the current front-runner in the design of AI agents, to macroeconomic problems. In the context of an investment under uncertainty model, I show that Deep RL agents can learn the rational expectations solution and I suggest design choices that aid such learning. Then, by manipulating our baseline framework I highlight three topics in which this technology can be useful: high dimensional problems, incomplete information problems and equilibrium selection in models with multiple equilibrium.
\end{abstract}

\newpage 
\section{Introduction}

\begin{itemize}
	\item Paragraph 1: Reframe the abstract in relation to Learning in Macroeconomics. Look at Learning in Macro book for queues. Motivate this particular learning technology. \medskip
	
	\item Paragraph 2: Explain model free RL. look at Calvano for queues. \medskip 
	
	\begin{itemize}
		\item RL is a class of algorithms that adapt dynamic programming techniques such as value function and policy iteration to the problem of online learning, that is, to learn how to control a system by interacting with it. Other historical names for this method are approximate dynamic programming, learning based control and automated optimization.  \medskip
		\item A key characteristic of RL algorithms is that they do not use any mathematical knowledge of  the system they are controlling. They feed actions to the system and observe the reward they get that period and how the state evolves. After millions of such transitions, if learning is successful, they are able to estimate the consequences of their actions accurately and thus make optimal decisions. \medskip
		\item Both in theory and in practice, one of the most important ingredients for successful learning is exploration. Agents do not always act according to what they currently believe to be optimal but they sometimes choose actions at random in order to learn. How much exploration to perform is an open problem, called the exploration-exploitation dilemma. \medskip
	\end{itemize}
	
	\item Paragraph 3: Explain why this could be useful besides the study of learning in macroeconomics: \medskip 
	
	\begin{itemize}
		\item With this technology, we only need to specify the economy's action space, observation space, initial state and transition laws. This makes it simple to setup a learning environment for the Deep RL agents. \medskip
		
		\item The algorithm's structure is mostly the same regarding the problem at hand. The algorithm only needs to adapt the dimensions of its function approximators (e.g., neural nets) to the state space and action space of the environment, which can be easily automated. Thus, the same algorithm can learn to play chess or to calculate optimal saving behavior. As a corollary, the information available to each agent can be manipulated easily, without making any changes to the algorithm.  \medskip
	\end{itemize}

	\item Paragraph 4: Traditional challenges of Deep RL and why economics may be particularly well suited for this technology.\medskip
	
	\begin{itemize}
		\item Rewards engineering is usually the main bottleneck. In many problems, such as games or robotics, you only get positive rewards after successfully achieving some final objective, so the researcher needs to design signals to indicate to the agents whether is making progress or not. In economics, rewards are well specified in every period. \medskip
		
		\item Also, RL algorithms usually require millions of transitions in order to learn, which may be computationally expensive. This problem is very serious in environments with high dimensional states, such as images, and with complex transitions logic, as in the case of games or robotics. In economics, sampling a transition is usually very fast, as we are only dealing with vectors whose transitions laws can be expressed with simple mathematical functions. \medskip
		
		\item Finally, in many problems the algorithms get stuck in local optima. While this is also a potential problem in complex economic models, for many models we have nice regularity properties. \medskip
	\end{itemize}
	
	\item Paragraph 5: But, economic problems present their own challenges. \medskip
	
	\begin{itemize}
		\item Markets are a multi-agent problem. While multi agent Deep RL is a very active topic with some impressive success cases, it faces a crucial challenge: Agents learn and explore while other agents learn and explore. This means that the environment is non-stationary and at the beginning it may be very chaotic. Whether order arises from such chaos is an empirical question with no theoretical guarantees. \medskip
		
		\item In economics, rigorous insights often rely on exact solutions. That is, we are trying to solve very precise control problems, so usual assumptions such as discrete action spaces may not be satisfactory. Deep RL agents might get a very high percentage of the optimal discounted utility, but the solution may still lack some of the qualitative features of the exact solution. Example: growth model with fixed saving rate instead of decreasing saving rate. \medskip
	\end{itemize}

	\end{itemize}

	\subsection{Preview of leading framework and exercises} 
	
	\begin{itemize}
	
	\item  Paragraph 6: Explain the leading framework. 
	
	\item Paragraph 7: Explain learning results.
	
	\item Paragraph 8: Explain high dimensional, incomplete information, and multiple equilibrium exercises.
	
	\item Paragraph 9: Explain design lessons.
	
\end{itemize}
	
	\subsection{Related Literature}
	
	\begin{itemize}
		\item paragraph 1: Learning in Macroeconomics. \medskip
		
		\item paragraph 2: Reinforcement Learning in economics. \medskip
	\end{itemize}
	

\section{A Primer on Reinforcement Learning}

\begin{itemize}
\item Define Markov Decision Problems.
\item Example 1: Monopolist problem.
\item Algorithm: Q-learning
\begin{itemize}
	\item We start from simplest single agent algorithm. The problem is: \medskip
	$$\underset{a \in A}{\max} \sum_{t=s}^{\infty} \gamma^{s-t} E_t[r_{t+s}]$$
	
	\item Two value functions: \medskip
	\begin{enumerate}
		\item state-value function: $V(s)=\max_{a\in A} \{E[r|s,a] + \gamma E[V(s')|s,a] \}$ \medskip
		\item action-value function: $Q(s,a)=E(r|s,a)+\gamma E[\max_{a'\in A} Q(s',a')|s,a]$ \medskip
	\end{enumerate}
	\item Q-Learning:  \medskip
	\begin{itemize}
		\item Initialize $Q_0(s,a)$. Loop until converge:  \medskip
		\begin{itemize}
			\item agent chooses $a= \text{arg} \max Q(s,a)$  and observes $\{r, s'  ,s, a\}$. \medskip
			\item Old Q: $Q(s,a)$. \medskip
			\item  Target Q: $r_t+\gamma \max_{a' \in A} Q(s',a')$. \medskip
			\item New Q: $$Q'(s,a)=(1-\alpha) Q(s,a) +\alpha [r+\gamma \max_{a' \in A} Q(s',a')]$$ \medskip
		\end{itemize}
	\end{itemize}
\end{itemize}
\item Proof of convergence and theory.
\begin{itemize}
\item \citet{tsitsiklis1994} perform an extensive convergence analysis of q-learning algorithms. His proof of convergence is based on contraction mapping theorem.
\item \citet{bertsekas2012} introduce a mix of policy iteration and q-learning
that improves efficiency even for approximate methods. It seems great
check it.
\item \citet{ma2020} show formally that the recursive specification of
the q values can be considered as a transformation of the bellman
operator. The authors use this q-transform to solve problems with
unbounded rewards.
\item \citet{ma2021} use computational complexity theory and numerical
experiments to conclude that the q-transform leads to gains in computational
efficiency. NOTES: WHY?


\end{itemize}
\end{itemize}

\subsection{Deep Reinforcement Learning}

\begin{itemize}
	\item Deep Q Network (DQN)
	\begin{itemize}
		\item DQN in approximating the Q function with a neural net: $\hat{Q}(s,a) \equiv  \mathcal{N}_\rho \sim Q(s,a)$.\medskip
		\item The parameters $\rho$ are estimated in order to minimize\medskip
		$$\mathcal{L}(\rho)=\E \left[ \left(\hat{Q}(s,a)-(r_t+\gamma \max_{a' \in A} \hat{Q}(s',a')) \right)^2 \right] $$
		
		in observed transitions $\{r, s'  ,s, a\}$ .\medskip
		
		\item The most widely used NN is a composite function made of nested linear functions:  \medskip
		
		$$\mathcal{N}_\rho (x)=\sigma_K(W_K ... \sigma_2(W_2 \sigma_1(W_1x+b_1)+b_2)...+b_K)$$
		
		where $x$ is the input (in our case the transitions $\{r, s'  ,s, a\}$), $W_i \in \mathbb{R}^{m_{i+1}\times m_i}$ are the weights, $b_i \in \mathbb{R}^{m_i+1}$ are the biases, and $\sigma()$ are activation functions.
	\end{itemize}
	\item Explain SAC or PPO.
\end{itemize}

\subsection{Multi-Agent Reinforcement Learning}

\begin{itemize}
\item Define Partially Observed Markov Games.
\item Example 1: Algorithmic Collusion.
\item Algorithm: Independent learning.
\item Challenges of multi-agent.
\item Soutions: QMix and other algorithms. 
\item Prominent success cases:
\begin{itemize}
	
\item \citet{berner2019} achieve super-human performance at the game Dota 2. The game is challenging for multiple reasons. It is multi-agent, stochastic, it has a large action and observation space and it requires long term strategy. They authors used a single neural net to approximate both the value function and the policiy function. In particular, they used a single-layer 4096-unit LSTM.

\item \citet{baker2019} show that agents self-playing hide and seek in a complex environment exhibit emergent curricular learning, in which they progressively discover strategies and then device counter-strategies. They use PPO with GAE. The distributed computation framework is called rapid. 
\item \citet{vinyals2019} achieve grand-master level at the game StarCraft 2. ``Observations of player and opponent units are processed using a self-attention mechanism. To integrate spatial and non-spatial information, we introduce scatter connections. To deal with partial observability, the temporal sequence of observations is processed by a deep long short-term memory (LSTM) system. To manage the structured, combinatorial action space, the agent uses an auto-regressive policy and recurrent
pointer network.''. ``For every training agent in the league, we run 16,000 concurrent StarCraft II matches and 16 actor tasks (each
using a TPU v3 device with eight TPU cores23) to perform inference. The game instances progress asynchronously on preemptible CPUs (roughly equivalent to 150 processors with 28 physical cores each), but requests for agent steps are batched together dynamically to make efficient use of the TPU. Using TPUs for batched inference provides large efficiency gains over previous work14,29. Actors send sequences of observations, actions, and rewards over the network to a central 128-core TPU learner
worker, which updates the parameters of the training agent. The received data are buffered in memory and replayed twice. The learner worker performs large-batch synchronous updates. Each TPU core processes a mini-batch of four sequences, for a total batch size of 512. The learner processes about 50,000 agent steps per second. The actors update their copy of the parameters from the learner every 10 s. We instantiate 12 separate copies of this actor--learner setup: one main agent, one main exploiter and two league exploiter agents for each StarCraft race. One central coordinator maintains an estimate of the payoff matrix, samples new matches on request, and resets main and league exploiters. Additional evaluator workers (running on the CPU) are used to supplement the payoff estimates. See Extended Data Fig. 6 for an overview of the training setup.''
\end{itemize}
\end{itemize}


\section{Baseline Framework}

\subsection{Planing Formulation}

\subsection{Market Formulation}

\subsection{Steady State}

\subsection{Global Solution}

\section{Applying Reinforcement Learning}


\subsection{Learning by a Planner}

\begin{itemize}
	\item Exercise 1: 1 capital good, many households (1 vs 4 vs 100)
	\item Exercise 2: many capital goods, (1v1, 2v2, 10v10)
\end{itemize}

\subsection{Multi-agent learning}
\begin{itemize}
	\item Explain centralized critic algorithm.
	\item Explain parallelized computation framework.
	\item Exercise 1: 1 vs 5 vs 100 households.
	\item Exercise 2: Capital Good firms as AI agents. 1 vs 1, 2 vs 2, 10 vs 10. Does it converge to competitive equilibrium?	
	\item Exercise 3: Half the households have full info and half the houselods only observe own and aggregate stock.
\end{itemize}


\section{Design Lessons}
\begin{itemize}
	\item Exercise 1: Choosing spending vs choosing quantities. General normalization issues.
	\item Exercise 2: How to specify markets.
	\item Exercise 3: How to impose constrains.
	\item Exercise 3: Learning rates and algorithmic issues?
\end{itemize}

\section{Conclusion}

bla bla


\bibliographystyle{plainnat}
\bibliography{MarketsAI.bib}

\end{document}
