
\def\covmac{/Users/matiascovarrubias/Documents/universidad/NYU/Research/Repositories/marketsAI/marketsai/Documents/Figures}
\let\dir=\covmac

\documentclass[11pt,english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{geometry}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{setspace}
\usepackage[authoryear]{natbib}
\onehalfspacing

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\newlength{\lyxlabelwidth}      % auxiliary length 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
% Added by lyx2lyx
%  for proper underlining
\PassOptionsToPackage{normalem}{ulem}
\usepackage{ulem}
\let\cite@rig\cite
\newcommand{\b@xcite}[2][\%]{\def\def@pt{\%}\def\pas@pt{#1}
  \mbox{\ifx\def@pt\pas@pt\cite@rig{#2}\else\cite@rig[#1]{#2}\fi}}
\renewcommand{\underbar}[1]{{\let\cite\b@xcite\uline{#1}}}

\usepackage{tikz}
\usetikzlibrary{arrows,snakes,shapes,calc}
\usepackage{pgfplots}
\usepackage{color}
\definecolor{darkred}{rgb}{0.8,0.1,.3}

%\usepackage[pagebackref=true]{hyperref}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=blue,%
    filecolor=black,%
    anchorcolor=darkred,
    linkcolor=darkred,%
    urlcolor=cyan
}

\usepackage[titletoc,toc,title]{appendix}
\usepackage{colortbl}
\usepackage[bottom]{footmisc}
\usepackage{hhline}

% For auto-creation of tables
\usepackage{amsmath,amsthm, graphicx, setspace, booktabs, tabularx, subcaption}
\usepackage{amsfonts, fancyhdr, epstopdf, color, verbatim, pdflscape}
\usepackage[font=small,format=plain,labelfont=bf,textfont=it]{caption}
\usepackage{times}
\makeatother
\usepackage{babel}


\begin{document}
\title{MarketsAI\\
A Deep Reinforcement Learning Approach to Simulating High-Dimensional and Imperfect Information Economies}
\author{Matias Covarrubias}
\maketitle
\begin{abstract}
This paper presents a new framework to set and solve economic models
that uses Reinforcement Learning (RL) as a description of the agents'
decision making process. 
\end{abstract}

\section{Single-Agent Reinforcement Learning}

\subsection{An introduction to Reinforcement Learning}
\begin{itemize}
\item Define Markov Decision Problems.
\item Example 1: Monopolist problem.
\item Algorithm: Q-learning.
\item Proof of convergence and theory.
\begin{itemize}
\item \citet{ma2020} show formally that the recursive specification of
the q values can be considered as a transformation of the bellman
operator. The authors use this q-transform to solve problems with
unbounded rewards.
\item \citet{ma2021} use computational complexity theory and numerical
experiments to conclude that the q-transform leads to gains in computational
efficiency. NOTES: WHY?
\item \citet{bertsekas2012} introduce a mix of policy iteration and q-learning
that improves efficiency even for approximate methods. It seems great
check it.
\item \citet{tsitsiklis1994} perform an extensive convergence analysis
of q-learning algorithms. His proof of convergence, which we will
review in chapter 1, is based on contraction mapping theorem,
\end{itemize}

\subsection{Deep Reinforcement Learning}

\begin{itemize}
	\item Explain DQN.
	\item Explain SAC.
\end{itemize}

\subsection{Solving Stochastic Growth Models}
\item Example 2: Stochastic Growth Model.
\item Q-learning vs Deep-Qlearning as the dimensionality increases.
\end{itemize}

\section{Multi-Agent Reinforcement Learning}

\subsection{Introduction to multi-agent Reinforcement learning}
\begin{itemize}
\item Define Partially Observed Markov Games.
\item Example 1: Algorithmic Collusion.
\item Algorithm: Independent learning.
\item Challenges of multi-agent.
\item Soutions: QMix and other algorithms. 
\item Famous examples.
\begin{itemize}
	
\item \citet{berner2019} achieve super-human performance at the game Dota 2. The game is challenging for multiple reasons. It is multi-agent, stochastic and it has a large action and observation space and it requires long term strategy. They authors used a single neural net to approzimate both the value function and the policiy function. In particular, they used a single-layer 4096-unit LSTM.

\item \citet{baker2019} show that agents self-playing hide and seek in a complex environment exhibit emergent curricular learning, in which they progressively discover strategies and the device counter-strategies. They use PPO with GAE. The distributed computation framework is called rapid. 
\item \citet{vinyals2019} achieve grand-master level at the game StarCraft 2. ``Observations of player and opponent units are processed using a self-attention mechanism. To integrate spatial and non-spatial information, we introduce scatter connections. To deal with partial observability, the temporal sequence of observations is processed by a deep long short-term memory (LSTM) system. To manage the structured, combinatorial action space, the agent uses an auto-regressive policy and recurrent
pointer network.''. ``For every training agent in the league, we run 16,000 concurrent StarCraft II matches and 16 actor tasks (each
using a TPU v3 device with eight TPU cores23) to perform inference. The game instances progress asynchronously on preemptible CPUs (roughly equivalent to 150 processors with 28 physical cores each), but requests for agent steps are batched together dynamically to make efficient use of the TPU. Using TPUs for batched inference provides large efficiency gains over previous work14,29. Actors send sequences of observations, actions, and rewards over the network to a central 128-core TPU learner
worker, which updates the parameters of the training agent. The received data are buffered in memory and replayed twice. The learner worker performs large-batch synchronous updates. Each TPU core processes a mini-batch of four sequences, for a total batch size of 512. The learner processes about 50,000 agent steps per second. The actors update their copy of the parameters from the learner every 10 s. We instantiate 12 separate copies of this actor--learner setup: one main agent, one main exploiter and two league exploiter agents for each StarCraft race. One central coordinator maintains an estimate of the payoff matrix, samples new matches on request, and resets main and league exploiters. Additional evaluator workers (running on the CPU) are used to supplement the payoff estimates. See Extended Data Fig. 6 for an overview of the training setup.''
\end{itemize}
\end{itemize}

\subsection{Market solution of the Stochastic Growth Model}

\begin{itemize}
	\item Do it
\end{itemize}

\section{Scaling up: Heterogenous Agents}

Intro

\subsection{Centralized learning, decentralized execution}
\begin{itemize}
	\item Explain centralized critic algorithm.
	\item Explain parallelized computation framework.
	\begin{itemize}
		\item \citet{espeholt2018} develop the IMPALA framework, which is dfesigned
		to learn in parallel in a stable and efficient way.
	\end{itemize}
\end{itemize}

\subsection{Solving the Krussel and Smidt framework}

Present model and solutions

\section{Multi-Market Environemnts: The Economy Constructor}
\begin{itemize}
\item Explain economy constructor.
\item Example 1: Two-sector model with labor.
\item Explain markets.
\end{itemize}



\bibliographystyle{plainnat}
\bibliography{MarketsAI.bib}

\end{document}
