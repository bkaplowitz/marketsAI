{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python387jvsc74a57bd01c99579f0a861f1ade1e1abc4784a15ca138f424327e789e2dba1116d0806699",
   "display_name": "Python 3.8.7 64-bit ('marketsai-reVLCGV_-py3.8': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "1c99579f0a861f1ade1e1abc4784a15ca138f424327e789e2dba1116d0806699"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "from marketsai.markets.diff_demand import DiffDemand\n",
    "from marketsai.economies.economies import Economy\n",
    "\n",
    "#import ray\n",
    "\n",
    "from ray import tune, shutdown, init\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.agents.a3c.a2c import A2CTrainer\n",
    "from ray.rllib.agents.dqn.dqn import DQNTrainer\n",
    "from ray.tune.integration.mlflow import MLflowLoggerCallback\n",
    "from ray.rllib.utils.schedules.exponential_schedule import ExponentialSchedule\n",
    "\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 0: Inititialize ray\n",
    "\n",
    "NUM_CPUS = 14\n",
    "shutdown()\n",
    "init(num_cpus=NUM_CPUS, logging_level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: register environment\n",
    "\n",
    "register_env(\"economy\", Economy)\n",
    "env = Economy()\n",
    "policy_ids = [\"policy_{}\".format(i) for i in range(env.n_agents)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Experiment configuration\n",
    "\n",
    "MAX_STEPS = 2000 * 1000\n",
    "PRICE_BAND_WIDE = 0.1\n",
    "LOWER_PRICE = 1.47 - PRICE_BAND_WIDE\n",
    "HIGHER_PRICE = 1.93 + PRICE_BAND_WIDE\n",
    "DEC_RATE = math.e ** (-4 * 10 ** (-6))\n",
    "DEC_RATE_HIGH = math.e ** (-4 * 10 ** (-6) * 4)\n",
    "mkt_config = {\n",
    "    \"lower_price\": [LOWER_PRICE for i in range(env.n_agents)],\n",
    "    \"higher_price\": [HIGHER_PRICE for i in range(env.n_agents)],\n",
    "}\n",
    "env_config = {\"markets_dict\": {\"market_0\": (DiffDemand, mkt_config), \"market_1\": (DiffDemand, mkt_config)}}\n",
    "\n",
    "exploration_config = {\n",
    "    \"type\": \"EpsilonGreedy\",\n",
    "    \"epsilon_schedule\": ExponentialSchedule(\n",
    "        schedule_timesteps=1,\n",
    "        framework=None,\n",
    "        initial_p=1,\n",
    "        decay_rate=DEC_RATE,\n",
    "    ),\n",
    "}\n",
    "\n",
    "config = {\n",
    "    \"gamma\": 0.95,\n",
    "    \"lr\": 0.15,\n",
    "    \"env\": \"economy\",\n",
    "    \"exploration_config\": exploration_config,\n",
    "    \"env_config\": env_config,\n",
    "    \"horizon\": 100,\n",
    "    \"soft_horizon\": True,\n",
    "    \"no_done_at_end\": True,\n",
    "    \"multiagent\": {\n",
    "        \"policies\": {\n",
    "            policy_ids[i]: (\n",
    "                None,\n",
    "                env.observation_space[\"agent_{}\".format(i)],\n",
    "                env.action_space[\"agent_{}\".format(i)],\n",
    "                {},\n",
    "            )\n",
    "            for i in range(env.n_agents)\n",
    "        },\n",
    "        \"policy_mapping_fn\": (lambda agent_id: policy_ids[int(agent_id.split(\"_\")[1])]),\n",
    "    },\n",
    "    \"framework\": \"torch\",\n",
    "    \"num_workers\": NUM_CPUS - 1,\n",
    "    \"num_gpus\": 0,\n",
    "    \"log_level\": \"ERROR\",\n",
    "    #\"normalize_actions\": False\n",
    "}\n",
    "\n",
    "stop = {\"info/num_steps_trained\": MAX_STEPS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(env.observation_space[\"agent_0\"].nvec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Step 3: Experiments\n",
    "\n",
    "exp_name = \"econ_PG_April8\"\n",
    "results = tune.run(\n",
    "    \"PG\",\n",
    "    name=exp_name,\n",
    "    config=config,\n",
    "    checkpoint_freq=250,\n",
    "    checkpoint_at_end=True,\n",
    "    stop=stop,\n",
    "    metric=\"episode_reward_mean\",\n",
    "    mode=\"max\",\n",
    "    callbacks=[MLflowLoggerCallback(experiment_name=exp_name, save_artifact=True)],\n",
    ")\n",
    "\n",
    "best_checkpoint = results.best_checkpoint\n",
    "print(\"Best checkpont:\", best_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = \"econ_PPO_April8\"\n",
    "results = tune.run(\n",
    "    \"PPO\",\n",
    "    name=exp_name,\n",
    "    config=config,\n",
    "    checkpoint_freq=250,\n",
    "    checkpoint_at_end=True,\n",
    "    stop=stop,\n",
    "    metric=\"episode_reward_mean\",\n",
    "    mode=\"max\",\n",
    "    callbacks=[MLflowLoggerCallback(experiment_name=exp_name, save_artifact=True)],\n",
    ")\n",
    "\n",
    "best_checkpoint = results.best_checkpoint\n",
    "print(\"Best checkpont:\", best_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = \"econ_APPO_April8\"\n",
    "results = tune.run(\n",
    "    \"APPO\",\n",
    "    name=exp_name,\n",
    "    config=config,\n",
    "    checkpoint_freq=250,\n",
    "    checkpoint_at_end=True,\n",
    "    stop=stop,\n",
    "    metric=\"episode_reward_mean\",\n",
    "    mode=\"max\",\n",
    "    callbacks=[MLflowLoggerCallback(experiment_name=exp_name, save_artifact=True)],\n",
    ")\n",
    "\n",
    "best_checkpoint = results.best_checkpoint\n",
    "print(\"Best checkpont:\", best_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = \"econ_IMPALA_April8\"\n",
    "results = tune.run(\n",
    "    \"IMPALA\",\n",
    "    name=exp_name,\n",
    "    config=config,\n",
    "    checkpoint_freq=250,\n",
    "    checkpoint_at_end=True,\n",
    "    stop=stop,\n",
    "    metric=\"episode_reward_mean\",\n",
    "    mode=\"max\",\n",
    "    callbacks=[MLflowLoggerCallback(experiment_name=exp_name, save_artifact=True)],\n",
    ")\n",
    "\n",
    "best_checkpoint = results.best_checkpoint\n",
    "print(\"Best checkpont:\", best_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Step 4: Evaluation\n",
    "\n",
    "config[\"evaluation_config\"] = {\"explore\": False}\n",
    "trained_trainer = DQNTrainer(config=config)\n",
    "trained_trainer.restore(best_checkpoint)\n",
    "price_agent0_list = []\n",
    "reward_agent0_list = []\n",
    "price_agent1_list = []\n",
    "reward_agent1_list = []\n",
    "obs, reward, done, info = env.step({\"agent_0\": 1, \"agent_1\": 11})\n",
    "for i in range(500):\n",
    "\n",
    "    action_agent0 = trained_trainer.compute_action(obs[\"agent_0\"], policy_id=\"policy_0\")\n",
    "    action_agent1 = trained_trainer.compute_action(obs[\"agent_1\"], policy_id=\"policy_1\")\n",
    "    obs, reward, done, info = env.step(\n",
    "        {\"agent_0\": action_agent0, \"agent_1\": action_agent1}\n",
    "    )\n",
    "    price_agent0_list.append(info[\"agent_0\"])\n",
    "    reward_agent0_list.append(reward[\"agent_0\"])\n",
    "    price_agent1_list.append(info[\"agent_1\"])\n",
    "    reward_agent1_list.append(reward[\"agent_1\"])\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "plt.plot(price_agent0_list)\n",
    "plt.show()\n",
    "plt.plot(price_agent1_list)\n",
    "plt.show()\n",
    "\n",
    "IRresults = {\n",
    "    \"Profits Agent 0\": reward_agent0_list,\n",
    "    \"Profits Agent 1\": reward_agent1_list,\n",
    "    \"Price Agent 0\": price_agent0_list,\n",
    "    \"Price Agent 1\": price_agent1_list,\n",
    "}\n",
    "df_IR = pd.DataFrame(IRresults)\n",
    "df_IR.to_csv(\"collusion_IR_DQN.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}