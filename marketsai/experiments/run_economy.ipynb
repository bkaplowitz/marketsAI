{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python387jvsc74a57bd01c99579f0a861f1ade1e1abc4784a15ca138f424327e789e2dba1116d0806699",
   "display_name": "Python 3.8.7 64-bit ('marketsai-reVLCGV_-py3.8': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "1c99579f0a861f1ade1e1abc4784a15ca138f424327e789e2dba1116d0806699"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/pandas/compat/__init__.py:97: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n",
      "WARNING:tensorflow:From /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "\n",
    "from marketsai.markets.diff_demand import DiffDemandDiscrete\n",
    "from marketsai.economies.economies import Economy\n",
    "\n",
    "#import ray\n",
    "\n",
    "from ray import tune, shutdown, init\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.agents.a3c.a2c import A2CTrainer\n",
    "from ray.rllib.agents.dqn.dqn import DQNTrainer\n",
    "from ray.tune.integration.mlflow import MLflowLoggerCallback\n",
    "from ray.rllib.utils.schedules.exponential_schedule import ExponentialSchedule\n",
    "\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.1.202',\n",
       " 'raylet_ip_address': '192.168.1.202',\n",
       " 'redis_address': '192.168.1.202:29974',\n",
       " 'object_store_address': '/tmp/ray/session_2021-04-08_10-58-19_259942_8782/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2021-04-08_10-58-19_259942_8782/sockets/raylet',\n",
       " 'webui_url': '127.0.0.1:8265',\n",
       " 'session_dir': '/tmp/ray/session_2021-04-08_10-58-19_259942_8782',\n",
       " 'metrics_export_port': 59454,\n",
       " 'node_id': '0522fa44f4e05c55e0dfc1dc6fd7f94c1ea823d181e6f4e6d15ae775'}"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# STEP 0: Inititialize ray\n",
    "\n",
    "NUM_CPUS = 14\n",
    "shutdown()\n",
    "init(num_cpus=NUM_CPUS, logging_level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: register environment\n",
    "\n",
    "register_env(\"economy\", Economy)\n",
    "env = Economy()\n",
    "policy_ids = [\"policy_{}\".format(i) for i in range(env.n_agents)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Experiment configuration\n",
    "\n",
    "MAX_STEPS = 10 * 1000\n",
    "PRICE_BAND_WIDE = 0.1\n",
    "LOWER_PRICE = 1.47 - PRICE_BAND_WIDE\n",
    "HIGHER_PRICE = 1.93 + PRICE_BAND_WIDE\n",
    "DEC_RATE = math.e ** (-4 * 10 ** (-6))\n",
    "DEC_RATE_HIGH = math.e ** (-4 * 10 ** (-6) * 4)\n",
    "mkt_config = {\n",
    "    \"lower_price\": [LOWER_PRICE for i in range(env.n_agents)],\n",
    "    \"higher_price\": [HIGHER_PRICE for i in range(env.n_agents)],\n",
    "}\n",
    "env_config = {\"markets_dict\": {\"market_0\": (DiffDemandDiscrete, mkt_config), \"market_1\": (DiffDemandDiscrete, mkt_config)}}\n",
    "\n",
    "exploration_config = {\n",
    "    \"type\": \"EpsilonGreedy\",\n",
    "    \"epsilon_schedule\": ExponentialSchedule(\n",
    "        schedule_timesteps=1,\n",
    "        framework=None,\n",
    "        initial_p=1,\n",
    "        decay_rate=DEC_RATE,\n",
    "    ),\n",
    "}\n",
    "\n",
    "config = {\n",
    "    \"gamma\": 0.95,\n",
    "    \"lr\": 0.15,\n",
    "    \"env\": \"economy\",\n",
    "    \"exploration_config\": exploration_config,\n",
    "    \"env_config\": env_config,\n",
    "    \"horizon\": 100,\n",
    "    \"soft_horizon\": True,\n",
    "    \"no_done_at_end\": True,\n",
    "    \"multiagent\": {\n",
    "        \"policies\": {\n",
    "            policy_ids[i]: (\n",
    "                None,\n",
    "                env.observation_space[\"agent_{}\".format(i)],\n",
    "                env.action_space[\"agent_{}\".format(i)],\n",
    "                {},\n",
    "            )\n",
    "            for i in range(env.n_agents)\n",
    "        },\n",
    "        \"policy_mapping_fn\": (lambda agent_id: policy_ids[int(agent_id.split(\"_\")[1])]),\n",
    "    },\n",
    "    \"framework\": \"torch\",\n",
    "    \"num_workers\": NUM_CPUS - 1,\n",
    "    \"num_gpus\": 0,\n",
    "    \"log_level\": \"ERROR\"\n",
    "}\n",
    "\n",
    "stop = {\"info/num_steps_trained\": MAX_STEPS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(env.observation_space[\"agent_0\"].nvec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 18.8/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14/14 CPUs, 0/0 GPUs, 0.0/10.74 GiB heap, 0.0/3.71 GiB objects<br>Result logdir: /Users/matiascovarrubias/ray_results/econ_PPO_TESApril8<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name             </th><th>status  </th><th>loc  </th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_economy_e4bbe_00000</td><td>RUNNING </td><td>     </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[2m\u001b[36m(pid=9957)\u001b[0m /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/pandas/compat/__init__.py:97: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "\u001b[2m\u001b[36m(pid=9957)\u001b[0m   warnings.warn(msg)\n",
      "\u001b[2m\u001b[36m(pid=9957)\u001b[0m WARNING:tensorflow:From /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=9957)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=9957)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=9957)\u001b[0m 2021-04-08 10:58:37,964\tINFO trainer.py:641 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=9954)\u001b[0m /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/pandas/compat/__init__.py:97: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "\u001b[2m\u001b[36m(pid=9954)\u001b[0m   warnings.warn(msg)\n",
      "\u001b[2m\u001b[36m(pid=9950)\u001b[0m /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/pandas/compat/__init__.py:97: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "\u001b[2m\u001b[36m(pid=9950)\u001b[0m   warnings.warn(msg)\n",
      "\u001b[2m\u001b[36m(pid=9955)\u001b[0m /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/pandas/compat/__init__.py:97: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "\u001b[2m\u001b[36m(pid=9955)\u001b[0m   warnings.warn(msg)\n",
      "\u001b[2m\u001b[36m(pid=9952)\u001b[0m /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/pandas/compat/__init__.py:97: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "\u001b[2m\u001b[36m(pid=9952)\u001b[0m   warnings.warn(msg)\n",
      "\u001b[2m\u001b[36m(pid=9956)\u001b[0m /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/pandas/compat/__init__.py:97: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "\u001b[2m\u001b[36m(pid=9956)\u001b[0m   warnings.warn(msg)\n",
      "\u001b[2m\u001b[36m(pid=9958)\u001b[0m /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/pandas/compat/__init__.py:97: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "\u001b[2m\u001b[36m(pid=9958)\u001b[0m   warnings.warn(msg)\n",
      "\u001b[2m\u001b[36m(pid=9959)\u001b[0m /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/pandas/compat/__init__.py:97: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "\u001b[2m\u001b[36m(pid=9959)\u001b[0m   warnings.warn(msg)\n",
      "\u001b[2m\u001b[36m(pid=9953)\u001b[0m /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/pandas/compat/__init__.py:97: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "\u001b[2m\u001b[36m(pid=9953)\u001b[0m   warnings.warn(msg)\n",
      "\u001b[2m\u001b[36m(pid=9951)\u001b[0m /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/pandas/compat/__init__.py:97: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "\u001b[2m\u001b[36m(pid=9951)\u001b[0m   warnings.warn(msg)\n",
      "\u001b[2m\u001b[36m(pid=9947)\u001b[0m /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/pandas/compat/__init__.py:97: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "\u001b[2m\u001b[36m(pid=9947)\u001b[0m   warnings.warn(msg)\n",
      "\u001b[2m\u001b[36m(pid=9948)\u001b[0m /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/pandas/compat/__init__.py:97: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "\u001b[2m\u001b[36m(pid=9948)\u001b[0m   warnings.warn(msg)\n",
      "\u001b[2m\u001b[36m(pid=9949)\u001b[0m /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/pandas/compat/__init__.py:97: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "\u001b[2m\u001b[36m(pid=9949)\u001b[0m   warnings.warn(msg)\n",
      "\u001b[2m\u001b[36m(pid=9946)\u001b[0m /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/pandas/compat/__init__.py:97: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "\u001b[2m\u001b[36m(pid=9946)\u001b[0m   warnings.warn(msg)\n",
      "\u001b[2m\u001b[36m(pid=9954)\u001b[0m WARNING:tensorflow:From /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=9954)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=9954)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=9950)\u001b[0m WARNING:tensorflow:From /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=9950)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=9950)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=9955)\u001b[0m WARNING:tensorflow:From /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=9955)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=9955)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=9952)\u001b[0m WARNING:tensorflow:From /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=9952)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=9952)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=9956)\u001b[0m WARNING:tensorflow:From /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=9956)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=9956)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=9958)\u001b[0m WARNING:tensorflow:From /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=9958)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=9958)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=9959)\u001b[0m WARNING:tensorflow:From /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=9959)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=9959)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=9953)\u001b[0m WARNING:tensorflow:From /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=9953)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=9953)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=9951)\u001b[0m WARNING:tensorflow:From /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=9951)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=9951)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=9947)\u001b[0m WARNING:tensorflow:From /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=9947)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=9947)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=9948)\u001b[0m WARNING:tensorflow:From /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=9948)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=9948)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=9946)\u001b[0m WARNING:tensorflow:From /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=9946)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=9946)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=9949)\u001b[0m WARNING:tensorflow:From /Users/matiascovarrubias/Library/Caches/pypoetry/virtualenvs/marketsai-reVLCGV_-py3.8/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=9949)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=9949)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=9957)\u001b[0m 2021-04-08 10:58:41,831\tWARNING util.py:47 -- Install gputil for GPU system monitoring.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "Result for PPO_economy_e4bbe_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-08_10-59-03\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 110.41983738504057\n",
      "  episode_reward_mean: 108.13572018048711\n",
      "  episode_reward_min: 105.4880316949738\n",
      "  episodes_this_iter: 52\n",
      "  episodes_total: 52\n",
      "  experiment_id: b52ec2d5c34f426a83fb5563d0a0440d\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      policy_0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 0.15000000000000002\n",
      "        entropy: 0.7466130765103849\n",
      "        entropy_coeff: 0.0\n",
      "        kl: .inf\n",
      "        policy_loss: 0.2711750632379113\n",
      "        total_loss: .inf\n",
      "        vf_explained_var: 4.252864164300263e-05\n",
      "        vf_loss: 7.767244885607464\n",
      "      policy_1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 0.15000000000000002\n",
      "        entropy: 0.6215735722742066\n",
      "        entropy_coeff: 0.0\n",
      "        kl: .inf\n",
      "        policy_loss: 0.2702961244234225\n",
      "        total_loss: .inf\n",
      "        vf_explained_var: 7.2688592922531825e-09\n",
      "        vf_loss: 7.725970489222829\n",
      "    num_steps_sampled: 5200\n",
      "    num_steps_trained: 5200\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 13\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.71935483870968\n",
      "    ram_util_percent: 64.41290322580643\n",
      "  pid: 9957\n",
      "  policy_reward_max:\n",
      "    policy_0: 56.93220113238118\n",
      "    policy_1: 57.34929576447299\n",
      "  policy_reward_mean:\n",
      "    policy_0: 54.07281097970303\n",
      "    policy_1: 54.0629092007841\n",
      "  policy_reward_min:\n",
      "    policy_0: 51.08498540707976\n",
      "    policy_1: 50.53951210178133\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.23134647027649952\n",
      "    mean_env_wait_ms: 0.20786326928123186\n",
      "    mean_inference_ms: 3.165104831451878\n",
      "    mean_raw_obs_processing_ms: 0.479926217798306\n",
      "  time_since_restore: 21.274221658706665\n",
      "  time_this_iter_s: 21.274221658706665\n",
      "  time_total_s: 21.274221658706665\n",
      "  timers:\n",
      "    learn_throughput: 266.32\n",
      "    learn_time_ms: 19525.375\n",
      "    sample_throughput: 2990.198\n",
      "    sample_time_ms: 1739.016\n",
      "    update_time_ms: 3.542\n",
      "  timestamp: 1617893943\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 5200\n",
      "  training_iteration: 1\n",
      "  trial_id: e4bbe_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 20.4/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14/14 CPUs, 0/0 GPUs, 0.0/10.74 GiB heap, 0.0/3.71 GiB objects<br>Current best trial: e4bbe_00000 with episode_reward_mean=108.13572018048711 and parameters={'num_workers': 13, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'num_gpus': 0, 'train_batch_size': 4000, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'num_framestacks': 'auto', 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, 'framestack': True}, 'optimizer': {}, 'gamma': 0.95, 'horizon': 100, 'soft_horizon': True, 'no_done_at_end': True, 'env_config': {'markets_dict': {'market_0': (<class 'marketsai.markets.diff_demand.DiffDemandDiscrete'>, {'lower_price': [1.3699999999999999, 1.3699999999999999], 'higher_price': [2.03, 2.03]}), 'market_1': (<class 'marketsai.markets.diff_demand.DiffDemandDiscrete'>, {'lower_price': [1.3699999999999999, 1.3699999999999999], 'higher_price': [2.03, 2.03]})}}, 'env': 'economy', 'normalize_actions': False, 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 0.15, 'monitor': False, 'log_level': 'ERROR', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'torch', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'EpsilonGreedy', 'epsilon_schedule': <ray.rllib.utils.schedules.exponential_schedule.ExponentialSchedule object at 0x2f7ba3370>}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, '_use_trajectory_view_api': True, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'memory': 0, 'object_store_memory': 0, 'memory_per_worker': 0, 'object_store_memory_per_worker': 0, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy_0': (<class 'ray.rllib.policy.policy_template.PPOTorchPolicy'>, Tuple(MultiDiscrete([16 16]), MultiDiscrete([16 16])), Tuple(Discrete(16), Discrete(16)), {}), 'policy_1': (<class 'ray.rllib.policy.policy_template.PPOTorchPolicy'>, Tuple(MultiDiscrete([16 16]), MultiDiscrete([16 16])), Tuple(Discrete(16), Discrete(16)), {})}, 'policy_mapping_fn': <function <lambda> at 0x2f7bac160>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, 'replay_sequence_length': 1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'simple_optimizer': True, '_fake_gpus': False, 'vf_share_layers': -1}<br>Result logdir: /Users/matiascovarrubias/ray_results/econ_PPO_TESApril8<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name             </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_economy_e4bbe_00000</td><td>RUNNING </td><td>192.168.1.202:9957</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         21.2742</td><td style=\"text-align: right;\">5200</td><td style=\"text-align: right;\"> 108.136</td><td style=\"text-align: right;\">              110.42</td><td style=\"text-align: right;\">             105.488</td><td style=\"text-align: right;\">               100</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "Result for PPO_economy_e4bbe_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-04-08_10-59-29\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 111.10751351376987\n",
      "  episode_reward_mean: 107.93730209795307\n",
      "  episode_reward_min: 103.02265823373419\n",
      "  episodes_this_iter: 52\n",
      "  episodes_total: 104\n",
      "  experiment_id: b52ec2d5c34f426a83fb5563d0a0440d\n",
      "  hostname: Matiass-MBP.fios-router.home\n",
      "  info:\n",
      "    learner:\n",
      "      policy_0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.30000000000000004\n",
      "        cur_lr: 0.15000000000000002\n",
      "        entropy: 0.2422735836993485\n",
      "        entropy_coeff: 0.0\n",
      "        kl: .inf\n",
      "        policy_loss: 0.2848011527846499\n",
      "        total_loss: .inf\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 1.8295596129283673\n",
      "      policy_1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.30000000000000004\n",
      "        cur_lr: 0.15000000000000002\n",
      "        entropy: 0.04782537410099438\n",
      "        entropy_coeff: 0.0\n",
      "        kl: .inf\n",
      "        policy_loss: 0.2662815288072679\n",
      "        total_loss: .inf\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 1.1878204025873325\n",
      "    num_steps_sampled: 10400\n",
      "    num_steps_trained: 10400\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.202\n",
      "  num_healthy_workers: 13\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.351351351351347\n",
      "    ram_util_percent: 63.14594594594594\n",
      "  pid: 9957\n",
      "  policy_reward_max:\n",
      "    policy_0: 57.66976114134418\n",
      "    policy_1: 57.34929576447299\n",
      "  policy_reward_mean:\n",
      "    policy_0: 53.95569775347406\n",
      "    policy_1: 53.98160434447904\n",
      "  policy_reward_min:\n",
      "    policy_0: 49.82847825226365\n",
      "    policy_1: 50.53951210178133\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.23401349540684632\n",
      "    mean_env_wait_ms: 0.21060528809923135\n",
      "    mean_inference_ms: 3.317517346287519\n",
      "    mean_raw_obs_processing_ms: 0.4845169428340884\n",
      "  time_since_restore: 47.24294090270996\n",
      "  time_this_iter_s: 25.968719244003296\n",
      "  time_total_s: 47.24294090270996\n",
      "  timers:\n",
      "    learn_throughput: 238.794\n",
      "    learn_time_ms: 21776.051\n",
      "    sample_throughput: 2832.599\n",
      "    sample_time_ms: 1835.77\n",
      "    update_time_ms: 3.255\n",
      "  timestamp: 1617893969\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10400\n",
      "  training_iteration: 2\n",
      "  trial_id: e4bbe_00000\n",
      "  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 20.2/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 14/14 CPUs, 0/0 GPUs, 0.0/10.74 GiB heap, 0.0/3.71 GiB objects<br>Current best trial: e4bbe_00000 with episode_reward_mean=107.93730209795307 and parameters={'num_workers': 13, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'num_gpus': 0, 'train_batch_size': 4000, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'num_framestacks': 'auto', 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, 'framestack': True}, 'optimizer': {}, 'gamma': 0.95, 'horizon': 100, 'soft_horizon': True, 'no_done_at_end': True, 'env_config': {'markets_dict': {'market_0': (<class 'marketsai.markets.diff_demand.DiffDemandDiscrete'>, {'lower_price': [1.3699999999999999, 1.3699999999999999], 'higher_price': [2.03, 2.03]}), 'market_1': (<class 'marketsai.markets.diff_demand.DiffDemandDiscrete'>, {'lower_price': [1.3699999999999999, 1.3699999999999999], 'higher_price': [2.03, 2.03]})}}, 'env': 'economy', 'normalize_actions': False, 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 0.15, 'monitor': False, 'log_level': 'ERROR', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'torch', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'EpsilonGreedy', 'epsilon_schedule': <ray.rllib.utils.schedules.exponential_schedule.ExponentialSchedule object at 0x2f7c02f10>}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, '_use_trajectory_view_api': True, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'memory': 0, 'object_store_memory': 0, 'memory_per_worker': 0, 'object_store_memory_per_worker': 0, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy_0': (<class 'ray.rllib.policy.policy_template.PPOTorchPolicy'>, Tuple(MultiDiscrete([16 16]), MultiDiscrete([16 16])), Tuple(Discrete(16), Discrete(16)), {}), 'policy_1': (<class 'ray.rllib.policy.policy_template.PPOTorchPolicy'>, Tuple(MultiDiscrete([16 16]), MultiDiscrete([16 16])), Tuple(Discrete(16), Discrete(16)), {})}, 'policy_mapping_fn': <function <lambda> at 0x2f7b2b820>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, 'replay_sequence_length': 1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'simple_optimizer': True, '_fake_gpus': False, 'vf_share_layers': -1}<br>Result logdir: /Users/matiascovarrubias/ray_results/econ_PPO_TESApril8<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name             </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_economy_e4bbe_00000</td><td>RUNNING </td><td>192.168.1.202:9957</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         47.2429</td><td style=\"text-align: right;\">10400</td><td style=\"text-align: right;\"> 107.937</td><td style=\"text-align: right;\">             111.108</td><td style=\"text-align: right;\">             103.023</td><td style=\"text-align: right;\">               100</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 20.2/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/14 CPUs, 0/0 GPUs, 0.0/10.74 GiB heap, 0.0/3.71 GiB objects<br>Current best trial: e4bbe_00000 with episode_reward_mean=107.93730209795307 and parameters={'num_workers': 13, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'num_gpus': 0, 'train_batch_size': 4000, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'num_framestacks': 'auto', 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, 'framestack': True}, 'optimizer': {}, 'gamma': 0.95, 'horizon': 100, 'soft_horizon': True, 'no_done_at_end': True, 'env_config': {'markets_dict': {'market_0': (<class 'marketsai.markets.diff_demand.DiffDemandDiscrete'>, {'lower_price': [1.3699999999999999, 1.3699999999999999], 'higher_price': [2.03, 2.03]}), 'market_1': (<class 'marketsai.markets.diff_demand.DiffDemandDiscrete'>, {'lower_price': [1.3699999999999999, 1.3699999999999999], 'higher_price': [2.03, 2.03]})}}, 'env': 'economy', 'normalize_actions': False, 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 0.15, 'monitor': False, 'log_level': 'ERROR', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'torch', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'EpsilonGreedy', 'epsilon_schedule': <ray.rllib.utils.schedules.exponential_schedule.ExponentialSchedule object at 0x2f7c02f10>}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, '_use_trajectory_view_api': True, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'memory': 0, 'object_store_memory': 0, 'memory_per_worker': 0, 'object_store_memory_per_worker': 0, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy_0': (<class 'ray.rllib.policy.policy_template.PPOTorchPolicy'>, Tuple(MultiDiscrete([16 16]), MultiDiscrete([16 16])), Tuple(Discrete(16), Discrete(16)), {}), 'policy_1': (<class 'ray.rllib.policy.policy_template.PPOTorchPolicy'>, Tuple(MultiDiscrete([16 16]), MultiDiscrete([16 16])), Tuple(Discrete(16), Discrete(16)), {})}, 'policy_mapping_fn': <function <lambda> at 0x2f7b2b820>, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, 'replay_sequence_length': 1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'simple_optimizer': True, '_fake_gpus': False, 'vf_share_layers': -1}<br>Result logdir: /Users/matiascovarrubias/ray_results/econ_PPO_TESApril8<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name             </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_economy_e4bbe_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         47.2429</td><td style=\"text-align: right;\">10400</td><td style=\"text-align: right;\"> 107.937</td><td style=\"text-align: right;\">             111.108</td><td style=\"text-align: right;\">             103.023</td><td style=\"text-align: right;\">               100</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best checkpont: /Users/matiascovarrubias/ray_results/econ_PPO_TESApril8/PPO_economy_e4bbe_00000_0_2021-04-08_10-58-35/checkpoint_2/checkpoint-2\n"
     ]
    }
   ],
   "source": [
    "#Step 3: Experiments\n",
    "\n",
    "exp_name = \"econ_PPO_TESApril8\"\n",
    "results = tune.run(\n",
    "    \"PPO\",\n",
    "    name=exp_name,\n",
    "    config=config,\n",
    "    checkpoint_freq=250,\n",
    "    checkpoint_at_end=True,\n",
    "    stop=stop,\n",
    "    metric=\"episode_reward_mean\",\n",
    "    mode=\"max\",\n",
    "    callbacks=[MLflowLoggerCallback(experiment_name=exp_name, save_artifact=True)],\n",
    ")\n",
    "\n",
    "best_checkpoint = results.best_checkpoint\n",
    "print(\"Best checkpont:\", best_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Step 4: Evaluation\n",
    "\n",
    "config[\"evaluation_config\"] = {\"explore\": False}\n",
    "trained_trainer = DQNTrainer(config=config)\n",
    "trained_trainer.restore(best_checkpoint)\n",
    "price_agent0_list = []\n",
    "reward_agent0_list = []\n",
    "price_agent1_list = []\n",
    "reward_agent1_list = []\n",
    "obs, reward, done, info = env.step({\"agent_0\": 1, \"agent_1\": 11})\n",
    "for i in range(500):\n",
    "\n",
    "    action_agent0 = trained_trainer.compute_action(obs[\"agent_0\"], policy_id=\"policy_0\")\n",
    "    action_agent1 = trained_trainer.compute_action(obs[\"agent_1\"], policy_id=\"policy_1\")\n",
    "    obs, reward, done, info = env.step(\n",
    "        {\"agent_0\": action_agent0, \"agent_1\": action_agent1}\n",
    "    )\n",
    "    price_agent0_list.append(info[\"agent_0\"])\n",
    "    reward_agent0_list.append(reward[\"agent_0\"])\n",
    "    price_agent1_list.append(info[\"agent_1\"])\n",
    "    reward_agent1_list.append(reward[\"agent_1\"])\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "plt.plot(price_agent0_list)\n",
    "plt.show()\n",
    "plt.plot(price_agent1_list)\n",
    "plt.show()\n",
    "\n",
    "IRresults = {\n",
    "    \"Profits Agent 0\": reward_agent0_list,\n",
    "    \"Profits Agent 1\": reward_agent1_list,\n",
    "    \"Price Agent 0\": price_agent0_list,\n",
    "    \"Price Agent 1\": price_agent1_list,\n",
    "}\n",
    "df_IR = pd.DataFrame(IRresults)\n",
    "df_IR.to_csv(\"collusion_IR_DQN.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}